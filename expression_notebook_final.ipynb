{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arazm21/ML-homework_4/blob/main/expression_notebook_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the data and organising it"
      ],
      "metadata": {
        "id": "ShlkPaeoBQ3k"
      },
      "id": "ShlkPaeoBQ3k"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNNvBwljBQFE",
        "outputId": "04b641c0-20d8-4c68-e645-962b5f35422a"
      },
      "id": "dNNvBwljBQFE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 96% 273M/285M [00:00<00:00, 582MB/s]\n",
            "100% 285M/285M [00:04<00:00, 74.7MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "9fXPkihKllSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987a3cdf-43fd-4959-ddbb-7d515790c538"
      },
      "id": "9fXPkihKllSZ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "id": "xib5nVmSLH0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de75c22-9b77-466f-bd34-3be314f1e280"
      },
      "id": "xib5nVmSLH0o",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ─── 1) Augmentations for minority classes ────────────────────────────────────\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),  # finally convert to tensor in [0,1]\n",
        "])\n",
        "\n",
        "# ─── 2) Base “no‐transform” behavior ─────────────────────────────────────────\n",
        "\n",
        "class ExpressionDatasetFromDF(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Expect columns exactly [\"emotion\", \"pixels\", \"Usage\"]\n",
        "        # Each “pixels” entry is a string of “230  19  …”\n",
        "        self.images = dataframe[\" pixels\"].apply(\n",
        "            lambda x: np.fromstring(x, sep=\" \", dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        # Stack into a (N, 1, 48, 48) float32 tensor in [0,1]\n",
        "        self.images = torch.tensor(\n",
        "            np.stack(self.images.values), dtype=torch.float32\n",
        "        ).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(dataframe[\"emotion\"].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ─── 3) Oversampling / augmentation wrapper ──────────────────────────────────\n",
        "\n",
        "class AugmentedExpressionDataset(Dataset):\n",
        "    def __init__(self, base_dataset, targets, num_aug_per_sample=1):\n",
        "        \"\"\"\n",
        "        base_dataset: an ExpressionDatasetFromDF (no transforms applied)\n",
        "        targets: list of ints (same length as base_dataset)\n",
        "        \"\"\"\n",
        "        self.base_dataset = base_dataset\n",
        "        self.targets = targets\n",
        "        self.class_counts = Counter(targets)\n",
        "        self.max_count = max(self.class_counts.values())\n",
        "\n",
        "        # Build a list of “base indices” to duplicate + augment\n",
        "        self.augmented_indices = []\n",
        "        for cls_label, count in self.class_counts.items():\n",
        "            n_to_add = self.max_count - count\n",
        "            if n_to_add <= 0:\n",
        "                continue\n",
        "            # pick with replacement from all indices whose target == cls_label\n",
        "            indices_of_cls = [\n",
        "                i for i, t in enumerate(targets) if t == cls_label\n",
        "            ]\n",
        "            sampled = random.choices(indices_of_cls, k=n_to_add * num_aug_per_sample)\n",
        "            self.augmented_indices.extend(sampled)\n",
        "\n",
        "        print(f\"‣ Base dataset size: {len(self.base_dataset)}\")\n",
        "        print(f\"‣ Augmenting {len(self.augmented_indices)} extra samples to balance classes.\")\n",
        "        print(f\"‣ Resulting dataset size: {len(self.base_dataset) + len(self.augmented_indices)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset) + len(self.augmented_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.base_dataset):\n",
        "            # no augmentation for the “original” sample\n",
        "            img_tensor, label = self.base_dataset[idx]\n",
        "            return img_tensor, label\n",
        "        else:\n",
        "            # for augmented ones: take the base index, convert back to PIL, augment, then ToTensor\n",
        "            base_idx = self.augmented_indices[idx - len(self.base_dataset)]\n",
        "            img_tensor, label = self.base_dataset[base_idx]\n",
        "            # img_tensor is (1,48,48) float in [0,1]. Convert back to uint8 PIL:\n",
        "            arr_uint8 = (img_tensor.squeeze().numpy() * 255).astype(np.uint8)\n",
        "            pil_img = Image.fromarray(arr_uint8, mode=\"L\")\n",
        "            img_aug = aug_transform(pil_img)  # now a (1,48,48) FloatTensor in [0,1]\n",
        "            return img_aug, label\n",
        "\n",
        "\n",
        "# ─── 4) Revised get_data (no more “slice”) ───────────────────────────────────\n",
        "\n",
        "def get_data(csv_file=\"icml_face_data.csv\", train=True):\n",
        "    \"\"\"\n",
        "    - Reads icml_face_data.csv (which has exactly [\"emotion\",\"pixels\",\"Usage\"]).\n",
        "    - Splits by Usage == \"Training\" vs. \"PublicTest\".\n",
        "    - For train: wraps in AugmentedExpressionDataset. For test/val: returns raw dataset.\n",
        "    \"\"\"\n",
        "    full_df = pd.read_csv(csv_file)\n",
        "\n",
        "    if train:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"Training\"].reset_index(drop=True)\n",
        "    else:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"PublicTest\"].reset_index(drop=True)\n",
        "\n",
        "    print(f\"‣ Loaded '{'Training' if train else 'PublicTest'}' => {len(df_part)} samples before augmentation/slicing.\")\n",
        "\n",
        "    base_ds = ExpressionDatasetFromDF(df_part)\n",
        "\n",
        "    if train:\n",
        "        targets = df_part[\"emotion\"].tolist()\n",
        "        balanced_ds = AugmentedExpressionDataset(base_ds, targets)\n",
        "        return balanced_ds\n",
        "    else:\n",
        "        return base_ds\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=1,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "b5Ptu8H4Lzx6"
      },
      "id": "b5Ptu8H4Lzx6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ─── 1) Augmentations for minority classes ────────────────────────────────────\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),  # finally convert to tensor in [0,1]\n",
        "])\n",
        "\n",
        "# ─── 2) Base “no‐transform” behavior ─────────────────────────────────────────\n",
        "\n",
        "class ExpressionDatasetFromDF(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Expect columns exactly [\"emotion\", \"pixels\", \"Usage\"]\n",
        "        # Each “pixels” entry is a string of “230  19  …”\n",
        "        self.images = dataframe[\" pixels\"].apply(\n",
        "            lambda x: np.fromstring(x, sep=\" \", dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        # Stack into a (N, 1, 48, 48) float32 tensor in [0,1]\n",
        "        self.images = torch.tensor(\n",
        "            np.stack(self.images.values), dtype=torch.float32\n",
        "        ).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(dataframe[\"emotion\"].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ─── 3) Oversampling / augmentation wrapper ──────────────────────────────────\n",
        "\n",
        "class AugmentedExpressionDataset(Dataset):\n",
        "    def __init__(self, base_dataset, targets, num_aug_per_sample=1):\n",
        "        \"\"\"\n",
        "        base_dataset: an ExpressionDatasetFromDF (no transforms applied)\n",
        "        targets: list of ints (same length as base_dataset)\n",
        "        \"\"\"\n",
        "        self.base_dataset = base_dataset\n",
        "        self.targets = targets\n",
        "        self.class_counts = Counter(targets)\n",
        "        self.max_count = max(self.class_counts.values())\n",
        "\n",
        "        # Build a list of “base indices” to duplicate + augment\n",
        "        self.augmented_indices = []\n",
        "        for cls_label, count in self.class_counts.items():\n",
        "            n_to_add = self.max_count - count\n",
        "            if n_to_add <= 0:\n",
        "                continue\n",
        "            # pick with replacement from all indices whose target == cls_label\n",
        "            indices_of_cls = [\n",
        "                i for i, t in enumerate(targets) if t == cls_label\n",
        "            ]\n",
        "            sampled = random.choices(indices_of_cls, k=n_to_add * num_aug_per_sample)\n",
        "            self.augmented_indices.extend(sampled)\n",
        "\n",
        "        print(f\"‣ Base dataset size: {len(self.base_dataset)}\")\n",
        "        print(f\"‣ Augmenting {len(self.augmented_indices)} extra samples to balance classes.\")\n",
        "        print(f\"‣ Resulting dataset size: {len(self.base_dataset) + len(self.augmented_indices)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset) + len(self.augmented_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.base_dataset):\n",
        "            # no augmentation for the “original” sample\n",
        "            img_tensor, label = self.base_dataset[idx]\n",
        "            return img_tensor, label\n",
        "        else:\n",
        "            # for augmented ones: take the base index, convert back to PIL, augment, then ToTensor\n",
        "            base_idx = self.augmented_indices[idx - len(self.base_dataset)]\n",
        "            img_tensor, label = self.base_dataset[base_idx]\n",
        "            # img_tensor is (1,48,48) float in [0,1]. Convert back to uint8 PIL:\n",
        "            arr_uint8 = (img_tensor.squeeze().numpy() * 255).astype(np.uint8)\n",
        "            pil_img = Image.fromarray(arr_uint8, mode=\"L\")\n",
        "            img_aug = aug_transform(pil_img)  # now a (1,48,48) FloatTensor in [0,1]\n",
        "            return img_aug, label\n",
        "\n",
        "\n",
        "# ─── 4) Revised get_data (no more “slice”) ───────────────────────────────────\n",
        "\n",
        "def get_data(csv_file=\"icml_face_data.csv\", train=True):\n",
        "    \"\"\"\n",
        "    - Reads icml_face_data.csv (which has exactly [\"emotion\",\"pixels\",\"Usage\"]).\n",
        "    - Splits by Usage == \"Training\" vs. \"PublicTest\".\n",
        "    - For train: wraps in AugmentedExpressionDataset. For test/val: returns raw dataset.\n",
        "    \"\"\"\n",
        "    full_df = pd.read_csv(csv_file)\n",
        "\n",
        "    if train:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"Training\"].reset_index(drop=True)\n",
        "    else:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"PublicTest\"].reset_index(drop=True)\n",
        "\n",
        "    print(f\"‣ Loaded '{'Training' if train else 'PublicTest'}' => {len(df_part)} samples before augmentation/slicing.\")\n",
        "\n",
        "    base_ds = ExpressionDatasetFromDF(df_part)\n",
        "\n",
        "    if train:\n",
        "        targets = df_part[\"emotion\"].tolist()\n",
        "        balanced_ds = AugmentedExpressionDataset(base_ds, targets)\n",
        "        return balanced_ds\n",
        "    else:\n",
        "        return base_ds\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "\n",
        "# ─── 5) Quick sanity check ───────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Inspect base “Training” size (should be ~28,709 for FER2013).\n",
        "    train_ds = get_data(csv_file=\"icml_face_data.csv\", train=True)\n",
        "    print(f\"Len of train_ds (after augmentation): {len(train_ds)}\\n\")\n",
        "\n",
        "    # 2) Inspect base “PublicTest” size (should be ~3,589).\n",
        "    val_ds = get_data(csv_file=\"icml_face_data.csv\", train=False)\n",
        "    print(f\"Len of val_ds (no augmentation): {len(val_ds)}\\n\")\n",
        "\n",
        "    # 3) Create DataLoader and confirm iteration count\n",
        "    train_loader = make_loader(train_ds, batch_size=64)\n",
        "    total_seen = sum(len(batch[0]) for batch in train_loader)\n",
        "    print(f\"Total images seen via train_loader: {total_seen}\")\n"
      ],
      "metadata": {
        "id": "D1ye0F1iHqSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc82e9a-b055-41fc-edaa-1d1abb1dd258"
      },
      "id": "D1ye0F1iHqSC",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "Len of train_ds (after augmentation): 50505\n",
            "\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "Len of val_ds (no augmentation): 3589\n",
            "\n",
            "Total images seen via train_loader: 50505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test that loading was ok"
      ],
      "metadata": {
        "id": "RsFhOzV_PHxI"
      },
      "id": "RsFhOzV_PHxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and create loader\n",
        "dataset = get_data(train=False)\n",
        "loader = make_loader(dataset, batch_size=3)\n",
        "\n",
        "# Get a batch\n",
        "images, labels = next(iter(loader))\n",
        "\n",
        "# Class names from FER2013\n",
        "emotion_names = [\n",
        "    \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"\n",
        "]\n",
        "\n",
        "# Plot the first 3 images\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    plt.title(f\"Label: {emotion_names[labels[i].item()]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NvgplaWYHJX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "71411aae-a0a4-4208-cfd7-ee0ef8500263"
      },
      "id": "NvgplaWYHJX6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connecting WANDB"
      ],
      "metadata": {
        "id": "XhwXYgieSf7u"
      },
      "id": "XhwXYgieSf7u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "zt2tymF2Se4p"
      },
      "id": "zt2tymF2Se4p",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "l2Op72b3SsY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e9e2456e-0151-4b5e-a1d0-2d610a3e0b58"
      },
      "id": "l2Op72b3SsY1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marazm21\u001b[0m (\u001b[33marazm21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# different attempted architectures"
      ],
      "metadata": {
        "id": "FjvVv9RPXFbN"
      },
      "id": "FjvVv9RPXFbN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## baseline model"
      ],
      "metadata": {
        "id": "5jXpkWMRxArB"
      },
      "id": "5jXpkWMRxArB"
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, padding = 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n",
        "      self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
        "      self.pooling = nn.MaxPool2d(2,2)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear = nn.Linear((128 * 6 * 6), 128)\n",
        "      self.output = nn.Linear(128, 7)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.linear(x)\n",
        "      x = self.output(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "vJXLEBWwxESi"
      },
      "id": "vJXLEBWwxESi",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enhanced baseline model"
      ],
      "metadata": {
        "id": "UdBMGddGyZl9"
      },
      "id": "UdBMGddGyZl9"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedBaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 48 -> 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 24 -> 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 12 -> 6\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 6 -> 3\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 7)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cy1jxwZQyaYe"
      },
      "id": "Cy1jxwZQyaYe",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImprovedModel"
      ],
      "metadata": {
        "id": "6Y6FOkFKqwWc"
      },
      "id": "6Y6FOkFKqwWc"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        # — conv backbone with 4 pooling stages —\n",
        "        self.conv1    = nn.Conv2d(1,   32, 3, padding=1, bias=False)\n",
        "        self.bn1      = nn.BatchNorm2d(32)\n",
        "        self.conv2    = nn.Conv2d(32,  64, 3, padding=1, bias=False)\n",
        "        self.bn2      = nn.BatchNorm2d(64)\n",
        "        self.conv3    = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3      = nn.BatchNorm2d(128)\n",
        "        self.conv4    = nn.Conv2d(128,256, 3, padding=1, bias=False)\n",
        "        self.bn4      = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool     = nn.MaxPool2d(2,2)\n",
        "        self.relu     = nn.ReLU(inplace=True)\n",
        "        self.drop_conv= nn.Dropout2d(dropout_p)\n",
        "\n",
        "        # # — two extra conv layers (no further pooling) —\n",
        "        # self.conv5    = nn.Conv2d(256, 512, 3, padding=1, bias=False)\n",
        "        # self.bn5      = nn.BatchNorm2d(512)\n",
        "        # self.conv6    = nn.Conv2d(512, 512, 3, padding=1, bias=False)\n",
        "        # self.bn6      = nn.BatchNorm2d(512)\n",
        "\n",
        "        # — richer head with 4 FCs —\n",
        "        self.flatten  = nn.Flatten()\n",
        "        # Corrected input size for the first linear layer\n",
        "        self.fc1      = nn.Linear(256 * 3 * 3, 512, bias=False) # 256 channels * 3x3 spatial size\n",
        "        self.bn_fc1   = nn.BatchNorm1d(512)\n",
        "        self.drop1    = nn.Dropout(dropout_p)\n",
        "        self.fc2      = nn.Linear(512, 256, bias=False)\n",
        "        self.bn_fc2   = nn.BatchNorm1d(256)\n",
        "        self.drop2    = nn.Dropout(dropout_p)\n",
        "        self.fc3      = nn.Linear(256, 128, bias=False)\n",
        "        self.bn_fc3   = nn.BatchNorm1d(128)\n",
        "        self.drop3    = nn.Dropout(dropout_p)\n",
        "        self.fc4      = nn.Linear(128, 64, bias=False)\n",
        "        self.bn_fc4   = nn.BatchNorm1d(64)\n",
        "        self.drop4    = nn.Dropout(dropout_p)\n",
        "        self.output   = nn.Linear(64, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv blocks 1–4 with pooling\n",
        "        for conv, bn in [(self.conv1,self.bn1),\n",
        "                         (self.conv2,self.bn2),\n",
        "                         (self.conv3,self.bn3),\n",
        "                         (self.conv4,self.bn4)]:\n",
        "            x = conv(x)\n",
        "            x = bn(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.pool(x)\n",
        "            x = self.drop_conv(x)\n",
        "\n",
        "        # # extra conv blocks 5–6 (no more pooling)\n",
        "        # x = self.conv5(x)\n",
        "        # x = self.bn5(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.drop_conv(x)\n",
        "\n",
        "        # x = self.conv6(x)\n",
        "        # x = self.bn6(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.drop_conv(x)\n",
        "\n",
        "        # FC head\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn_fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn_fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop4(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YVdHBSmXqwFC"
      },
      "id": "YVdHBSmXqwFC",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convnet"
      ],
      "metadata": {
        "id": "jjCDTHA1RHhj"
      },
      "id": "jjCDTHA1RHhj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# class ConvNet_super_simple(nn.Module):\n",
        "#     def __init__(self, kernels, classes=7):\n",
        "#         super(ConvNet_super_simple, self).__init__()\n",
        "\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         # Assuming 48x48 input, after two 2x2 poolings -> 48/2/2 = 12x12\n",
        "#         self.fc = nn.Linear(12 * 12 * kernels[1], classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet_Improved(nn.Module):\n",
        "    def __init__(self, kernels, classes=7):\n",
        "        super(ConvNet_Improved, self).__init__()\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(1, kernels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(kernels[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(kernels[0], kernels[1], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(kernels[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(kernels[1], kernels[2], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(kernels[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Assuming 48x48 input → 3 poolings: 48 → 24 → 12 → 6\n",
        "        self.flattened_dim = 6 * 6 * kernels[2]\n",
        "        self.fc = nn.Linear(self.flattened_dim, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.dropout1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "        x = self.pool2(self.dropout2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "        x = self.pool3(self.dropout3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RaFvctrZTf1t"
      },
      "id": "RaFvctrZTf1t",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "cumrZOH9Sj1A"
      },
      "id": "cumrZOH9Sj1A"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        # out = self.dropout(out)  # ✅ Dropout after residual addition\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SimpleResNet15(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.13):\n",
        "        super(SimpleResNet15, self).__init__()\n",
        "\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layers = nn.Sequential(\n",
        "          ResidualBlock(32, 64, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(64, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(512, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(1024, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "          ResidualBlock(2048, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(1024, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(512, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "          ResidualBlock(256, 256, downsample=False, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(256, 256, downsample=False, dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, 256)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "PFYKchqhXTSt"
      },
      "id": "PFYKchqhXTSt",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## googlenet(mini)"
      ],
      "metadata": {
        "id": "ltk-bzVKSnDU"
      },
      "id": "ltk-bzVKSnDU"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class MiniInception(nn.Module):\n",
        "#     def __init__(self, in_ch, c1, c3red, c3, pool_proj):\n",
        "#         super().__init__()\n",
        "#         # 1×1 branch\n",
        "#         self.b1 = nn.Conv2d(in_ch, c1, kernel_size=1)\n",
        "#         # 1×1 → 3×3 branch\n",
        "#         self.b2_1 = nn.Conv2d(in_ch, c3red, kernel_size=1)\n",
        "#         self.b2_2 = nn.Conv2d(c3red, c3,   kernel_size=3, padding=1)\n",
        "#         # pool → 1×1 branch\n",
        "#         self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "#         self.b3_proj = nn.Conv2d(in_ch, pool_proj, kernel_size=1)\n",
        "#         self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b1 = self.b1(x)\n",
        "#         b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "#         b3 = self.b3_proj(self.b3_pool(x))\n",
        "#         out = torch.cat([b1, b2, b3], dim=1)\n",
        "#         return F.relu(self.bn(out))\n",
        "\n",
        "\n",
        "# class MiniGoogLeNet(nn.Module):\n",
        "#     def __init__(self, num_classes=7, aux_on=True):\n",
        "#         super().__init__()\n",
        "#         self.aux_on = aux_on\n",
        "\n",
        "#         # ---- stem ----\n",
        "#         self.stem = nn.Sequential(\n",
        "#             nn.Conv2d(1, 32, 3, padding=1),\n",
        "#             nn.BatchNorm2d(32), nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, 2)  # 48→24\n",
        "#         )\n",
        "\n",
        "#         # ---- two Inception blocks ----\n",
        "#         self.inc1 = MiniInception(32, c1=16, c3red=16, c3=24, pool_proj=16)  # outputs 56\n",
        "#         self.inc2 = MiniInception(56, c1=32, c3red=24, c3=32, pool_proj=24)  # outputs 88\n",
        "\n",
        "#         # auxiliary head (after inc1)\n",
        "#         if aux_on:\n",
        "#             self.aux = nn.Sequential(\n",
        "#                 nn.AdaptiveAvgPool2d((4,4)),\n",
        "#                 nn.Conv2d(56, 32, 1), nn.ReLU(),\n",
        "#                 nn.Flatten(),\n",
        "#                 nn.Linear(32*4*4, 128), nn.ReLU(), nn.Dropout(0.5),\n",
        "#                 nn.Linear(128, num_classes)\n",
        "#             )\n",
        "\n",
        "#         # ---- classifier head ----\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "#         self.fc   = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.Linear(88, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.stem(x)            # → [B,32,24,24]\n",
        "#         x1 = self.inc1(x)           # → [B,56,24,24]\n",
        "\n",
        "#         # you can still compute aux_out if you want,\n",
        "#         # but we won't return it so training code stays unchanged\n",
        "#         if self.training and self.aux_on:\n",
        "#             _ = self.aux(x1)\n",
        "\n",
        "#         x2 = F.max_pool2d(x1, 2, 2) # → [B,56,12,12]\n",
        "#         x2 = self.inc2(x2)          # → [B,88,12,12]\n",
        "\n",
        "#         x3 = self.pool(x2)          # → [B,88,1,1]\n",
        "#         main_out = self.fc(x3)      # → [B,num_classes]\n",
        "\n",
        "#         return main_out\n",
        "class MiniInception(nn.Module):\n",
        "    def __init__(self, in_ch, c1, c3red, c3, pool_proj, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 1×1 branch\n",
        "        self.b1 = nn.Conv2d(in_ch, c1, 1)\n",
        "        # 1×1 → 3×3 branch\n",
        "        self.b2_1 = nn.Conv2d(in_ch, c3red, 1)\n",
        "        self.b2_2 = nn.Conv2d(c3red, c3, 3, padding=1)\n",
        "        # pool → 1×1 branch\n",
        "        self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.b3_proj = nn.Conv2d(in_ch, pool_proj, 1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "        b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "        b3 = self.b3_proj(self.b3_pool(x))\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class ComplexMiniGoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # ── Stem ──\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 48 → 24\n",
        "        )\n",
        "\n",
        "        # ── Inception Blocks ──\n",
        "        self.inc1 = MiniInception(32, c1=16, c3red=16, c3=32, pool_proj=16, dropout=0.2)   # →64 ch\n",
        "        self.inc2 = MiniInception(64, c1=24, c3red=24, c3=48, pool_proj=24, dropout=0.2)   # →96 ch\n",
        "        self.inc3 = MiniInception(96, c1=32, c3red=32, c3=64, pool_proj=32, dropout=0.3)   # →128 ch\n",
        "\n",
        "        # ── Auxiliary head (computed but not returned) ──\n",
        "        if aux_on:\n",
        "            self.aux = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d((4, 4)),\n",
        "                nn.Conv2d(96, 48, 1), nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(48 * 4 * 4, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "\n",
        "        # ── Final classifier ──\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)          # → [B,32,24,24]\n",
        "        x1 = self.inc1(x)         # → [B,64,24,24]\n",
        "        x2 = self.inc2(x1)        # → [B,96,24,24]\n",
        "\n",
        "        # compute aux but ignore its output\n",
        "        if self.training and self.aux_on:\n",
        "            _ = self.aux(x2)\n",
        "\n",
        "        x3 = F.max_pool2d(x2, 2, 2)  # → [B,96,12,12]\n",
        "        x3 = self.inc3(x3)           # → [B,128,12,12]\n",
        "\n",
        "        x4 = self.final_pool(x3)     # → [B,128,1,1]\n",
        "        main_out = self.classifier(x4)\n",
        "        return main_out"
      ],
      "metadata": {
        "id": "2ydG-CJ4SsQ_"
      },
      "id": "2ydG-CJ4SsQ_",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## densenet"
      ],
      "metadata": {
        "id": "IH2tbOH_Hd8Z"
      },
      "id": "IH2tbOH_Hd8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        return torch.cat([x, out], dim=1)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(DenseLayer(in_channels, growth_rate))\n",
        "            in_channels += growth_rate\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.relu(self.bn(x)))\n",
        "        return self.pool(x)\n",
        "\n",
        "class MiniDenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 2 * growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        num_channels = 2 * growth_rate\n",
        "\n",
        "        self.block1 = DenseBlock(num_channels, growth_rate, num_layers=4)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans1 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.block2 = DenseBlock(num_channels, growth_rate, num_layers=4)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans2 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(num_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(self.block1(x))\n",
        "        x = self.trans2(self.block2(x))\n",
        "        x = self.relu(self.bn(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "s_2N0muZHftN"
      },
      "id": "s_2N0muZHftN",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vision transformer"
      ],
      "metadata": {
        "id": "LIri0mc2NmsS"
      },
      "id": "LIri0mc2NmsS"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import repeat\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=1, patch_size=8, emb_size=128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class ExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True, img_size=48, patch_size=8,\n",
        "                 emb_dim=128, n_layers=6, dropout=0.1, heads=4):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head (after layer n_layers//2, similar to your original design)\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after middle layer)\n",
        "            if i == self.n_layers // 2 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]  # Extract cls token\n",
        "                _ = self.aux_head(aux_cls_token)  # Compute but don't return\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "\n",
        "class CompactExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # Compact configuration for faster training\n",
        "        img_size = 48\n",
        "        patch_size = 6\n",
        "        emb_dim = 96\n",
        "        n_layers = 4\n",
        "        heads = 3\n",
        "        dropout = 0.2\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2  # 16 patches\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim * 2, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after 2nd layer for compact model)\n",
        "            if i == 1 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]\n",
        "                _ = self.aux_head(aux_cls_token)\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 1, patch_size = 8, emb_size = 128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "from einops import repeat\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, ch=1, img_size=48, patch_size=4, emb_dim=64,\n",
        "                n_layers=12, out_dim=7, dropout=0.2, heads=8):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.channels = ch\n",
        "        self.height = img_size\n",
        "        self.width = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "        # Output based on classification token\n",
        "        return self.head(x[:, 0, :])\n",
        "\n"
      ],
      "metadata": {
        "id": "_2nPvP7QNlqo"
      },
      "id": "_2nPvP7QNlqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def ViT(cf):\n",
        "    inputs = Input(shape=cf[\"input_shape\"])\n",
        "    patches = Patches(cf[\"patch_size\"])(inputs)\n",
        "    x = PatchEncoder(num_patches=cf[\"num_patches\"], projection_dim=cf[\"projection_dim\"])(patches)\n",
        "    cls_token = ClassToken()(x)\n",
        "    x = Concatenate(axis=1)([cls_token, x])\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = x[:, 0]\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "IzpTX__egM29"
      },
      "id": "IzpTX__egM29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# geting everything ready"
      ],
      "metadata": {
        "id": "zaVbntQmXODJ"
      },
      "id": "zaVbntQmXODJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train_dataset = get_data(train=True)\n",
        "    test_dataset = get_data(train=False)\n",
        "    train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = ComplexMiniGoogLeNet().to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "4GOnNEKyT1v2"
      },
      "id": "4GOnNEKyT1v2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "Jwbd9gvXUp4R"
      },
      "id": "Jwbd9gvXUp4R",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## early stop training"
      ],
      "metadata": {
        "id": "6Vih9mc96ns6"
      },
      "id": "6Vih9mc96ns6"
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=15, min_delta=0.001)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            loss, batch_correct, batch_total = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            running_correct += batch_correct\n",
        "            running_total += batch_total\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        train_acc = running_correct / running_total\n",
        "\n",
        "        # ⏱️ Validate at the end of the epoch\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_accuracy\": train_acc\n",
        "        })\n",
        "        print(f\"Epoch {epoch + 1}: val_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}, train_acc = {train_acc:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopper(val_loss)\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "\n",
        "    return loss, correct, total"
      ],
      "metadata": {
        "id": "2gDrRJB56ihi"
      },
      "id": "2gDrRJB56ihi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normal training"
      ],
      "metadata": {
        "id": "Ts0xewao6rsq"
      },
      "id": "Ts0xewao6rsq"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config, class_names= [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]):\n",
        "    # wandb.init(\n",
        "    #     project=\"\",\n",
        "    #     config={\n",
        "    #         \"epochs\": config.epochs,\n",
        "    #         \"batch_size\": train_loader.batch_size,\n",
        "    #         \"optimizer\": optimizer.__class__.__name__,\n",
        "    #         \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "    #         \"criterion\": criterion.__class__.__name__,\n",
        "    #     }\n",
        "    # )\n",
        "    # wandb.watch(model, log=\"all\", log_freq=100)\n",
        "    model.to(config.device)\n",
        "\n",
        "    train_loss_plot, val_loss_plot = [], []\n",
        "    train_acc_plot, val_acc_plot = [], []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
        "            images, labels = images.to(config.device), labels.to(config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = total_train_loss / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc, all_targets, all_preds = validate(model, val_loader, criterion, config.device)\n",
        "\n",
        "        # F1 and Confusion Matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "        log_data = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "        }\n",
        "        for i, name in enumerate(class_names):\n",
        "            log_data[f\"f1_{name}\"] = f1_per_class[i]\n",
        "        wandb.log(log_data)\n",
        "\n",
        "        # Confusion matrix image\n",
        "        fig, ax = plt.subplots(figsize=(6,6))\n",
        "        im = ax.imshow(cm, cmap=\"Blues\")\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set_xticks(np.arange(len(class_names)))\n",
        "        ax.set_yticks(np.arange(len(class_names)))\n",
        "        ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
        "        ax.set_yticklabels(class_names)\n",
        "        for i in range(len(class_names)):\n",
        "            for j in range(len(class_names)):\n",
        "                ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "        ax.set_title(f\"Confusion Matrix - Epoch {epoch+1}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Store for final plots\n",
        "        train_loss_plot.append(train_loss)\n",
        "        val_loss_plot.append(val_loss)\n",
        "        train_acc_plot.append(train_acc)\n",
        "        val_acc_plot.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs}: \"\n",
        "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
        "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    # Final loss/accuracy curves\n",
        "    epochs_range = list(range(1, config.epochs + 1))\n",
        "\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(epochs_range, train_loss_plot, label=\"Train Loss\")\n",
        "    ax1.plot(epochs_range, val_loss_plot, label=\"Val Loss\")\n",
        "    ax1.set_title(\"Loss vs Epoch\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    wandb.log({\"loss_curve\": wandb.Image(fig1)})\n",
        "    plt.close(fig1)\n",
        "\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(epochs_range, train_acc_plot, label=\"Train Accuracy\")\n",
        "    ax2.plot(epochs_range, val_acc_plot, label=\"Val Accuracy\")\n",
        "    ax2.set_title(\"Accuracy vs Epoch\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    ax2.legend()\n",
        "    wandb.log({\"accuracy_curve\": wandb.Image(fig2)})\n",
        "    plt.close(fig2)\n",
        "\n",
        "    return model\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / total\n",
        "    val_acc = correct / total\n",
        "    return avg_val_loss, val_acc, all_targets, all_preds\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "def test(model, test_loader, device, class_names=None):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "    misclassified_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Save misclassified images\n",
        "            mis_mask = (predicted != labels)\n",
        "            mis_imgs = images[mis_mask]\n",
        "            mis_lbls = labels[mis_mask]\n",
        "            mis_preds = predicted[mis_mask]\n",
        "            for img, true_lbl, pred_lbl in zip(mis_imgs, mis_lbls, mis_preds):\n",
        "                misclassified_images.append((img.cpu(), true_lbl.item(), pred_lbl.item()))\n",
        "\n",
        "    acc = correct / total\n",
        "    f1_macro = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {acc:.2%}\")\n",
        "    print(f\"F1 Macro: {f1_macro:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_targets, all_preds, target_names=class_names))\n",
        "\n",
        "    # Log numeric metrics\n",
        "    log_data = {\n",
        "        \"test_accuracy\": acc,\n",
        "        \"test_f1_macro\": f1_macro,\n",
        "    }\n",
        "    if class_names:\n",
        "        for i, name in enumerate(class_names):\n",
        "            log_data[f\"f1_{name}\"] = f1_per_class[i]\n",
        "    wandb.log(log_data)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        xticklabels=class_names if class_names else \"auto\",\n",
        "        yticklabels=class_names if class_names else \"auto\",\n",
        "        cmap=\"Blues\"\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_title(\"Confusion Matrix (Test)\")\n",
        "    # wandb.log({\n",
        "    #     \"confusion_matrix_final\": wandb.Image(fig),\n",
        "    #     \"confusion_matrix_raw\": wandb.Table(data=cm.tolist(), columns=class_names, rows=class_names),\n",
        "    # })\n",
        "    plt.close(fig)\n",
        "\n",
        "    wandb.log({\n",
        "    \"confusion_matrix_final\": wandb.Image(fig),\n",
        "    \"confusion_matrix_raw\": wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=all_targets,\n",
        "        preds=all_preds,\n",
        "        class_names=class_names if class_names else None\n",
        "    )\n",
        "})\n",
        "    # Log a few misclassified images\n",
        "    if class_names:\n",
        "        wrong_table = wandb.Table(columns=[\"Image\", \"True Label\", \"Predicted Label\"])\n",
        "        for img, true_lbl, pred_lbl in misclassified_images[:25]:\n",
        "            img_np = img.squeeze().numpy()  # assumes grayscale\n",
        "            wrong_table.add_data(\n",
        "                wandb.Image(img_np, caption=f\"Pred: {class_names[pred_lbl]}, GT: {class_names[true_lbl]}\"),\n",
        "                class_names[true_lbl],\n",
        "                class_names[pred_lbl]\n",
        "            )\n",
        "        wandb.log({\"misclassified_samples\": wrong_table})\n",
        "\n",
        "    # Export ONNX\n",
        "    dummy_input = torch.randn(1, *images.shape[1:]).to(device)\n",
        "    torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")\n",
        "\n",
        "    # Optionally log model architecture as a string\n",
        "    model_arch_str = str(model)\n",
        "    wandb.log({\"model_architecture\": wandb.Html(f\"<pre>{model_arch_str}</pre>\")})\n"
      ],
      "metadata": {
        "id": "F3VQ1HxlUk6V"
      },
      "id": "F3VQ1HxlUk6V",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipeline"
      ],
      "metadata": {
        "id": "UU0HM605Auy7"
      },
      "id": "UU0HM605Auy7"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"expression_dataset_final\",\n",
        "                    config=hyperparameters,\n",
        "                    name = \"ComplexMiniGoogLeNet\"):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      # train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # # and test its final performance\n",
        "      # test(model, test_loader)\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, config)\n",
        "      test(model, test_loader, config.device)  # final test; you can use actual test set here if available\n",
        "\n",
        "    wandb.finish()\n",
        "    return model"
      ],
      "metadata": {
        "id": "cEsYfYCkUbYw"
      },
      "id": "cEsYfYCkUbYw",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=50,\n",
        "    classes=7,\n",
        "    #kernels=[32, 64, 128],\n",
        "    batch_size=256,\n",
        "    learning_rate=6e-4,\n",
        "    dataset=\"Facial Expression Recognition\",\n",
        "    architecture=\"ComplexMiniGoogLeNet_longer\",\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "GoXMnqIWY9fa"
      },
      "id": "GoXMnqIWY9fa",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "train_dataset = get_data(train=True)\n",
        "test_dataset = get_data(train=False)\n",
        "train_loader = make_loader(train_dataset, batch_size=config['batch_size'])\n",
        "test_loader = make_loader(test_dataset, batch_size=config['batch_size'])\n",
        "for batch in train_loader:\n",
        "    images, labels = batch\n",
        "    total += len(images)\n",
        "# Count class occurrences\n",
        "all_labels = []\n",
        "for _, labels in train_loader:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "label_counts = Counter(all_labels)\n",
        "\n",
        "# Print counts\n",
        "print(\"Label distribution in augmented training data:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "# Optional: plot for visual confirmation\n",
        "plt.bar(label_counts.keys(), label_counts.values(), tick_label=[str(i) for i in label_counts.keys()])\n",
        "plt.xlabel('Emotion Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution After Augmentation')\n",
        "plt.show()\n",
        "print(f\"Total images seen via train_loader: {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "uTtkeP3VhRZO",
        "outputId": "21eedf9d-a034-440b-c1db-71093389347e"
      },
      "id": "uTtkeP3VhRZO",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "Label distribution in augmented training data:\n",
            "Class 0: 7215 samples\n",
            "Class 1: 7215 samples\n",
            "Class 2: 7215 samples\n",
            "Class 3: 7215 samples\n",
            "Class 4: 7215 samples\n",
            "Class 5: 7215 samples\n",
            "Class 6: 7215 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUYJJREFUeJzt3XlcTfn/B/DXLd3bXoq2QVKWsmQnjb0pZDCMNaOxMzUoY+k3xpIZxpJtZB3KzGRsX2uGJNsgW2SNYURmqGZI0aioz+8Pj864KtOlunFez8fjPB7O5/O557zPueHV555zrkIIIUBEREQkYzraLoCIiIhI2xiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIjonVC9enV8+umn2i7jjU2fPh0KhaJM9tWuXTu0a9dOWj906BAUCgW2bNlSJvv/9NNPUb169TLZ15s4ffo0WrVqBSMjIygUCsTHx2u7JCoD4eHhUCgUuHXrlrZLoTLCQETl2u+//46RI0eiRo0a0NfXh6mpKdzd3bF48WI8efJE2+W9Uv4/qPmLvr4+7Ozs4OXlhSVLluDRo0clsp+7d+9i+vTp5fI/6vJcGwAkJCRI783Dhw8L9D99+hS9e/fGgwcPsHDhQvz444+wt7fHsmXLEB4eXub1AkBubi7s7OygUCiwZ88erdRQXvzyyy+YPn36G21j1qxZ2L59e4nUQ285QVRORUZGCgMDA2Fubi7GjBkjVq1aJZYuXSr69esn9PT0xPDhw6Wx9vb2wtfXV3vFFiIsLEwAEMHBweLHH38Ua9euFbNmzRKenp5CoVAIe3t7cf78ebXXPH36VDx58kSj/Zw+fVoAEGFhYRq9Ljs7W2RnZ0vrBw8eFADE5s2bNdrO69aWk5MjsrKySmxfr+P//u//hI2NjVCpVGL16tUF+hMSEgSAAn1169YVbdu2LaMq1e3bt08AENWrVxc+Pj5aqaG88PPzE2/635iRkVGh/3Y8e/ZMPHnyROTl5b3R9untUUGLWYyoSImJiejXrx/s7e1x4MAB2NraSn1+fn64ceMGdu/ercUKi69z585o2rSptB4UFIQDBw6ga9eu6NatGxISEmBgYAAAqFChAipUKN2/lv/88w8MDQ2hVCpLdT//RU9PT6v7F0Jg/fr1GDBgABITExEREYFhw4apjUlNTQUAmJubl3o9z549Q15e3n++Lz/99BMaN24MX19f/N///R8yMzNhZGRU6vXJja6uLnR1dbVdBpUlbScyosKMGjVKABDHjh0r1viXZ4ju378vxo8fL+rVqyeMjIyEiYmJ6NSpk4iPjy/w2iVLlggXFxdpNqpJkyYiIiJC6s/IyBBjx44V9vb2QqlUisqVKwsPDw8RFxf3ypryZ4hOnz5daP+sWbMEALFq1Sqpbdq0aQV+4923b59wd3cXZmZmwsjISNSqVUsEBQUJIf6d1Xl5yZ+Radu2rahbt644c+aMaN26tTAwMBBjx46V+l6c5cjf1oYNG0RQUJCwtrYWhoaG4sMPPxRJSUmvPN/5Xtzmf9Xm6+sr7O3t1V7/+PFjERgYKKpUqSKUSqWoVauWmDdvXoHf0gEIPz8/sW3bNlG3bl2hVCqFi4uL2LNnT6HnujC//vqrACBOnTolNm7cKHR0dMSdO3ekfl9f3wK1t23bVtjb2xfani8tLU2MHTtWOgZHR0fx7bffitzcXGlMYmKiACDmzZsnFi5cKGrUqCF0dHTEuXPnXlnzP//8I0xMTMTcuXPFvXv3hI6OjtrPar6X39sXj+nlc/7333+LgQMHChMTE2FmZiYGDRok4uPjC8zs+fr6CiMjI3H79m3h7e0tjIyMhJ2dnVi6dKkQQogLFy6I9u3bC0NDQ1GtWrVC69L03KxcuVLUqFFDKJVK0bRpU3Hq1KlXvj8v/t2ZN2+ecHNzExYWFkJfX180bty4wOxnYa/P/7nO//ubmJio9prQ0FDh4uIilEqlsLW1FZ999plIS0srcP7r1q0rLl++LNq1aycMDAyEnZ2dmDNnToFzQuUHZ4ioXNq1axdq1KiBVq1avdbrb968ie3bt6N3795wcHBASkoKVq5cibZt2+LKlSuws7MDAKxevRpjxozBxx9/jLFjxyIrKwsXLlzAyZMnMWDAAADAqFGjsGXLFvj7+8PFxQX379/H0aNHkZCQgMaNG7/2MX7yySf4v//7P+zbtw/Dhw8vdMzly5fRtWtXNGjQAMHBwVCpVLhx4waOHTsGAHB2dkZwcDCmTp2KESNGoHXr1gCgdt7u37+Pzp07o1+/fhg4cCCsra1fWdc333wDhUKBSZMmITU1FYsWLYKHhwfi4+OlmaziKE5tLxJCoFu3bjh48CCGDh2Khg0bIioqChMmTMCff/6JhQsXqo0/evQotm7dis8++wwmJiZYsmQJevXqhaSkJFhaWv5nfREREXB0dESzZs1Qr149GBoa4ueff8aECRMAACNHjsR7772HWbNmYcyYMWjWrBmsra2RmZmJzz//HMbGxvjyyy8BQDqn//zzD9q2bYs///wTI0eORLVq1XD8+HEEBQXh3r17WLRokVoNYWFhyMrKwogRI6BSqWBhYfHKmnfu3InHjx+jX79+sLGxQbt27RARESH9rGoqLy8PH374IU6dOoXRo0ejTp062LFjB3x9fQsdn5ubi86dO6NNmzaYO3cuIiIi4O/vDyMjI3z55Zfw8fFBz549sWLFCgwaNAhubm5wcHB4rXOzfv16PHr0CCNHjoRCocDcuXPRs2dP3Lx5E3p6ehg5ciTu3r2L6Oho/PjjjwVqXbx4Mbp16wYfHx/k5ORgw4YN6N27NyIjI+Ht7Q0A+PHHHzFs2DA0b94cI0aMAAA4OjoWeb6mT5+OGTNmwMPDA6NHj8a1a9ewfPlynD59GseOHVOb9UxLS0OnTp3Qs2dP9OnTB1u2bMGkSZNQv359dO7cWaP3icqIthMZ0cvS09MFANG9e/div+blGYusrCy13zqFeP6bp0qlEsHBwVJb9+7dRd26dV+5bTMzM+Hn51fsWvL91wxR/rYbNWokrb88Q7Rw4UIBQPz1119FbuNV1+m0bdtWABArVqwotK+wGaL33ntPZGRkSO2bNm0SAMTixYultuLMEP1XbS/PVmzfvl0AEF9//bXauI8//lgoFApx48YNqQ2AUCqVam3nz58XAMR3331XYF8vy8nJEZaWluLLL7+U2gYMGCBcXV3VxhV1XVVR1xDNnDlTGBkZid9++02tffLkyUJXV1eaacufBTE1NRWpqan/WW++rl27Cnd3d2l91apVokKFCgW2UdwZov/9738CgFi0aJHUlpubKzp06FDoDBEAMWvWLKktLS1NGBgYCIVCITZs2CC1X716VQAQ06ZNk9o0PTeWlpbiwYMH0rgdO3YIAGLXrl1S26uuIfrnn3/U1nNyckS9evVEhw4d1NqLuobo5Rmi1NRUoVQqhaenp9q/LUuXLhUAxNq1a6W2/L93P/zwg9SWnZ0tbGxsRK9evQqtl7SPd5lRuZORkQEAMDExee1tqFQq6Og8//HOzc3F/fv3YWxsjNq1a+Ps2bPSOHNzc/zxxx84ffp0kdsyNzfHyZMncffu3deupyjGxsavvNss/9qVHTt2IC8v77X2oVKpMHjw4GKPHzRokNq5//jjj2Fra4tffvnltfZfXL/88gt0dXUxZswYtfbx48dDCFHgjioPDw+13+YbNGgAU1NT3Lx58z/3tWfPHty/fx/9+/eX2vr374/z58/j8uXLr30MmzdvRuvWrVGxYkX8/fff0uLh4YHc3FwcOXJEbXyvXr1QuXLlYm37/v37iIqKUqu5V69eUCgU2LRp02vVu3fvXujp6anNUOro6MDPz6/I17x4nZW5uTlq164NIyMj9OnTR2qvXbs2zM3N1d4LTc9N3759UbFiRWk9f4axOO8vALXZzLS0NKSnp6N169Zqf/81sX//fuTk5GDcuHHSvy0AMHz4cJiamha4ptHY2BgDBw6U1pVKJZo3b17s+qnsMRBRuWNqagoAb3Rbel5eHhYuXIiaNWtCpVKhUqVKqFy5Mi5cuID09HRp3KRJk2BsbIzmzZujZs2a8PPzkz6Oyjd37lxcunQJVatWRfPmzTF9+vQS+0ft8ePHrwx+ffv2hbu7O4YNGwZra2v069cPmzZt0igcvffeexpdQF2zZk21dYVCAScnp1J/Hsvt27dhZ2dX4Hw4OztL/S+qVq1agW1UrFgRaWlp/7mvn376CQ4ODtJHkDdu3ICjoyMMDQ0RERHx2sdw/fp17N27F5UrV1ZbPDw8APx7kXa+/I+TimPjxo14+vQpGjVqJNX84MEDtGjR4rVrvn37NmxtbWFoaKjW7uTkVOh4fX39AgHOzMwMVapUKfD8LDMzM7X3QtNz8/L7mx+OivP+AkBkZCRatmwJfX19WFhYoHLlyli+fLna339N5P/81a5dW61dqVSiRo0aBX4+Czsnxf35JO3gNURU7piamsLOzg6XLl167W3MmjULX331FYYMGYKZM2fCwsICOjo6GDdunFqYcHZ2xrVr1xAZGYm9e/fif//7H5YtW4apU6dixowZAIA+ffqgdevW2LZtG/bt24d58+Zhzpw52Lp16xtdC/DHH38gPT29yP98gOe/5R45cgQHDx7E7t27sXfvXmzcuBEdOnTAvn37inUXjCbX/RRXUQ+PzM3NLbM7c4rajxDila/LyMjArl27kJWVVSD8Ac+vXcm/jkpTeXl5+OCDDzBx4sRC+2vVqqW2rsl7kx963N3dC+2/efMmatSoAeD5+1PYecjNzS32/gpT1Dkvznuh6bl53fcXAH799Vd069YNbdq0wbJly2Braws9PT2EhYVh/fr1//n6kvAm9ZN2MBBRudS1a1esWrUKsbGxcHNz0/j1W7ZsQfv27bFmzRq19ocPH6JSpUpqbUZGRujbty/69u2LnJwc9OzZE9988w2CgoKgr68PALC1tcVnn32Gzz77DKmpqWjcuDG++eabNwpE+ReCenl5vXKcjo4OOnbsiI4dO2LBggWYNWsWvvzySxw8eBAeHh4l/mTr69evq60LIXDjxg00aNBAaqtYsWKhDzK8ffu29J8yUHRwKoy9vT3279+PR48eqc0SXb16VeovCVu3bkVWVhaWL19e4Gfh2rVrmDJlCo4dO4b333+/yG0UdVyOjo54/PixNOtRUhITE3H8+HH4+/ujbdu2an15eXn45JNPsH79ekyZMgXA8/ensFnMl2cx7O3tcfDgQelRDPlu3LhRovUDpXNuinof/ve//0FfXx9RUVFQqVRSe1hYWLG38bL8n79r166p/Yzn5OQgMTGxxN9zKnv8yIzKpYkTJ8LIyAjDhg1DSkpKgf7ff/8dixcvLvL1urq6BX4T27x5M/7880+1tvv376utK5VKuLi4QAiBp0+fIjc3t8AUu5WVFezs7JCdna3pYUkOHDiAmTNnwsHBAT4+PkWOe/DgQYG2hg0bAoC0//xn0BQWUF7HDz/8oPZx5ZYtW3Dv3j218Ofo6IgTJ04gJydHaouMjMSdO3fUtqVJbV26dEFubi6WLl2q1r5w4UIoFIoSuzPnp59+Qo0aNTBq1Ch8/PHHassXX3wBY2Pj//wIysjIqNBj6tOnD2JjYxEVFVWg7+HDh3j27Nlr1Zxfz8SJEwvU3KdPH7Rt21atZkdHR1y9ehV//fWX1Hb+/PkCHwd7eXnh6dOnWL16tdSWl5eH0NDQ16rzVUrj3BT186WrqwuFQqE2I3br1q1Cn0hd1Hv5Mg8PDyiVSixZskTt35Y1a9YgPT1dunON3l6cIaJyydHREevXr0ffvn3h7OyMQYMGoV69esjJycHx48exefPmV353WdeuXREcHIzBgwejVatWuHjxIiIiItR+swMAT09P2NjYwN3dHdbW1khISMDSpUvh7e0NExMTPHz4EFWqVMHHH38MV1dXGBsbY//+/Th9+jRCQkKKdSx79uzB1atX8ezZM6SkpODAgQOIjo6Gvb09du7cKc1CFSY4OBhHjhyBt7c37O3tkZqaimXLlqFKlSrSDIajoyPMzc2xYsUKmJiYwMjICC1atNDo+pQXWVhY4P3338fgwYORkpKCRYsWwcnJSe3C22HDhmHLli3o1KkT+vTpg99//x0//fRTgVuWNantww8/RPv27fHll1/i1q1bcHV1xb59+7Bjxw6MGzfulbdDF9fdu3dx8ODBAhdu51OpVPDy8sLmzZuxZMmSIrfTpEkTLF++HF9//TWcnJxgZWWFDh06YMKECdi5cye6du2KTz/9FE2aNEFmZiYuXryILVu24NatWwVmpYojIiICDRs2RNWqVQvt79atGz7//HOcPXsWjRs3xpAhQ7BgwQJ4eXlh6NChSE1NxYoVK1C3bl3ppgUA6NGjB5o3b47x48fjxo0bqFOnDnbu3CkF8ZKcfSyNc9OkSRMAwJgxY+Dl5QVdXV3069cP3t7eWLBgATp16oQBAwYgNTUVoaGhcHJywoULFwpsY//+/ViwYAHs7Ozg4OCAFi1aFNhX5cqVERQUhBkzZqBTp07o1q0brl27hmXLlqFZs2ZqF1DTW0pbt7cRFcdvv/0mhg8fLqpXry6USqUwMTER7u7u4rvvvlP72ofCbrsfP368sLW1FQYGBsLd3V3ExsYWuB155cqVok2bNsLS0lKoVCrh6OgoJkyYINLT04UQz2+VnTBhgnB1dRUmJibCyMhIuLq6imXLlv1n7fm37eYvSqVS2NjYiA8++EAsXrxY7db2fC/fdh8TEyO6d+8u7OzshFKpFHZ2dqJ///4Fbl3esWOHcHFxERUqVCj0wYyFKeq2+59//lkEBQUJKysrYWBgILy9vcXt27cLvD4kJES89957QqVSCXd3d3HmzJlCb/cuqrbCHhL46NEjERAQIOzs7ISenp6oWbPmKx/M+LL/+gqXkJAQAUDExMQUOSY8PFwAEDt27Cjytvvk5GTh7e0tTExMCjyY8dGjRyIoKEg4OTkJpVIpKlWqJFq1aiXmz58vcnJyhBDqDx/8L3FxcQKA+Oqrr4occ+vWLQFABAQESG0//fST9FDDhg0biqioqELP+V9//SUGDBggPZjx008/FceOHZMe0pkv/8GMLyvqZ8ze3l54e3urtb3pucFLt/I/e/ZMfP7556Jy5cpCoVCo/d1Zs2aNqFmzplCpVKJOnToiLCys0AefXr16VbRp00YYGBgU68GMS5cuFXXq1BF6enrC2tpajB49usgHM76ssPNP5YdCCF7hRURE/9q+fTs++ugjHD16tMiLuIneNQxEREQy9uTJE7W73XJzc+Hp6YkzZ84gOTm5VO5SJCqPeA0REZGMff7553jy5Anc3NyQnZ2NrVu34vjx45g1axbDEMkKZ4iIiGRs/fr1CAkJwY0bN5CVlQUnJyeMHj0a/v7+2i6NqEwxEBEREZHs8TlEREREJHsMRERERCR7vKi6GPLy8nD37l2YmJiU+NckEBERUekQQuDRo0ews7ODjs6r54AYiIrh7t27RT4hloiIiMq3O3fuoEqVKq8cw0BUDPlfNHnnzh2YmppquRoiIiIqjoyMDFStWlXtC6OLwkBUDPkfk5mamjIQERERvWWKc7kLL6omIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZq6DtAgioPnm3tksocbe+9db4NTwP/+K5eI7n4Tmeh+fexfMA8Fzke91/L0sKZ4iIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPa0GoiqV68OhUJRYPHz8wMAZGVlwc/PD5aWljA2NkavXr2QkpKito2kpCR4e3vD0NAQVlZWmDBhAp49e6Y25tChQ2jcuDFUKhWcnJwQHh5eVodIREREbwGtBqLTp0/j3r170hIdHQ0A6N27NwAgICAAu3btwubNm3H48GHcvXsXPXv2lF6fm5sLb29v5OTk4Pjx41i3bh3Cw8MxdepUaUxiYiK8vb3Rvn17xMfHY9y4cRg2bBiioqLK9mCJiIio3NLqc4gqV66stv7tt9/C0dERbdu2RXp6OtasWYP169ejQ4cOAICwsDA4OzvjxIkTaNmyJfbt24crV65g//79sLa2RsOGDTFz5kxMmjQJ06dPh1KpxIoVK+Dg4ICQkBAAgLOzM44ePYqFCxfCy8urzI+ZiIiIyp9ycw1RTk4OfvrpJwwZMgQKhQJxcXF4+vQpPDw8pDF16tRBtWrVEBsbCwCIjY1F/fr1YW1tLY3x8vJCRkYGLl++LI15cRv5Y/K3UZjs7GxkZGSoLURERPTuKjeBaPv27Xj48CE+/fRTAEBycjKUSiXMzc3VxllbWyM5OVka82IYyu/P73vVmIyMDDx58qTQWmbPng0zMzNpqVq16pseHhEREZVj5SYQrVmzBp07d4adnZ22S0FQUBDS09Ol5c6dO9ouiYiIiEpRufgus9u3b2P//v3YunWr1GZjY4OcnBw8fPhQbZYoJSUFNjY20phTp06pbSv/LrQXx7x8Z1pKSgpMTU1hYGBQaD0qlQoqleqNj4uIiIjeDuVihigsLAxWVlbw9v73i92aNGkCPT09xMTESG3Xrl1DUlIS3NzcAABubm64ePEiUlNTpTHR0dEwNTWFi4uLNObFbeSPyd8GERERkdYDUV5eHsLCwuDr64sKFf6dsDIzM8PQoUMRGBiIgwcPIi4uDoMHD4abmxtatmwJAPD09ISLiws++eQTnD9/HlFRUZgyZQr8/PykGZ5Ro0bh5s2bmDhxIq5evYply5Zh06ZNCAgI0MrxEhERUfmj9Y/M9u/fj6SkJAwZMqRA38KFC6Gjo4NevXohOzsbXl5eWLZsmdSvq6uLyMhIjB49Gm5ubjAyMoKvry+Cg4OlMQ4ODti9ezcCAgKwePFiVKlSBd9//z1vuSciIiKJ1gORp6cnhBCF9unr6yM0NBShoaFFvt7e3h6//PLLK/fRrl07nDt37o3qJCIioneX1j8yIyIiItI2BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPa0Hoj///BMDBw6EpaUlDAwMUL9+fZw5c0bqF0Jg6tSpsLW1hYGBATw8PHD9+nW1bTx48AA+Pj4wNTWFubk5hg4disePH6uNuXDhAlq3bg19fX1UrVoVc+fOLZPjIyIiovJPq4EoLS0N7u7u0NPTw549e3DlyhWEhISgYsWK0pi5c+diyZIlWLFiBU6ePAkjIyN4eXkhKytLGuPj44PLly8jOjoakZGROHLkCEaMGCH1Z2RkwNPTE/b29oiLi8O8efMwffp0rFq1qkyPl4iIiMqnCtrc+Zw5c1C1alWEhYVJbQ4ODtKfhRBYtGgRpkyZgu7duwMAfvjhB1hbW2P79u3o168fEhISsHfvXpw+fRpNmzYFAHz33Xfo0qUL5s+fDzs7O0RERCAnJwdr166FUqlE3bp1ER8fjwULFqgFJyIiIpInrc4Q7dy5E02bNkXv3r1hZWWFRo0aYfXq1VJ/YmIikpOT4eHhIbWZmZmhRYsWiI2NBQDExsbC3NxcCkMA4OHhAR0dHZw8eVIa06ZNGyiVSmmMl5cXrl27hrS0tAJ1ZWdnIyMjQ20hIiKid5dWA9HNmzexfPly1KxZE1FRURg9ejTGjBmDdevWAQCSk5MBANbW1mqvs7a2lvqSk5NhZWWl1l+hQgVYWFiojSlsGy/u40WzZ8+GmZmZtFStWrUEjpaIiIjKK60Gory8PDRu3BizZs1Co0aNMGLECAwfPhwrVqzQZlkICgpCenq6tNy5c0er9RAREVHp0mogsrW1hYuLi1qbs7MzkpKSAAA2NjYAgJSUFLUxKSkpUp+NjQ1SU1PV+p89e4YHDx6ojSlsGy/u40UqlQqmpqZqCxEREb27tBqI3N3dce3aNbW23377Dfb29gCeX2BtY2ODmJgYqT8jIwMnT56Em5sbAMDNzQ0PHz5EXFycNObAgQPIy8tDixYtpDFHjhzB06dPpTHR0dGoXbu22h1tREREJE9aDUQBAQE4ceIEZs2ahRs3bmD9+vVYtWoV/Pz8AAAKhQLjxo3D119/jZ07d+LixYsYNGgQ7Ozs0KNHDwDPZ5Q6deqE4cOH49SpUzh27Bj8/f3Rr18/2NnZAQAGDBgApVKJoUOH4vLly9i4cSMWL16MwMBAbR06ERERlSNave2+WbNm2LZtG4KCghAcHAwHBwcsWrQIPj4+0piJEyciMzMTI0aMwMOHD/H+++9j79690NfXl8ZERETA398fHTt2hI6ODnr16oUlS5ZI/WZmZti3bx/8/PzQpEkTVKpUCVOnTuUt90RERARAy4EIALp27YquXbsW2a9QKBAcHIzg4OAix1hYWGD9+vWv3E+DBg3w66+/vnadRERE9O7S+ld3EBEREWkbAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREcmexoHozp07+OOPP6T1U6dOYdy4cVi1alWJFkZERERUVjQORAMGDMDBgwcBAMnJyfjggw9w6tQpfPnllwgODi7xAomIiIhKm8aB6NKlS2jevDkAYNOmTahXrx6OHz+OiIgIhIeHl3R9RERERKVO40D09OlTqFQqAMD+/fvRrVs3AECdOnVw7949jbY1ffp0KBQKtaVOnTpSf1ZWFvz8/GBpaQljY2P06tULKSkpattISkqCt7c3DA0NYWVlhQkTJuDZs2dqYw4dOoTGjRtDpVLBycmJwY2IiIjUaByI6tatixUrVuDXX39FdHQ0OnXqBAC4e/cuLC0tNS6gbt26uHfvnrQcPXpU6gsICMCuXbuwefNmHD58GHfv3kXPnj2l/tzcXHh7eyMnJwfHjx/HunXrEB4ejqlTp0pjEhMT4e3tjfbt2yM+Ph7jxo3DsGHDEBUVpXGtRERE9G6qoOkL5syZg48++gjz5s2Dr68vXF1dAQA7d+6UPkrTqIAKFWBjY1OgPT09HWvWrMH69evRoUMHAEBYWBicnZ1x4sQJtGzZEvv27cOVK1ewf/9+WFtbo2HDhpg5cyYmTZqE6dOnQ6lUYsWKFXBwcEBISAgAwNnZGUePHsXChQvh5eWlcb1ERET07tF4hqhdu3b4+++/8ffff2Pt2rVS+4gRI7BixQqNC7h+/Trs7OxQo0YN+Pj4ICkpCQAQFxeHp0+fwsPDQxpbp04dVKtWDbGxsQCA2NhY1K9fH9bW1tIYLy8vZGRk4PLly9KYF7eRPyZ/G0RERESv9RwiIQTi4uKwcuVKPHr0CACgVCphaGio0XZatGiB8PBw7N27F8uXL0diYiJat26NR48eITk5GUqlEubm5mqvsba2RnJyMoDnd7m9GIby+/P7XjUmIyMDT548KbSu7OxsZGRkqC1ERET07tL4I7Pbt2+jU6dOSEpKQnZ2Nj744AOYmJhgzpw5yM7O1miWqHPnztKfGzRogBYtWsDe3h6bNm2CgYGBpqWVmNmzZ2PGjBla2z8RERGVLY1niMaOHYumTZsiLS1NLbR89NFHiImJeaNizM3NUatWLdy4cQM2NjbIycnBw4cP1cakpKRI1xzZ2NgUuOssf/2/xpiamhYZuoKCgpCeni4td+7ceaPjIiIiovJN40D066+/YsqUKVAqlWrt1atXx59//vlGxTx+/Bi///47bG1t0aRJE+jp6amFrGvXriEpKQlubm4AADc3N1y8eBGpqanSmOjoaJiamsLFxUUa83JQi46OlrZRGJVKBVNTU7WFiIiI3l0aB6K8vDzk5uYWaP/jjz9gYmKi0ba++OILHD58GLdu3cLx48fx0UcfQVdXF/3794eZmRmGDh2KwMBAHDx4EHFxcRg8eDDc3NzQsmVLAICnpydcXFzwySef4Pz584iKisKUKVPg5+cnPStp1KhRuHnzJiZOnIirV69i2bJl2LRpEwICAjQ9dCIiInpHaRyIPD09sWjRImldoVDg8ePHmDZtGrp06aLRtv744w/0798ftWvXRp8+fWBpaYkTJ06gcuXKAICFCxeia9eu6NWrF9q0aQMbGxts3bpVer2uri4iIyOhq6sLNzc3DBw4EIMGDVL7ChEHBwfs3r0b0dHRcHV1RUhICL7//nveck9EREQSjS+qDgkJgZeXF1xcXJCVlYUBAwbg+vXrqFSpEn7++WeNtrVhw4ZX9uvr6yM0NBShoaFFjrG3t8cvv/zyyu20a9cO586d06g2IiIikg+NA1GVKlVw/vx5bNiwARcuXMDjx48xdOhQ+Pj4aPXOMCIiIqLXpXEgAp4/XXrgwIElXQsRERGRVhQrEO3cubPYG8z/slciIiKit0WxAlGPHj2KtTGFQlHoHWhERERE5VmxAlFeXl5p10FERESkNa/1XWZERERE75LXCkQxMTHo2rUrHB0d4ejoiK5du2L//v0lXRsRERFRmdA4EC1btgydOnWCiYkJxo4di7Fjx8LU1BRdunR55fOCiIiIiMorjW+7nzVrFhYuXAh/f3+pbcyYMXB3d8esWbPg5+dXogUSERERlTaNZ4gePnyITp06FWj39PREenp6iRRFREREVJY0DkTdunXDtm3bCrTv2LEDXbt2LZGiiIiIiMqSxh+Zubi44JtvvsGhQ4fg5uYGADhx4gSOHTuG8ePHY8mSJdLYMWPGlFylRERERKVE40C0Zs0aVKxYEVeuXMGVK1ekdnNzc6xZs0ZaVygUDERERET0VtA4ECUmJpZGHURERERawwczEhERkexpPEMkhMCWLVtw8OBBpKamFvhaj61bt5ZYcURERERlQeNANG7cOKxcuRLt27eHtbU1FApFadRFREREVGY0DkQ//vgjtm7dii5dupRGPURERERlTuNriMzMzFCjRo3SqIWIiIhIKzQORNOnT8eMGTPw5MmT0qiHiIiIqMxp/JFZnz598PPPP8PKygrVq1eHnp6eWv/Zs2dLrDgiIiKisqBxIPL19UVcXBwGDhzIi6qJiIjonaBxINq9ezeioqLw/vvvl0Y9RERERGVO42uIqlatClNT09KohYiIiEgrNA5EISEhmDhxIm7dulUK5RARERGVPY0/Mhs4cCD++ecfODo6wtDQsMBF1Q8ePCix4oiIiIjKgsaBaNGiRaVQBhEREZH2vNZdZkRERETvEo0D0YuysrKQk5Oj1sYLromIiOhto/FF1ZmZmfD394eVlRWMjIxQsWJFtYWIiIjobaNxIJo4cSIOHDiA5cuXQ6VS4fvvv8eMGTNgZ2eHH374oTRqJCIiIipVGn9ktmvXLvzwww9o164dBg8ejNatW8PJyQn29vaIiIiAj49PadRJREREVGo0niF68OCB9G33pqam0m3277//Po4cOVKy1RERERGVAY0DUY0aNZCYmAgAqFOnDjZt2gTg+cyRubl5iRZHREREVBY0DkSDBw/G+fPnAQCTJ09GaGgo9PX1ERAQgAkTJpR4gURERESlTeNriAICAqQ/e3h4ICEhAWfPnoWTkxMaNGhQosURERERlYU3eg4RAFSvXh3Vq1cvgVKIiIiItKPYH5nFxsYiMjJSre2HH36Ag4MDrKysMGLECGRnZ5d4gURERESlrdiBKDg4GJcvX5bWL168iKFDh8LDwwOTJ0/Grl27MHv27Ncu5Ntvv4VCocC4ceOktqysLPj5+cHS0hLGxsbo1asXUlJS1F6XlJQEb29vGBoawsrKChMmTMCzZ8/Uxhw6dAiNGzeGSqWCk5MTwsPDX7tOIiIievcUOxDFx8ejY8eO0vqGDRvQokULrF69GoGBgViyZIl0x5mmTp8+jZUrVxa4BikgIAC7du3C5s2bcfjwYdy9exc9e/aU+nNzc+Ht7Y2cnBwcP34c69atQ3h4OKZOnSqNSUxMhLe3N9q3b4/4+HiMGzcOw4YNQ1RU1GvVSkRERO+eYgeitLQ0WFtbS+uHDx9G586dpfVmzZrhzp07Ghfw+PFj+Pj4YPXq1Wpf/ZGeno41a9ZgwYIF6NChA5o0aYKwsDAcP34cJ06cAADs27cPV65cwU8//YSGDRuic+fOmDlzJkJDQ6XvWFuxYgUcHBwQEhICZ2dn+Pv74+OPP8bChQs1rpWIiIjeTcUORNbW1tLzh3JycnD27Fm0bNlS6n/06BH09PQ0LsDPzw/e3t7w8PBQa4+Li8PTp0/V2uvUqYNq1aohNjYWwPPrmurXr68W1Ly8vJCRkSF9vBcbG1tg215eXtI2iIiIiIp9l1mXLl0wefJkzJkzB9u3b4ehoSFat24t9V+4cAGOjo4a7XzDhg04e/YsTp8+XaAvOTkZSqWywMMera2tkZycLI15MQzl9+f3vWpMRkYGnjx5AgMDgwL7zs7OVrtAPCMjQ6PjIiIiordLsWeIZs6ciQoVKqBt27ZYvXo1Vq9eDaVSKfWvXbsWnp6exd7xnTt3MHbsWEREREBfX1+zqkvZ7NmzYWZmJi1Vq1bVdklERERUioo9Q1SpUiUcOXIE6enpMDY2hq6urlr/5s2bYWxsXOwdx8XFITU1FY0bN5bacnNzceTIESxduhRRUVHIycnBw4cP1WaJUlJSYGNjAwCwsbHBqVOn1Labfxfai2NevjMtJSUFpqamhc4OAUBQUBACAwOl9YyMDIYiIiKid5jGX91hZmZWIAwBgIWFhdqM0X/p2LEjLl68iPj4eGlp2rQpfHx8pD/r6ekhJiZGes21a9eQlJQENzc3AICbmxsuXryI1NRUaUx0dDRMTU3h4uIijXlxG/lj8rdRGJVKBVNTU7WFiIiI3l1v/KTq12ViYoJ69eqptRkZGcHS0lJqHzp0KAIDA2FhYQFTU1N8/vnncHNzky7m9vT0hIuLCz755BPMnTsXycnJmDJlCvz8/KBSqQAAo0aNwtKlSzFx4kQMGTIEBw4cwKZNm7B79+6yPWAiIiIqt7QWiIpj4cKF0NHRQa9evZCdnQ0vLy8sW7ZM6tfV1UVkZCRGjx4NNzc3GBkZwdfXF8HBwdIYBwcH7N69GwEBAVi8eDGqVKmC77//Hl5eXto4JCIiIiqHylUgOnTokNq6vr4+QkNDERoaWuRr7O3t8csvv7xyu+3atcO5c+dKokQiIiJ6BxXrGqLGjRsjLS0NwPOv8Pjnn39KtSgiIiKislSsQJSQkIDMzEwAwIwZM/D48eNSLYqIiIioLBXrI7OGDRti8ODBeP/99yGEwPz584u8xf7F7xEjIiIiehsUKxCFh4dj2rRpiIyMhEKhwJ49e1ChQsGXKhQKBiIiIiJ66xQrENWuXRsbNmwAAOjo6CAmJgZWVlalWhgRERFRWdH4LrO8vLzSqIOIiIhIa17rtvvff/8dixYtQkJCAgDAxcUFY8eO1fjLXYmIiIjKA42/uiMqKgouLi44deoUGjRogAYNGuDkyZOoW7cuoqOjS6NGIiIiolKl8QzR5MmTERAQgG+//bZA+6RJk/DBBx+UWHFEREREZUHjGaKEhAQMHTq0QPuQIUNw5cqVEimKiIiIqCxpHIgqV66M+Pj4Au3x8fG884yIiIjeShp/ZDZ8+HCMGDECN2/eRKtWrQAAx44dw5w5cxAYGFjiBRIRERGVNo0D0VdffQUTExOEhIQgKCgIAGBnZ4fp06djzJgxJV4gERERUWnTOBApFAoEBAQgICAAjx49AgCYmJiUeGFEREREZeW1nkOUj0GIiIiI3gUaX1RNRERE9K5hICIiIiLZYyAiIiIi2dMoED19+hQdO3bE9evXS6seIiIiojKnUSDS09PDhQsXSqsWIiIiIq3Q+COzgQMHYs2aNaVRCxEREZFWaHzb/bNnz7B27Vrs378fTZo0gZGRkVr/ggULSqw4IiIiorKgcSC6dOkSGjduDAD47bff1PoUCkXJVEVERERUhjQORAcPHiyNOoiIiIi05rVvu79x4waioqLw5MkTAIAQosSKIiIiIipLGgei+/fvo2PHjqhVqxa6dOmCe/fuAQCGDh2K8ePHl3iBRERERKVN40AUEBAAPT09JCUlwdDQUGrv27cv9u7dW6LFEREREZUFja8h2rdvH6KiolClShW19po1a+L27dslVhgRERFRWdF4higzM1NtZijfgwcPoFKpSqQoIiIiorKkcSBq3bo1fvjhB2ldoVAgLy8Pc+fORfv27Uu0OCIiIqKyoPFHZnPnzkXHjh1x5swZ5OTkYOLEibh8+TIePHiAY8eOlUaNRERERKVK4xmievXq4bfffsP777+P7t27IzMzEz179sS5c+fg6OhYGjUSERERlSqNZ4gAwMzMDF9++WVJ10JERESkFa8ViNLS0rBmzRokJCQAAFxcXDB48GBYWFiUaHFEREREZUHjj8yOHDmC6tWrY8mSJUhLS0NaWhqWLFkCBwcHHDlypDRqJCIiIipVGs8Q+fn5oW/fvli+fDl0dXUBALm5ufjss8/g5+eHixcvlniRRERERKVJ4xmiGzduYPz48VIYAgBdXV0EBgbixo0bJVocERERUVnQOBA1btxYunboRQkJCXB1dS2RooiIiIjKUrEC0YULF6RlzJgxGDt2LObPn4+jR4/i6NGjmD9/PgICAhAQEKDRzpcvX44GDRrA1NQUpqamcHNzw549e6T+rKws+Pn5wdLSEsbGxujVqxdSUlLUtpGUlARvb28YGhrCysoKEyZMwLNnz9TGHDp0CI0bN4ZKpYKTkxPCw8M1qpOIiIjebcW6hqhhw4ZQKBQQQkhtEydOLDBuwIAB6Nu3b7F3XqVKFXz77beoWbMmhBBYt24dunfvjnPnzqFu3boICAjA7t27sXnzZpiZmcHf3x89e/aUHgCZm5sLb29v2NjY4Pjx47h37x4GDRoEPT09zJo1CwCQmJgIb29vjBo1ChEREYiJicGwYcNga2sLLy+vYtdKRERE765iBaLExMRS2fmHH36otv7NN99g+fLlOHHiBKpUqYI1a9Zg/fr16NChAwAgLCwMzs7OOHHiBFq2bIl9+/bhypUr2L9/P6ytrdGwYUPMnDkTkyZNwvTp06FUKrFixQo4ODggJCQEAODs7IyjR49i4cKFDEREREQEoJiByN7evrTrQG5uLjZv3ozMzEy4ubkhLi4OT58+hYeHhzSmTp06qFatGmJjY9GyZUvExsaifv36sLa2lsZ4eXlh9OjRuHz5Mho1aoTY2Fi1beSPGTduXJG1ZGdnIzs7W1rPyMgouQMlIiKicue1Hsx49+5dHD16FKmpqcjLy1PrGzNmjEbbunjxItzc3JCVlQVjY2Ns27YNLi4uiI+Ph1KphLm5udp4a2trJCcnAwCSk5PVwlB+f37fq8ZkZGTgyZMnMDAwKFDT7NmzMWPGDI2Og4iIiN5eGgei8PBwjBw5EkqlEpaWllAoFFKfQqHQOBDVrl0b8fHxSE9Px5YtW+Dr64vDhw9rWlaJCgoKQmBgoLSekZGBqlWrarEiIiIiKk0aB6KvvvoKU6dORVBQEHR0NL5rvwClUgknJycAQJMmTXD69GksXrwYffv2RU5ODh4+fKg2S5SSkgIbGxsAgI2NDU6dOqW2vfy70F4c8/KdaSkpKTA1NS10dggAVCoVVCrVGx8bERERvR00TjT//PMP+vXrVyJhqDB5eXnIzs5GkyZNoKenh5iYGKnv2rVrSEpKgpubGwDAzc0NFy9eRGpqqjQmOjoapqamcHFxkca8uI38MfnbICIiItI41QwdOhSbN28ukZ0HBQXhyJEjuHXrFi5evIigoCAcOnQIPj4+MDMzw9ChQxEYGIiDBw8iLi4OgwcPhpubG1q2bAkA8PT0hIuLCz755BOcP38eUVFRmDJlCvz8/KQZnlGjRuHmzZuYOHEirl69imXLlmHTpk0aPzOJiIiI3l0af2Q2e/ZsdO3aFXv37kX9+vWhp6en1r9gwYJibys1NRWDBg3CvXv3YGZmhgYNGiAqKgoffPABAGDhwoXQ0dFBr169kJ2dDS8vLyxbtkx6va6uLiIjIzF69Gi4ubnByMgIvr6+CA4OlsY4ODhg9+7dCAgIwOLFi1GlShV8//33vOWeiIiIJK8ViKKiolC7dm0AKHBRtSbWrFnzyn59fX2EhoYiNDS0yDH29vb45ZdfXrmddu3a4dy5cxrVRkRERPKhcSAKCQnB2rVr8emnn5ZCOURERERlT+NriFQqFdzd3UujFiIiIiKt0DgQjR07Ft99911p1EJERESkFRp/ZHbq1CkcOHAAkZGRqFu3boGLqrdu3VpixRERERGVBY0Dkbm5OXr27FkatRARERFphcaBKCwsrDTqICIiItKa0nncNBEREdFbROMZIgcHh1c+b+jmzZtvVBARERFRWdM4EI0bN05t/enTpzh37hz27t2LCRMmlFRdRERERGVG40A0duzYQttDQ0Nx5syZNy6IiIiIqKyV2DVEnTt3xv/+97+S2hwRERFRmSmxQLRlyxZYWFiU1OaIiIiIyozGH5k1atRI7aJqIQSSk5Px119/qX0TPREREdHbQuNA1KNHD7V1HR0dVK5cGe3atUOdOnVKqi4iIiKiMqNxIJo2bVpp1EFERESkNXwwIxEREclesWeIdHR0XvlARgBQKBR49uzZGxdFREREVJaKHYi2bdtWZF9sbCyWLFmCvLy8EimKiIiIqCwVOxB17969QNu1a9cwefJk7Nq1Cz4+PggODi7R4oiIiIjKwmtdQ3T37l0MHz4c9evXx7NnzxAfH49169bB3t6+pOsjIiIiKnUaBaL09HRMmjQJTk5OuHz5MmJiYrBr1y7Uq1evtOojIiIiKnXF/shs7ty5mDNnDmxsbPDzzz8X+hEaERER0duo2IFo8uTJMDAwgJOTE9atW4d169YVOm7r1q0lVhwRERFRWSh2IBo0aNB/3nZPRERE9DYqdiAKDw8vxTKIiIiItIdPqiYiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZ02ogmj17Npo1awYTExNYWVmhR48euHbtmtqYrKws+Pn5wdLSEsbGxujVqxdSUlLUxiQlJcHb2xuGhoawsrLChAkT8OzZM7Uxhw4dQuPGjaFSqeDk5ITw8PDSPjwiIiJ6S2g1EB0+fBh+fn44ceIEoqOj8fTpU3h6eiIzM1MaExAQgF27dmHz5s04fPgw7t69i549e0r9ubm58Pb2Rk5ODo4fP45169YhPDwcU6dOlcYkJibC29sb7du3R3x8PMaNG4dhw4YhKiqqTI+XiIiIyqcK2tz53r171dbDw8NhZWWFuLg4tGnTBunp6VizZg3Wr1+PDh06AADCwsLg7OyMEydOoGXLlti3bx+uXLmC/fv3w9raGg0bNsTMmTMxadIkTJ8+HUqlEitWrICDgwNCQkIAAM7Ozjh69CgWLlwILy+vMj9uIiIiKl/K1TVE6enpAAALCwsAQFxcHJ4+fQoPDw9pTJ06dVCtWjXExsYCAGJjY1G/fn1YW1tLY7y8vJCRkYHLly9LY17cRv6Y/G28LDs7GxkZGWoLERERvbvKTSDKy8vDuHHj4O7ujnr16gEAkpOToVQqYW5urjbW2toaycnJ0pgXw1B+f37fq8ZkZGTgyZMnBWqZPXs2zMzMpKVq1aolcoxERERUPpWbQOTn54dLly5hw4YN2i4FQUFBSE9Pl5Y7d+5ouyQiIiIqRVq9hiifv78/IiMjceTIEVSpUkVqt7GxQU5ODh4+fKg2S5SSkgIbGxtpzKlTp9S2l38X2otjXr4zLSUlBaampjAwMChQj0qlgkqlKpFjIyIiovJPqzNEQgj4+/tj27ZtOHDgABwcHNT6mzRpAj09PcTExEht165dQ1JSEtzc3AAAbm5uuHjxIlJTU6Ux0dHRMDU1hYuLizTmxW3kj8nfBhEREcmbVmeI/Pz8sH79euzYsQMmJibSNT9mZmYwMDCAmZkZhg4disDAQFhYWMDU1BSff/453Nzc0LJlSwCAp6cnXFxc8Mknn2Du3LlITk7GlClT4OfnJ83yjBo1CkuXLsXEiRMxZMgQHDhwAJs2bcLu3bu1duxERERUfmh1hmj58uVIT09Hu3btYGtrKy0bN26UxixcuBBdu3ZFr1690KZNG9jY2GDr1q1Sv66uLiIjI6Grqws3NzcMHDgQgwYNQnBwsDTGwcEBu3fvRnR0NFxdXRESEoLvv/+et9wTERERAC3PEAkh/nOMvr4+QkNDERoaWuQYe3t7/PLLL6/cTrt27XDu3DmNayQiIqJ3X7m5y4yIiIhIWxiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPa0GoiOHDmCDz/8EHZ2dlAoFNi+fbtavxACU6dOha2tLQwMDODh4YHr16+rjXnw4AF8fHxgamoKc3NzDB06FI8fP1Ybc+HCBbRu3Rr6+vqoWrUq5s6dW9qHRkRERG8RrQaizMxMuLq6IjQ0tND+uXPnYsmSJVixYgVOnjwJIyMjeHl5ISsrSxrj4+ODy5cvIzo6GpGRkThy5AhGjBgh9WdkZMDT0xP29vaIi4vDvHnzMH36dKxatarUj4+IiIjeDhW0ufPOnTujc+fOhfYJIbBo0SJMmTIF3bt3BwD88MMPsLa2xvbt29GvXz8kJCRg7969OH36NJo2bQoA+O6779ClSxfMnz8fdnZ2iIiIQE5ODtauXQulUom6desiPj4eCxYsUAtOREREJF/l9hqixMREJCcnw8PDQ2ozMzNDixYtEBsbCwCIjY2Fubm5FIYAwMPDAzo6Ojh58qQ0pk2bNlAqldIYLy8vXLt2DWlpaYXuOzs7GxkZGWoLERERvbvKbSBKTk4GAFhbW6u1W1tbS33JycmwsrJS669QoQIsLCzUxhS2jRf38bLZs2fDzMxMWqpWrfrmB0RERETlVrkNRNoUFBSE9PR0ablz5462SyIiIqJSVG4DkY2NDQAgJSVFrT0lJUXqs7GxQWpqqlr/s2fP8ODBA7UxhW3jxX28TKVSwdTUVG0hIiKid1e5DUQODg6wsbFBTEyM1JaRkYGTJ0/Czc0NAODm5oaHDx8iLi5OGnPgwAHk5eWhRYsW0pgjR47g6dOn0pjo6GjUrl0bFStWLKOjISIiovJMq4Ho8ePHiI+PR3x8PIDnF1LHx8cjKSkJCoUC48aNw9dff42dO3fi4sWLGDRoEOzs7NCjRw8AgLOzMzp16oThw4fj1KlTOHbsGPz9/dGvXz/Y2dkBAAYMGAClUomhQ4fi8uXL2LhxIxYvXozAwEAtHTURERGVN1q97f7MmTNo3769tJ4fUnx9fREeHo6JEyciMzMTI0aMwMOHD/H+++9j79690NfXl14TEREBf39/dOzYETo6OujVqxeWLFki9ZuZmWHfvn3w8/NDkyZNUKlSJUydOpW33BMREZFEq4GoXbt2EEIU2a9QKBAcHIzg4OAix1hYWGD9+vWv3E+DBg3w66+/vnadRERE9G4rt9cQEREREZUVBiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9WQWi0NBQVK9eHfr6+mjRogVOnTql7ZKIiIioHJBNINq4cSMCAwMxbdo0nD17Fq6urvDy8kJqaqq2SyMiIiItk00gWrBgAYYPH47BgwfDxcUFK1asgKGhIdauXavt0oiIiEjLZBGIcnJyEBcXBw8PD6lNR0cHHh4eiI2N1WJlREREVB5U0HYBZeHvv/9Gbm4urK2t1dqtra1x9erVAuOzs7ORnZ0traenpwMAMjIySqW+vOx/SmW72vQ654rn4V88F8/xPDzH8/Dcu3geAJ6LfKXxf2z+NoUQ/zlWFoFIU7Nnz8aMGTMKtFetWlUL1bydzBZpu4LygefhXzwXz/E8PMfz8C+ei+dK8zw8evQIZmZmrxwji0BUqVIl6OrqIiUlRa09JSUFNjY2BcYHBQUhMDBQWs/Ly8ODBw9gaWkJhUJR6vWWhoyMDFStWhV37tyBqamptsvRKp6L53genuN5+BfPxXM8D8+9C+dBCIFHjx7Bzs7uP8fKIhAplUo0adIEMTEx6NGjB4DnIScmJgb+/v4FxqtUKqhUKrU2c3PzMqi09Jmamr61P9gljefiOZ6H53ge/sVz8RzPw3Nv+3n4r5mhfLIIRAAQGBgIX19fNG3aFM2bN8eiRYuQmZmJwYMHa7s0IiIi0jLZBKK+ffvir7/+wtSpU5GcnIyGDRti7969BS60JiIiIvmRTSACAH9//0I/IpMDlUqFadOmFfgoUI54Lp7jeXiO5+FfPBfP8Tw8J7fzoBDFuReNiIiI6B0miwczEhEREb0KAxERERHJHgMRERERyR4DEREREckeA5FMhIaGonr16tDX10eLFi1w6tQpbZdU5o4cOYIPP/wQdnZ2UCgU2L59u7ZL0orZs2ejWbNmMDExgZWVFXr06IFr165pu6wyt3z5cjRo0EB66Jybmxv27Nmj7bK07ttvv4VCocC4ceO0XUqZmz59OhQKhdpSp04dbZelFX/++ScGDhwIS0tLGBgYoH79+jhz5oy2yypVDEQysHHjRgQGBmLatGk4e/YsXF1d4eXlhdTUVG2XVqYyMzPh6uqK0NBQbZeiVYcPH4afnx9OnDiB6OhoPH36FJ6ensjMzNR2aWWqSpUq+PbbbxEXF4czZ86gQ4cO6N69Oy5fvqzt0rTm9OnTWLlyJRo0aKDtUrSmbt26uHfvnrQcPXpU2yWVubS0NLi7u0NPTw979uzBlStXEBISgooVK2q7tFLF2+5loEWLFmjWrBmWLl0K4PnXllStWhWff/45Jk+erOXqtEOhUGDbtm3SV7nI2V9//QUrKyscPnwYbdq00XY5WmVhYYF58+Zh6NCh2i6lzD1+/BiNGzfGsmXL8PXXX6Nhw4ZYtGiRtssqU9OnT8f27dsRHx+v7VK0avLkyTh27Bh+/fVXbZdSpjhD9I7LyclBXFwcPDw8pDYdHR14eHggNjZWi5VReZGeng7geRiQq9zcXGzYsAGZmZlwc3PTdjla4efnB29vb7V/K+To+vXrsLOzQ40aNeDj44OkpCRtl1Tmdu7ciaZNm6J3796wsrJCo0aNsHr1am2XVeoYiN5xf//9N3Jzcwt8RYm1tTWSk5O1VBWVF3l5eRg3bhzc3d1Rr149bZdT5i5evAhjY2OoVCqMGjUK27Ztg4uLi7bLKnMbNmzA2bNnMXv2bG2XolUtWrRAeHg49u7di+XLlyMxMRGtW7fGo0ePtF1ambp58yaWL1+OmjVrIioqCqNHj8aYMWOwbt06bZdWqmT11R1EpM7Pzw+XLl2S5XUSAFC7dm3Ex8cjPT0dW7Zsga+vLw4fPiyrUHTnzh2MHTsW0dHR0NfX13Y5WtW5c2fpzw0aNECLFi1gb2+PTZs2yepj1Ly8PDRt2hSzZs0CADRq1AiXLl3CihUr4Ovrq+XqSg9niN5xlSpVgq6uLlJSUtTaU1JSYGNjo6WqqDzw9/dHZGQkDh48iCpVqmi7HK1QKpVwcnJCkyZNMHv2bLi6umLx4sXaLqtMxcXFITU1FY0bN0aFChVQoUIFHD58GEuWLEGFChWQm5ur7RK1xtzcHLVq1cKNGze0XUqZsrW1LfBLgbOz8zv/8SED0TtOqVSiSZMmiImJkdry8vIQExMj22sl5E4IAX9/f2zbtg0HDhyAg4ODtksqN/Ly8pCdna3tMspUx44dcfHiRcTHx0tL06ZN4ePjg/j4eOjq6mq7RK15/Pgxfv/9d9ja2mq7lDLl7u5e4FEcv/32G+zt7bVUUdngR2YyEBgYCF9fXzRt2hTNmzfHokWLkJmZicGDB2u7tDL1+PFjtd/0EhMTER8fDwsLC1SrVk2LlZUtPz8/rF+/Hjt27ICJiYl0LZmZmRkMDAy0XF3ZCQoKQufOnVGtWjU8evQI69evx6FDhxAVFaXt0sqUiYlJgevHjIyMYGlpKbvryr744gt8+OGHsLe3x927dzFt2jTo6uqif//+2i6tTAUEBKBVq1aYNWsW+vTpg1OnTmHVqlVYtWqVtksrXYJk4bvvvhPVqlUTSqVSNG/eXJw4cULbJZW5gwcPCgAFFl9fX22XVqYKOwcARFhYmLZLK1NDhgwR9vb2QqlUisqVK4uOHTuKffv2abuscqFt27Zi7Nix2i6jzPXt21fY2toKpVIp3nvvPdG3b19x48YNbZelFbt27RL16tUTKpVK1KlTR6xatUrbJZU6PoeIiIiIZI/XEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRAR0Ttn+vTpaNiwobbLAAAcOnQICoUCDx8+1HYpRPQKDERE9MY+/fRTKBSKAkunTp1Kfd8KhQLbt29Xa/viiy/Uvr+vNJ07dw69e/eGtbU19PX1UbNmTQwfPhy//fZbmeyfiEoGAxERlYhOnTrh3r17asvPP/+slVqMjY1haWlZ6vuJjIxEy5YtkZ2djYiICCQkJOCnn36CmZkZvvrqq1LfPxGVHAYiIioRKpUKNjY2akvFihWlfoVCgZUrV6Jr164wNDSEs7MzYmNjcePGDbRr1w5GRkZo1aoVfv/9d7XtLl++HI6OjlAqlahduzZ+/PFHqa969eoAgI8++ggKhUJaf/kjs7y8PAQHB6NKlSpQqVRo2LAh9u7dK/XfunULCoUCW7duRfv27WFoaAhXV1fExsYWebz//PMPBg8ejC5dumDnzp3w8PCAg4MDWrRogfnz52PlypWFvu7+/fvo378/3nvvPRgaGqJ+/foFguOWLVtQv359GBgYwNLSEh4eHsjMzATw/CO45s2bw8jICObm5nB3d8ft27eLfmOIqFgYiIiozMycORODBg1CfHw86tSpgwEDBmDkyJEICgrCmTNnIISAv7+/NH7btm0YO3Ysxo8fj0uXLmHkyJEYPHgwDh48CAA4ffo0ACAsLAz37t2T1l+2ePFihISEYP78+bhw4QK8vLzQrVs3XL9+XW3cl19+iS+++ALx8fGoVasW+vfvj2fPnhW6zaioKPz999+YOHFiof3m5uaFtmdlZaFJkybYvXs3Ll26hBEjRuCTTz7BqVOnAAD37t1D//79MWTIECQkJODQoUPo2bMnhBB49uwZevTogbZt2+LChQuIjY3FiBEjoFAoij7pRFQ82v1uWSJ6F/j6+gpdXV1hZGSktnzzzTfSGABiypQp0npsbKwAINasWSO1/fzzz0JfX19ab9WqlRg+fLjavnr37i26dOmitt1t27apjZk2bZpwdXWV1u3s7NRqEUKIZs2aic8++0wIIURiYqIAIL7//nup//LlywKASEhIKPSY58yZIwCIBw8eFHVahBBCHDx4UAAQaWlpRY7x9vYW48ePF0IIERcXJwCIW7duFRh3//59AUAcOnTolfskIs1xhoiISkT79u0RHx+vtowaNUptTIMGDaQ/W1tbAwDq16+v1paVlYWMjAwAQEJCAtzd3dW24e7ujoSEhGLXlZGRgbt37xZrOy/WZ2trCwBITU0tdLtCiGLX8KLc3FzMnDkT9evXh4WFBYyNjREVFYWkpCQAgKurKzp27Ij69eujd+/eWL16NdLS0gAAFhYW+PTTT+Hl5YUPP/wQixcvxr17916rDiJSx0BERCXCyMgITk5OaouFhYXaGD09PenP+R/zFNaWl5dXBhUXpEkttWrVAgBcvXpVo33MmzcPixcvxqRJk3Dw4EHEx8fDy8sLOTk5AABdXV1ER0djz549cHFxwXfffYfatWsjMTERwPOPB2NjY9GqVSts3LgRtWrVwokTJzQ+ViJSx0BEROWWs7Mzjh07ptZ27NgxuLi4SOt6enrIzc0tchumpqaws7P7z+1oytPTE5UqVcLcuXML7S/quUPHjh1D9+7dMXDgQLi6uqJGjRoFbtFXKBRwd3fHjBkzcO7cOSiVSmzbtk3qb9SoEYKCgnD8+HHUq1cP69evf+3jIKLnKmi7ACJ6N2RnZyM5OVmtrUKFCqhUqdJrb3PChAno06cPGjVqBA8PD+zatQtbt27F/v37pTHVq1dHTEwM3N3doVKp1O5se3E706ZNg6OjIxo2bIiwsDDEx8cjIiLitWszMjLC999/j969e6Nbt24YM2YMnJyc8Pfff2PTpk1ISkrChg0bCryuZs2a2LJlC44fP46KFStiwYIFSElJkcLZyZMnERMTA09PT1hZWeHkyZP466+/4OzsjMTERKxatQrdunWDnZ0drl27huvXr2PQoEGvfRxE9BwDERGViL1790rX3eSrXbu2xh8pvahHjx5YvHgx5s+fj7Fjx8LBwQFhYWFo166dNCYkJASBgYFYvXo13nvvPdy6davAdsaMGYP09HSMHz8eqampcHFxwc6dO1GzZs3Xrg0AunfvjuPHj2P27NkYMGAAMjIyULVqVXTo0AFff/11oa+ZMmUKbt68CS8vLxgaGmLEiBHo0aMH0tPTATyf0Tpy5AgWLVqEjIwM2NvbIyQkBJ07d0ZKSgquXr2KdevW4f79+7C1tYWfnx9Gjhz5RsdBRIBCvO6VgURERETvCF5DRERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREsvf/6rlKgtoElpkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images seen via train_loader: 50505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train!"
      ],
      "metadata": {
        "id": "MQvkFnUNxXls"
      },
      "id": "MQvkFnUNxXls"
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MifXx_iuXfLi",
        "outputId": "f8d9d0a8-09fc-4e4e-bf55-9ce3ad4b650d"
      },
      "id": "MifXx_iuXfLi",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_075040-q7h5ah99</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/q7h5ah99' target=\"_blank\">ComplexMiniGoogLeNet</a></strong> to <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/q7h5ah99' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/q7h5ah99</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "ComplexMiniGoogLeNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (inc1): MiniInception(\n",
            "    (b1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (b3_pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (b3_proj): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "  )\n",
            "  (inc2): MiniInception(\n",
            "    (b1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_2): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (b3_pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (b3_proj): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "  )\n",
            "  (inc3): MiniInception(\n",
            "    (b1): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_1): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (b3_pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (b3_proj): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "  )\n",
            "  (aux): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(4, 4))\n",
            "    (1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU()\n",
            "    (3): Flatten(start_dim=1, end_dim=-1)\n",
            "    (4): Linear(in_features=768, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "    (7): Linear(in_features=256, out_features=7, bias=True)\n",
            "  )\n",
            "  (final_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Dropout(p=0.5, inplace=False)\n",
            "    (2): Linear(in_features=128, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50: Train Loss=1.8966, Train Acc=0.2091 | Val Loss=1.8009, Val Acc=0.2538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50: Train Loss=1.7951, Train Acc=0.2661 | Val Loss=1.7765, Val Acc=0.2538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50: Train Loss=1.7337, Train Acc=0.3058 | Val Loss=1.7193, Val Acc=0.2895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50: Train Loss=1.6753, Train Acc=0.3368 | Val Loss=1.6472, Val Acc=0.3305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50: Train Loss=1.6443, Train Acc=0.3527 | Val Loss=1.6426, Val Acc=0.3154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50: Train Loss=1.6215, Train Acc=0.3629 | Val Loss=1.6165, Val Acc=0.3480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50: Train Loss=1.6042, Train Acc=0.3715 | Val Loss=1.6354, Val Acc=0.3293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50: Train Loss=1.5807, Train Acc=0.3841 | Val Loss=1.5507, Val Acc=0.3686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50: Train Loss=1.5683, Train Acc=0.3929 | Val Loss=1.5403, Val Acc=0.3842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50: Train Loss=1.5454, Train Acc=0.4022 | Val Loss=1.5200, Val Acc=0.3918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50: Train Loss=1.5335, Train Acc=0.4098 | Val Loss=1.5017, Val Acc=0.4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50: Train Loss=1.5218, Train Acc=0.4114 | Val Loss=1.4660, Val Acc=0.4310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50: Train Loss=1.5113, Train Acc=0.4174 | Val Loss=1.4676, Val Acc=0.4302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50: Train Loss=1.4998, Train Acc=0.4221 | Val Loss=1.4488, Val Acc=0.4363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50: Train Loss=1.4904, Train Acc=0.4263 | Val Loss=1.4380, Val Acc=0.4441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50: Train Loss=1.4788, Train Acc=0.4294 | Val Loss=1.4164, Val Acc=0.4570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50: Train Loss=1.4754, Train Acc=0.4313 | Val Loss=1.4327, Val Acc=0.4355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50: Train Loss=1.4640, Train Acc=0.4365 | Val Loss=1.4065, Val Acc=0.4617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50: Train Loss=1.4565, Train Acc=0.4399 | Val Loss=1.4537, Val Acc=0.4372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50: Train Loss=1.4488, Train Acc=0.4442 | Val Loss=1.3900, Val Acc=0.4745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50: Train Loss=1.4353, Train Acc=0.4480 | Val Loss=1.4472, Val Acc=0.4402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50: Train Loss=1.4336, Train Acc=0.4510 | Val Loss=1.3716, Val Acc=0.4773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50: Train Loss=1.4283, Train Acc=0.4535 | Val Loss=1.3737, Val Acc=0.4739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50: Train Loss=1.4218, Train Acc=0.4546 | Val Loss=1.3554, Val Acc=0.4862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50: Train Loss=1.4127, Train Acc=0.4563 | Val Loss=1.3464, Val Acc=0.4960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50: Train Loss=1.4036, Train Acc=0.4600 | Val Loss=1.3444, Val Acc=0.4868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50: Train Loss=1.4075, Train Acc=0.4598 | Val Loss=1.3465, Val Acc=0.4840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50: Train Loss=1.3983, Train Acc=0.4631 | Val Loss=1.3443, Val Acc=0.4870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50: Train Loss=1.3965, Train Acc=0.4662 | Val Loss=1.3438, Val Acc=0.4898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50: Train Loss=1.3879, Train Acc=0.4652 | Val Loss=1.3370, Val Acc=0.4937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50: Train Loss=1.3900, Train Acc=0.4650 | Val Loss=1.3270, Val Acc=0.4971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50: Train Loss=1.3835, Train Acc=0.4686 | Val Loss=1.3527, Val Acc=0.4751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50: Train Loss=1.3766, Train Acc=0.4700 | Val Loss=1.3304, Val Acc=0.4915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50: Train Loss=1.3722, Train Acc=0.4724 | Val Loss=1.3228, Val Acc=0.5007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50: Train Loss=1.3708, Train Acc=0.4727 | Val Loss=1.3006, Val Acc=0.5052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50: Train Loss=1.3678, Train Acc=0.4732 | Val Loss=1.3252, Val Acc=0.4904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50: Train Loss=1.3628, Train Acc=0.4780 | Val Loss=1.3123, Val Acc=0.4957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50: Train Loss=1.3579, Train Acc=0.4782 | Val Loss=1.3046, Val Acc=0.4976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 [Train]: 100%|██████████| 198/198 [00:16<00:00, 11.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50: Train Loss=1.3574, Train Acc=0.4789 | Val Loss=1.3033, Val Acc=0.5029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50: Train Loss=1.3507, Train Acc=0.4808 | Val Loss=1.3134, Val Acc=0.4935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50: Train Loss=1.3528, Train Acc=0.4801 | Val Loss=1.3170, Val Acc=0.4979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50: Train Loss=1.3449, Train Acc=0.4818 | Val Loss=1.2966, Val Acc=0.5007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50: Train Loss=1.3469, Train Acc=0.4817 | Val Loss=1.2941, Val Acc=0.4985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50: Train Loss=1.3448, Train Acc=0.4847 | Val Loss=1.2891, Val Acc=0.5127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50: Train Loss=1.3397, Train Acc=0.4835 | Val Loss=1.3129, Val Acc=0.5043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50: Train Loss=1.3355, Train Acc=0.4849 | Val Loss=1.2812, Val Acc=0.5102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 [Train]: 100%|██████████| 198/198 [00:17<00:00, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50: Train Loss=1.3358, Train Acc=0.4840 | Val Loss=1.2882, Val Acc=0.5026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50: Train Loss=1.3380, Train Acc=0.4868 | Val Loss=1.2854, Val Acc=0.5141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 [Train]: 100%|██████████| 198/198 [00:16<00:00, 12.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50: Train Loss=1.3287, Train Acc=0.4886 | Val Loss=1.2793, Val Acc=0.5060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50 [Train]: 100%|██████████| 198/198 [00:15<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50: Train Loss=1.3308, Train Acc=0.4862 | Val Loss=1.2747, Val Acc=0.4982\n",
            "\n",
            "Test Accuracy: 49.82%\n",
            "F1 Macro: 0.4278\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.11      0.17       467\n",
            "           1       0.53      0.29      0.37        56\n",
            "           2       0.27      0.10      0.15       496\n",
            "           3       0.58      0.85      0.69       895\n",
            "           4       0.36      0.52      0.43       653\n",
            "           5       0.76      0.64      0.70       415\n",
            "           6       0.47      0.50      0.49       607\n",
            "\n",
            "    accuracy                           0.50      3589\n",
            "   macro avg       0.48      0.43      0.43      3589\n",
            "weighted avg       0.48      0.50      0.46      3589\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_Angry</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▂▂▅▆▅▅▄▅▆▃▅▆▅▅▆▇▆▇▇██▇▆</td></tr><tr><td>f1_Disgust</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▅▆▆▆▆▆▆▆█▆▇▇▇▆▇█▇█▇▆█</td></tr><tr><td>f1_Fear</td><td>▁▁▁▃▂▄▄▄▄▅▅▇▄▇▇▅▄▆▆▇▇▇▆▇▇▆▆▆▆█▇▇▇█▅█▆▇█▇</td></tr><tr><td>f1_Happy</td><td>▁▁▁▂▂▂▃▄▅▅▆▆▇▇▇▇▅▇▇▇▇▇▇█▇▇▇▇█▇█▇▇▇▇██▇█▇</td></tr><tr><td>f1_Neutral</td><td>▁▁▁▁▁▁▂▂▃▄▆▆▆▆▆▆▇▆▇▇█▇▇█▇▇▇███▇███▇▇████</td></tr><tr><td>f1_Sad</td><td>▃▁▃▄▂▅▆▇▇▇▇▇▇█▇▇█▇▇█████████████████████</td></tr><tr><td>f1_Surprise</td><td>▁▁▅▇▇▆█▇▇███▇▇██▇████████▇██████████████</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1_macro</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▃▃▃▄▅▅▅▆▆▆▆▆▆▇▆▇▇█▇▇▇▇█▇▇███▇████████</td></tr><tr><td>val_loss</td><td>██▇▆▆▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>f1_Angry</td><td>0.17057</td></tr><tr><td>f1_Disgust</td><td>0.37209</td></tr><tr><td>f1_Fear</td><td>0.15138</td></tr><tr><td>f1_Happy</td><td>0.69006</td></tr><tr><td>f1_Neutral</td><td>0.48519</td></tr><tr><td>f1_Sad</td><td>0.42804</td></tr><tr><td>f1_Surprise</td><td>0.69713</td></tr><tr><td>test_accuracy</td><td>0.49819</td></tr><tr><td>test_f1_macro</td><td>0.42778</td></tr><tr><td>train_acc</td><td>0.48621</td></tr><tr><td>train_loss</td><td>1.33083</td></tr><tr><td>val_acc</td><td>0.49819</td></tr><tr><td>val_loss</td><td>1.27468</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ComplexMiniGoogLeNet</strong> at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/q7h5ah99' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/q7h5ah99</a><br> View project at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final</a><br>Synced 5 W&B file(s), 55 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_075040-q7h5ah99/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "PhusPwNHkvAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62f87b2-f01c-4699-f6d4-2348a2515f19"
      },
      "id": "PhusPwNHkvAx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}