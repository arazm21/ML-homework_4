{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arazm21/ML-homework_4/blob/main/expression_notebook_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the data and organising it"
      ],
      "metadata": {
        "id": "ShlkPaeoBQ3k"
      },
      "id": "ShlkPaeoBQ3k"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNNvBwljBQFE",
        "outputId": "e78fee55-f892-4a63-f710-3f5bce74527f"
      },
      "id": "dNNvBwljBQFE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 83% 236M/285M [00:00<00:00, 802MB/s] \n",
            "100% 285M/285M [00:00<00:00, 827MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "9fXPkihKllSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5c36af-f6f4-4491-ec12-32d60dc2987e"
      },
      "id": "9fXPkihKllSZ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "id": "xib5nVmSLH0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b1a2e2-5a66-4aa6-8cbe-642a0a2cf3c9"
      },
      "id": "xib5nVmSLH0o",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ─── 1) Augmentations for minority classes ────────────────────────────────────\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),  # finally convert to tensor in [0,1]\n",
        "])\n",
        "\n",
        "# ─── 2) Base “no‐transform” behavior ─────────────────────────────────────────\n",
        "\n",
        "class ExpressionDatasetFromDF(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Expect columns exactly [\"emotion\", \"pixels\", \"Usage\"]\n",
        "        # Each “pixels” entry is a string of “230  19  …”\n",
        "        self.images = dataframe[\" pixels\"].apply(\n",
        "            lambda x: np.fromstring(x, sep=\" \", dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        # Stack into a (N, 1, 48, 48) float32 tensor in [0,1]\n",
        "        self.images = torch.tensor(\n",
        "            np.stack(self.images.values), dtype=torch.float32\n",
        "        ).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(dataframe[\"emotion\"].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ─── 3) Oversampling / augmentation wrapper ──────────────────────────────────\n",
        "\n",
        "class AugmentedExpressionDataset(Dataset):\n",
        "    def __init__(self, base_dataset, targets, num_aug_per_sample=1):\n",
        "        \"\"\"\n",
        "        base_dataset: an ExpressionDatasetFromDF (no transforms applied)\n",
        "        targets: list of ints (same length as base_dataset)\n",
        "        \"\"\"\n",
        "        self.base_dataset = base_dataset\n",
        "        self.targets = targets\n",
        "        self.class_counts = Counter(targets)\n",
        "        self.max_count = max(self.class_counts.values())\n",
        "\n",
        "        # Build a list of “base indices” to duplicate + augment\n",
        "        self.augmented_indices = []\n",
        "        for cls_label, count in self.class_counts.items():\n",
        "            n_to_add = self.max_count - count\n",
        "            if n_to_add <= 0:\n",
        "                continue\n",
        "            # pick with replacement from all indices whose target == cls_label\n",
        "            indices_of_cls = [\n",
        "                i for i, t in enumerate(targets) if t == cls_label\n",
        "            ]\n",
        "            sampled = random.choices(indices_of_cls, k=n_to_add * num_aug_per_sample)\n",
        "            self.augmented_indices.extend(sampled)\n",
        "\n",
        "        print(f\"‣ Base dataset size: {len(self.base_dataset)}\")\n",
        "        print(f\"‣ Augmenting {len(self.augmented_indices)} extra samples to balance classes.\")\n",
        "        print(f\"‣ Resulting dataset size: {len(self.base_dataset) + len(self.augmented_indices)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset) + len(self.augmented_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.base_dataset):\n",
        "            # no augmentation for the “original” sample\n",
        "            img_tensor, label = self.base_dataset[idx]\n",
        "            return img_tensor, label\n",
        "        else:\n",
        "            # for augmented ones: take the base index, convert back to PIL, augment, then ToTensor\n",
        "            base_idx = self.augmented_indices[idx - len(self.base_dataset)]\n",
        "            img_tensor, label = self.base_dataset[base_idx]\n",
        "            # img_tensor is (1,48,48) float in [0,1]. Convert back to uint8 PIL:\n",
        "            arr_uint8 = (img_tensor.squeeze().numpy() * 255).astype(np.uint8)\n",
        "            pil_img = Image.fromarray(arr_uint8, mode=\"L\")\n",
        "            img_aug = aug_transform(pil_img)  # now a (1,48,48) FloatTensor in [0,1]\n",
        "            return img_aug, label\n",
        "\n",
        "\n",
        "# ─── 4) Revised get_data (no more “slice”) ───────────────────────────────────\n",
        "\n",
        "def get_data(csv_file=\"icml_face_data.csv\", train=True):\n",
        "    \"\"\"\n",
        "    - Reads icml_face_data.csv (which has exactly [\"emotion\",\"pixels\",\"Usage\"]).\n",
        "    - Splits by Usage == \"Training\" vs. \"PublicTest\".\n",
        "    - For train: wraps in AugmentedExpressionDataset. For test/val: returns raw dataset.\n",
        "    \"\"\"\n",
        "    full_df = pd.read_csv(csv_file)\n",
        "\n",
        "    if train:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"Training\"].reset_index(drop=True)\n",
        "    else:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"PublicTest\"].reset_index(drop=True)\n",
        "\n",
        "    print(f\"‣ Loaded '{'Training' if train else 'PublicTest'}' => {len(df_part)} samples before augmentation/slicing.\")\n",
        "\n",
        "    base_ds = ExpressionDatasetFromDF(df_part)\n",
        "\n",
        "    if train:\n",
        "        targets = df_part[\"emotion\"].tolist()\n",
        "        balanced_ds = AugmentedExpressionDataset(base_ds, targets)\n",
        "        return balanced_ds\n",
        "    else:\n",
        "        return base_ds\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=1,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "b5Ptu8H4Lzx6"
      },
      "id": "b5Ptu8H4Lzx6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ─── 1) Augmentations for minority classes ────────────────────────────────────\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),  # finally convert to tensor in [0,1]\n",
        "])\n",
        "\n",
        "# ─── 2) Base “no‐transform” behavior ─────────────────────────────────────────\n",
        "\n",
        "class ExpressionDatasetFromDF(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Expect columns exactly [\"emotion\", \"pixels\", \"Usage\"]\n",
        "        # Each “pixels” entry is a string of “230  19  …”\n",
        "        self.images = dataframe[\" pixels\"].apply(\n",
        "            lambda x: np.fromstring(x, sep=\" \", dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        # Stack into a (N, 1, 48, 48) float32 tensor in [0,1]\n",
        "        self.images = torch.tensor(\n",
        "            np.stack(self.images.values), dtype=torch.float32\n",
        "        ).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(dataframe[\"emotion\"].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ─── 3) Oversampling / augmentation wrapper ──────────────────────────────────\n",
        "\n",
        "class AugmentedExpressionDataset(Dataset):\n",
        "    def __init__(self, base_dataset, targets, num_aug_per_sample=1):\n",
        "        \"\"\"\n",
        "        base_dataset: an ExpressionDatasetFromDF (no transforms applied)\n",
        "        targets: list of ints (same length as base_dataset)\n",
        "        \"\"\"\n",
        "        self.base_dataset = base_dataset\n",
        "        self.targets = targets\n",
        "        self.class_counts = Counter(targets)\n",
        "        self.max_count = max(self.class_counts.values())\n",
        "\n",
        "        # Build a list of “base indices” to duplicate + augment\n",
        "        self.augmented_indices = []\n",
        "        for cls_label, count in self.class_counts.items():\n",
        "            n_to_add = self.max_count - count\n",
        "            if n_to_add <= 0:\n",
        "                continue\n",
        "            # pick with replacement from all indices whose target == cls_label\n",
        "            indices_of_cls = [\n",
        "                i for i, t in enumerate(targets) if t == cls_label\n",
        "            ]\n",
        "            sampled = random.choices(indices_of_cls, k=n_to_add * num_aug_per_sample)\n",
        "            self.augmented_indices.extend(sampled)\n",
        "\n",
        "        print(f\"‣ Base dataset size: {len(self.base_dataset)}\")\n",
        "        print(f\"‣ Augmenting {len(self.augmented_indices)} extra samples to balance classes.\")\n",
        "        print(f\"‣ Resulting dataset size: {len(self.base_dataset) + len(self.augmented_indices)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset) + len(self.augmented_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.base_dataset):\n",
        "            # no augmentation for the “original” sample\n",
        "            img_tensor, label = self.base_dataset[idx]\n",
        "            return img_tensor, label\n",
        "        else:\n",
        "            # for augmented ones: take the base index, convert back to PIL, augment, then ToTensor\n",
        "            base_idx = self.augmented_indices[idx - len(self.base_dataset)]\n",
        "            img_tensor, label = self.base_dataset[base_idx]\n",
        "            # img_tensor is (1,48,48) float in [0,1]. Convert back to uint8 PIL:\n",
        "            arr_uint8 = (img_tensor.squeeze().numpy() * 255).astype(np.uint8)\n",
        "            pil_img = Image.fromarray(arr_uint8, mode=\"L\")\n",
        "            img_aug = aug_transform(pil_img)  # now a (1,48,48) FloatTensor in [0,1]\n",
        "            return img_aug, label\n",
        "\n",
        "\n",
        "# ─── 4) Revised get_data (no more “slice”) ───────────────────────────────────\n",
        "\n",
        "def get_data(csv_file=\"icml_face_data.csv\", train=True, test=False):\n",
        "    \"\"\"\n",
        "    - Reads icml_face_data.csv (which has exactly [\"emotion\",\"pixels\",\"Usage\"]).\n",
        "    - Splits by Usage == \"Training\" vs. \"PublicTest\".\n",
        "    - For train: wraps in AugmentedExpressionDataset. For test/val: returns raw dataset.\n",
        "    \"\"\"\n",
        "    full_df = pd.read_csv(csv_file)\n",
        "\n",
        "    if train:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"Training\"].reset_index(drop=True)\n",
        "    elif test==False:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"PublicTest\"].reset_index(drop=True)\n",
        "    else:\n",
        "        df_part = full_df[full_df[\" Usage\"] == \"PrivateTest\"].reset_index(drop=True)\n",
        "    print(f\"‣ Loaded '{'Training' if train else 'PublicTest'}' => {len(df_part)} samples before augmentation/slicing.\")\n",
        "\n",
        "    base_ds = ExpressionDatasetFromDF(df_part)\n",
        "\n",
        "    if train:\n",
        "        targets = df_part[\"emotion\"].tolist()\n",
        "        balanced_ds = AugmentedExpressionDataset(base_ds, targets)\n",
        "        return balanced_ds\n",
        "    else:\n",
        "        return base_ds\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "\n",
        "# ─── 5) Quick sanity check ───────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Inspect base “Training” size (should be ~28,709 for FER2013).\n",
        "    train_ds = get_data(csv_file=\"icml_face_data.csv\", train=True)\n",
        "    print(f\"Len of train_ds (after augmentation): {len(train_ds)}\\n\")\n",
        "\n",
        "    # 2) Inspect base “PublicTest” size (should be ~3,589).\n",
        "    val_ds = get_data(csv_file=\"icml_face_data.csv\", train=False)\n",
        "    print(f\"Len of val_ds (no augmentation): {len(val_ds)}\\n\")\n",
        "\n",
        "    # 3) Create DataLoader and confirm iteration count\n",
        "    train_loader = make_loader(train_ds, batch_size=64)\n",
        "    total_seen = sum(len(batch[0]) for batch in train_loader)\n",
        "    print(f\"Total images seen via train_loader: {total_seen}\")\n"
      ],
      "metadata": {
        "id": "D1ye0F1iHqSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872887e5-226c-498d-eec5-b5ae5b2d1a12"
      },
      "id": "D1ye0F1iHqSC",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "Len of train_ds (after augmentation): 50505\n",
            "\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "Len of val_ds (no augmentation): 3589\n",
            "\n",
            "Total images seen via train_loader: 50505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test that loading was ok"
      ],
      "metadata": {
        "id": "RsFhOzV_PHxI"
      },
      "id": "RsFhOzV_PHxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and create loader\n",
        "dataset = get_data(train=False)\n",
        "loader = make_loader(dataset, batch_size=3)\n",
        "\n",
        "# Get a batch\n",
        "images, labels = next(iter(loader))\n",
        "\n",
        "# Class names from FER2013\n",
        "emotion_names = [\n",
        "    \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"\n",
        "]\n",
        "\n",
        "# Plot the first 3 images\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    plt.title(f\"Label: {emotion_names[labels[i].item()]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NvgplaWYHJX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "77ad8bbf-4258-4402-8e3a-4ccdb7ec986f"
      },
      "id": "NvgplaWYHJX6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connecting WANDB"
      ],
      "metadata": {
        "id": "XhwXYgieSf7u"
      },
      "id": "XhwXYgieSf7u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "zt2tymF2Se4p"
      },
      "id": "zt2tymF2Se4p",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "l2Op72b3SsY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "324856e6-c74c-4701-9bc9-8adcaea1993d"
      },
      "id": "l2Op72b3SsY1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marazm21\u001b[0m (\u001b[33marazm21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# different attempted architectures"
      ],
      "metadata": {
        "id": "FjvVv9RPXFbN"
      },
      "id": "FjvVv9RPXFbN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## baseline model"
      ],
      "metadata": {
        "id": "5jXpkWMRxArB"
      },
      "id": "5jXpkWMRxArB"
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, padding = 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n",
        "      self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
        "      self.pooling = nn.MaxPool2d(2,2)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear = nn.Linear((128 * 6 * 6), 128)\n",
        "      self.output = nn.Linear(128, 7)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.linear(x)\n",
        "      x = self.output(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "vJXLEBWwxESi"
      },
      "id": "vJXLEBWwxESi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enhanced baseline model"
      ],
      "metadata": {
        "id": "UdBMGddGyZl9"
      },
      "id": "UdBMGddGyZl9"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedBaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 48 -> 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 24 -> 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 12 -> 6\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 6 -> 3\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 7)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cy1jxwZQyaYe"
      },
      "id": "Cy1jxwZQyaYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImprovedModel"
      ],
      "metadata": {
        "id": "6Y6FOkFKqwWc"
      },
      "id": "6Y6FOkFKqwWc"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        # — conv backbone with 4 pooling stages —\n",
        "        self.conv1    = nn.Conv2d(1,   32, 3, padding=1, bias=False)\n",
        "        self.bn1      = nn.BatchNorm2d(32)\n",
        "        self.conv2    = nn.Conv2d(32,  64, 3, padding=1, bias=False)\n",
        "        self.bn2      = nn.BatchNorm2d(64)\n",
        "        self.conv3    = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3      = nn.BatchNorm2d(128)\n",
        "        self.conv4    = nn.Conv2d(128,256, 3, padding=1, bias=False)\n",
        "        self.bn4      = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool     = nn.MaxPool2d(2,2)\n",
        "        self.relu     = nn.ReLU(inplace=True)\n",
        "        self.drop_conv= nn.Dropout2d(dropout_p)\n",
        "\n",
        "        # # — two extra conv layers (no further pooling) —\n",
        "        # self.conv5    = nn.Conv2d(256, 512, 3, padding=1, bias=False)\n",
        "        # self.bn5      = nn.BatchNorm2d(512)\n",
        "        # self.conv6    = nn.Conv2d(512, 512, 3, padding=1, bias=False)\n",
        "        # self.bn6      = nn.BatchNorm2d(512)\n",
        "\n",
        "        # — richer head with 4 FCs —\n",
        "        self.flatten  = nn.Flatten()\n",
        "        # Corrected input size for the first linear layer\n",
        "        self.fc1      = nn.Linear(256 * 3 * 3, 512, bias=False) # 256 channels * 3x3 spatial size\n",
        "        self.bn_fc1   = nn.BatchNorm1d(512)\n",
        "        self.drop1    = nn.Dropout(dropout_p)\n",
        "        self.fc2      = nn.Linear(512, 256, bias=False)\n",
        "        self.bn_fc2   = nn.BatchNorm1d(256)\n",
        "        self.drop2    = nn.Dropout(dropout_p)\n",
        "        self.fc3      = nn.Linear(256, 128, bias=False)\n",
        "        self.bn_fc3   = nn.BatchNorm1d(128)\n",
        "        self.drop3    = nn.Dropout(dropout_p)\n",
        "        self.fc4      = nn.Linear(128, 64, bias=False)\n",
        "        self.bn_fc4   = nn.BatchNorm1d(64)\n",
        "        self.drop4    = nn.Dropout(dropout_p)\n",
        "        self.output   = nn.Linear(64, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv blocks 1–4 with pooling\n",
        "        for conv, bn in [(self.conv1,self.bn1),\n",
        "                         (self.conv2,self.bn2),\n",
        "                         (self.conv3,self.bn3),\n",
        "                         (self.conv4,self.bn4)]:\n",
        "            x = conv(x)\n",
        "            x = bn(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.pool(x)\n",
        "            x = self.drop_conv(x)\n",
        "\n",
        "        # # extra conv blocks 5–6 (no more pooling)\n",
        "        # x = self.conv5(x)\n",
        "        # x = self.bn5(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.drop_conv(x)\n",
        "\n",
        "        # x = self.conv6(x)\n",
        "        # x = self.bn6(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.drop_conv(x)\n",
        "\n",
        "        # FC head\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn_fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn_fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop4(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YVdHBSmXqwFC"
      },
      "id": "YVdHBSmXqwFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convnet"
      ],
      "metadata": {
        "id": "jjCDTHA1RHhj"
      },
      "id": "jjCDTHA1RHhj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# class ConvNet_super_simple(nn.Module):\n",
        "#     def __init__(self, kernels, classes=7):\n",
        "#         super(ConvNet_super_simple, self).__init__()\n",
        "\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         # Assuming 48x48 input, after two 2x2 poolings -> 48/2/2 = 12x12\n",
        "#         self.fc = nn.Linear(12 * 12 * kernels[1], classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet_Improved(nn.Module):\n",
        "    def __init__(self, kernels, classes=7):\n",
        "        super(ConvNet_Improved, self).__init__()\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(1, kernels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(kernels[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(kernels[0], kernels[1], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(kernels[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(kernels[1], kernels[2], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(kernels[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Assuming 48x48 input → 3 poolings: 48 → 24 → 12 → 6\n",
        "        self.flattened_dim = 6 * 6 * kernels[2]\n",
        "        self.fc = nn.Linear(self.flattened_dim, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.dropout1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "        x = self.pool2(self.dropout2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "        x = self.pool3(self.dropout3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RaFvctrZTf1t"
      },
      "id": "RaFvctrZTf1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "cumrZOH9Sj1A"
      },
      "id": "cumrZOH9Sj1A"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        # out = self.dropout(out)  # ✅ Dropout after residual addition\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SimpleResNet15(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.13):\n",
        "        super(SimpleResNet15, self).__init__()\n",
        "\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layers = nn.Sequential(\n",
        "          ResidualBlock(32, 64, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(64, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(512, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(1024, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "          ResidualBlock(2048, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(1024, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(512, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "          ResidualBlock(256, 256, downsample=False, dropout_rate=dropout_rate),\n",
        "          ResidualBlock(256, 256, downsample=False, dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, 256)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "PFYKchqhXTSt"
      },
      "id": "PFYKchqhXTSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## googlenet(mini)"
      ],
      "metadata": {
        "id": "ltk-bzVKSnDU"
      },
      "id": "ltk-bzVKSnDU"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class MiniInception(nn.Module):\n",
        "#     def __init__(self, in_ch, c1, c3red, c3, pool_proj):\n",
        "#         super().__init__()\n",
        "#         # 1×1 branch\n",
        "#         self.b1 = nn.Conv2d(in_ch, c1, kernel_size=1)\n",
        "#         # 1×1 → 3×3 branch\n",
        "#         self.b2_1 = nn.Conv2d(in_ch, c3red, kernel_size=1)\n",
        "#         self.b2_2 = nn.Conv2d(c3red, c3,   kernel_size=3, padding=1)\n",
        "#         # pool → 1×1 branch\n",
        "#         self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "#         self.b3_proj = nn.Conv2d(in_ch, pool_proj, kernel_size=1)\n",
        "#         self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b1 = self.b1(x)\n",
        "#         b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "#         b3 = self.b3_proj(self.b3_pool(x))\n",
        "#         out = torch.cat([b1, b2, b3], dim=1)\n",
        "#         return F.relu(self.bn(out))\n",
        "\n",
        "\n",
        "# class MiniGoogLeNet(nn.Module):\n",
        "#     def __init__(self, num_classes=7, aux_on=True):\n",
        "#         super().__init__()\n",
        "#         self.aux_on = aux_on\n",
        "\n",
        "#         # ---- stem ----\n",
        "#         self.stem = nn.Sequential(\n",
        "#             nn.Conv2d(1, 32, 3, padding=1),\n",
        "#             nn.BatchNorm2d(32), nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, 2)  # 48→24\n",
        "#         )\n",
        "\n",
        "#         # ---- two Inception blocks ----\n",
        "#         self.inc1 = MiniInception(32, c1=16, c3red=16, c3=24, pool_proj=16)  # outputs 56\n",
        "#         self.inc2 = MiniInception(56, c1=32, c3red=24, c3=32, pool_proj=24)  # outputs 88\n",
        "\n",
        "#         # auxiliary head (after inc1)\n",
        "#         if aux_on:\n",
        "#             self.aux = nn.Sequential(\n",
        "#                 nn.AdaptiveAvgPool2d((4,4)),\n",
        "#                 nn.Conv2d(56, 32, 1), nn.ReLU(),\n",
        "#                 nn.Flatten(),\n",
        "#                 nn.Linear(32*4*4, 128), nn.ReLU(), nn.Dropout(0.5),\n",
        "#                 nn.Linear(128, num_classes)\n",
        "#             )\n",
        "\n",
        "#         # ---- classifier head ----\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "#         self.fc   = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.Linear(88, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.stem(x)            # → [B,32,24,24]\n",
        "#         x1 = self.inc1(x)           # → [B,56,24,24]\n",
        "\n",
        "#         # you can still compute aux_out if you want,\n",
        "#         # but we won't return it so training code stays unchanged\n",
        "#         if self.training and self.aux_on:\n",
        "#             _ = self.aux(x1)\n",
        "\n",
        "#         x2 = F.max_pool2d(x1, 2, 2) # → [B,56,12,12]\n",
        "#         x2 = self.inc2(x2)          # → [B,88,12,12]\n",
        "\n",
        "#         x3 = self.pool(x2)          # → [B,88,1,1]\n",
        "#         main_out = self.fc(x3)      # → [B,num_classes]\n",
        "\n",
        "#         return main_out\n",
        "class MiniInception(nn.Module):\n",
        "    def __init__(self, in_ch, c1, c3red, c3, pool_proj, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 1×1 branch\n",
        "        self.b1 = nn.Conv2d(in_ch, c1, 1)\n",
        "        # 1×1 → 3×3 branch\n",
        "        self.b2_1 = nn.Conv2d(in_ch, c3red, 1)\n",
        "        self.b2_2 = nn.Conv2d(c3red, c3, 3, padding=1)\n",
        "        # pool → 1×1 branch\n",
        "        self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.b3_proj = nn.Conv2d(in_ch, pool_proj, 1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "        b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "        b3 = self.b3_proj(self.b3_pool(x))\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class ComplexMiniGoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # ── Stem ──\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 48 → 24\n",
        "        )\n",
        "\n",
        "        # ── Inception Blocks ──\n",
        "        self.inc1 = MiniInception(32, c1=16, c3red=16, c3=32, pool_proj=16, dropout=0.2)   # →64 ch\n",
        "        self.inc2 = MiniInception(64, c1=24, c3red=24, c3=48, pool_proj=24, dropout=0.2)   # →96 ch\n",
        "        self.inc3 = MiniInception(96, c1=32, c3red=32, c3=64, pool_proj=32, dropout=0.3)   # →128 ch\n",
        "\n",
        "        # ── Auxiliary head (computed but not returned) ──\n",
        "        if aux_on:\n",
        "            self.aux = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d((4, 4)),\n",
        "                nn.Conv2d(96, 48, 1), nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(48 * 4 * 4, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "\n",
        "        # ── Final classifier ──\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)          # → [B,32,24,24]\n",
        "        x1 = self.inc1(x)         # → [B,64,24,24]\n",
        "        x2 = self.inc2(x1)        # → [B,96,24,24]\n",
        "\n",
        "        # compute aux but ignore its output\n",
        "        if self.training and self.aux_on:\n",
        "            _ = self.aux(x2)\n",
        "\n",
        "        x3 = F.max_pool2d(x2, 2, 2)  # → [B,96,12,12]\n",
        "        x3 = self.inc3(x3)           # → [B,128,12,12]\n",
        "\n",
        "        x4 = self.final_pool(x3)     # → [B,128,1,1]\n",
        "        main_out = self.classifier(x4)\n",
        "        return main_out"
      ],
      "metadata": {
        "id": "2ydG-CJ4SsQ_"
      },
      "id": "2ydG-CJ4SsQ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## densenet"
      ],
      "metadata": {
        "id": "IH2tbOH_Hd8Z"
      },
      "id": "IH2tbOH_Hd8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        out = self.dropout(out)\n",
        "        return torch.cat([x, out], dim=1)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(DenseLayer(in_channels, growth_rate, dropout_rate))\n",
        "            in_channels += growth_rate\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.relu(self.bn(x)))\n",
        "        return self.pool(x)\n",
        "\n",
        "class MiniDenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, num_classes=7, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 2 * growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        num_channels = 2 * growth_rate\n",
        "\n",
        "        self.block1 = DenseBlock(num_channels, growth_rate, num_layers=4, dropout_rate=dropout_rate)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans1 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.block2 = DenseBlock(num_channels, growth_rate, num_layers=4, dropout_rate=dropout_rate)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans2 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc = nn.Linear(num_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(self.block1(x))\n",
        "        x = self.trans2(self.block2(x))\n",
        "        x = self.relu(self.bn(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "s_2N0muZHftN"
      },
      "id": "s_2N0muZHftN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vision transformer"
      ],
      "metadata": {
        "id": "LIri0mc2NmsS"
      },
      "id": "LIri0mc2NmsS"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import repeat\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=1, patch_size=8, emb_size=128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class ExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True, img_size=48, patch_size=8,\n",
        "                 emb_dim=128, n_layers=6, dropout=0.1, heads=4):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head (after layer n_layers//2, similar to your original design)\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after middle layer)\n",
        "            if i == self.n_layers // 2 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]  # Extract cls token\n",
        "                _ = self.aux_head(aux_cls_token)  # Compute but don't return\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "\n",
        "class CompactExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # Compact configuration for faster training\n",
        "        img_size = 48\n",
        "        patch_size = 6\n",
        "        emb_dim = 96\n",
        "        n_layers = 4\n",
        "        heads = 3\n",
        "        dropout = 0.2\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2  # 16 patches\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim * 2, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after 2nd layer for compact model)\n",
        "            if i == 1 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]\n",
        "                _ = self.aux_head(aux_cls_token)\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 1, patch_size = 8, emb_size = 128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "from einops import repeat\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, ch=1, img_size=48, patch_size=4, emb_dim=64,\n",
        "                n_layers=12, out_dim=7, dropout=0.2, heads=8):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.channels = ch\n",
        "        self.height = img_size\n",
        "        self.width = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "        # Output based on classification token\n",
        "        return self.head(x[:, 0, :])\n",
        "\n"
      ],
      "metadata": {
        "id": "_2nPvP7QNlqo"
      },
      "id": "_2nPvP7QNlqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def ViT(cf):\n",
        "    inputs = Input(shape=cf[\"input_shape\"])\n",
        "    patches = Patches(cf[\"patch_size\"])(inputs)\n",
        "    x = PatchEncoder(num_patches=cf[\"num_patches\"], projection_dim=cf[\"projection_dim\"])(patches)\n",
        "    cls_token = ClassToken()(x)\n",
        "    x = Concatenate(axis=1)([cls_token, x])\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = x[:, 0]\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "IzpTX__egM29"
      },
      "id": "IzpTX__egM29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit 2"
      ],
      "metadata": {
        "id": "wT8I_RqAXblc"
      },
      "id": "wT8I_RqAXblc"
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class LSA(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n",
        "\n",
        "        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "        dots = dots.masked_fill(mask, mask_value)\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
        "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class SPT(nn.Module):\n",
        "    def __init__(self, *, dim, patch_size, channels = 3):\n",
        "        super().__init__()\n",
        "        patch_dim = patch_size * patch_size * 5 * channels\n",
        "\n",
        "        self.to_patch_tokens = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
        "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
        "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
        "        return self.to_patch_tokens(x_with_shifts)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "G8zcfOi3XbAz"
      },
      "id": "G8zcfOi3XbAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit 3"
      ],
      "metadata": {
        "id": "U6aCWrdhc7zL"
      },
      "id": "U6aCWrdhc7zL"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PatchDropout(nn.Module):\n",
        "    def __init__(self, prob):\n",
        "        super().__init__()\n",
        "        assert 0 <= prob < 1.\n",
        "        self.prob = prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.prob == 0.:\n",
        "            return x\n",
        "\n",
        "        b, n, _, device = *x.shape, x.device\n",
        "\n",
        "        batch_indices = torch.arange(b, device = device)\n",
        "        batch_indices = rearrange(batch_indices, '... -> ... 1')\n",
        "        num_patches_keep = max(1, int(n * (1 - self.prob)))\n",
        "        patch_indices_keep = torch.randn(b, n, device = device).topk(num_patches_keep, dim = -1).indices\n",
        "\n",
        "        return x[batch_indices, patch_indices_keep]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
        "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., patch_dropout = 0.25):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        self.patch_dropout = PatchDropout(patch_dropout)\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        x += self.pos_embedding\n",
        "\n",
        "        x = self.patch_dropout(x)\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "Fzy2KwpUc9bd"
      },
      "id": "Fzy2KwpUc9bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleViT"
      ],
      "metadata": {
        "id": "2kkq2Ig1O_hr"
      },
      "id": "2kkq2Ig1O_hr"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ViT + Hyper-Connections + Register Tokens\n",
        "https://arxiv.org/abs/2409.19606\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.nn import Module, ModuleList\n",
        "\n",
        "from einops import rearrange, repeat, reduce, einsum, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# b - batch, h - heads, n - sequence, e - expansion rate / residual streams, d - feature dimension\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
        "    omega = 1.0 / (temperature ** omega)\n",
        "\n",
        "    y = y.flatten()[:, None] * omega[None, :]\n",
        "    x = x.flatten()[:, None] * omega[None, :]\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
        "    return pe.type(dtype)\n",
        "\n",
        "# hyper connections\n",
        "\n",
        "class HyperConnection(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_residual_streams,\n",
        "        layer_index\n",
        "    ):\n",
        "        \"\"\" Appendix J - Algorithm 2, Dynamic only \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim, bias = False)\n",
        "\n",
        "        self.num_residual_streams = num_residual_streams\n",
        "        self.layer_index = layer_index\n",
        "\n",
        "        self.static_beta = nn.Parameter(torch.ones(num_residual_streams))\n",
        "\n",
        "        init_alpha0 = torch.zeros((num_residual_streams, 1))\n",
        "        init_alpha0[layer_index % num_residual_streams, 0] = 1.\n",
        "\n",
        "        self.static_alpha = nn.Parameter(torch.cat([init_alpha0, torch.eye(num_residual_streams)], dim = 1))\n",
        "\n",
        "        self.dynamic_alpha_fn = nn.Parameter(torch.zeros(dim, num_residual_streams + 1))\n",
        "        self.dynamic_alpha_scale = nn.Parameter(tensor(1e-2))\n",
        "        self.dynamic_beta_fn = nn.Parameter(torch.zeros(dim))\n",
        "        self.dynamic_beta_scale = nn.Parameter(tensor(1e-2))\n",
        "\n",
        "    def width_connection(self, residuals):\n",
        "        normed = self.norm(residuals)\n",
        "\n",
        "        wc_weight = (normed @ self.dynamic_alpha_fn).tanh()\n",
        "        dynamic_alpha = wc_weight * self.dynamic_alpha_scale\n",
        "        alpha = dynamic_alpha + self.static_alpha\n",
        "\n",
        "        dc_weight = (normed @ self.dynamic_beta_fn).tanh()\n",
        "        dynamic_beta = dc_weight * self.dynamic_beta_scale\n",
        "        beta = dynamic_beta + self.static_beta\n",
        "\n",
        "        # width connection\n",
        "        mix_h = einsum(alpha, residuals, '... e1 e2, ... e1 d -> ... e2 d')\n",
        "\n",
        "        branch_input, residuals = mix_h[..., 0, :], mix_h[..., 1:, :]\n",
        "\n",
        "        return branch_input, residuals, beta\n",
        "\n",
        "    def depth_connection(\n",
        "        self,\n",
        "        branch_output,\n",
        "        residuals,\n",
        "        beta\n",
        "    ):\n",
        "        return einsum(branch_output, beta, \"b n d, b n e -> b n e d\") + residuals\n",
        "\n",
        "# classes\n",
        "\n",
        "class FeedForward(Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, num_residual_streams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_residual_streams = num_residual_streams\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = ModuleList([])\n",
        "\n",
        "        for layer_index in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                HyperConnection(dim, num_residual_streams, layer_index),\n",
        "                Attention(dim, heads = heads, dim_head = dim_head),\n",
        "                HyperConnection(dim, num_residual_streams, layer_index),\n",
        "                FeedForward(dim, mlp_dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = repeat(x, 'b n d -> b n e d', e = self.num_residual_streams)\n",
        "\n",
        "        for attn_hyper_conn, attn, ff_hyper_conn, ff in self.layers:\n",
        "\n",
        "            x, attn_res, beta = attn_hyper_conn.width_connection(x)\n",
        "\n",
        "            x = attn(x)\n",
        "\n",
        "            x = attn_hyper_conn.depth_connection(x, attn_res, beta)\n",
        "\n",
        "            x, ff_res, beta = ff_hyper_conn.width_connection(x)\n",
        "\n",
        "            x = ff(x)\n",
        "\n",
        "            x = ff_hyper_conn.depth_connection(x, ff_res, beta)\n",
        "\n",
        "        x = reduce(x, 'b n e d -> b n d', 'sum')\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, num_residual_streams, num_register_tokens = 4, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim))\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, num_residual_streams)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch, device = img.shape[0], img.device\n",
        "\n",
        "        x = self.to_patch_embedding(img)\n",
        "        x += self.pos_embedding.to(x)\n",
        "\n",
        "        r = repeat(self.register_tokens, 'n d -> b n d', b = batch)\n",
        "\n",
        "        x, ps = pack([x, r], 'b * d')\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x, _ = unpack(x, ps, 'b * d')\n",
        "\n",
        "        x = x.mean(dim = 1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)\n",
        "\n",
        "# main"
      ],
      "metadata": {
        "id": "3NOENr78O-pM"
      },
      "id": "3NOENr78O-pM",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## timm vit"
      ],
      "metadata": {
        "id": "R4VHkF1nUAZq"
      },
      "id": "R4VHkF1nUAZq"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "print(timm.list_models('*vit*'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXFqSDRIUDGM",
        "outputId": "68b1c8be-01ad-4edf-882b-e8f6b2595813"
      },
      "id": "bXFqSDRIUDGM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.32.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "['convit_base', 'convit_small', 'convit_tiny', 'crossvit_9_240', 'crossvit_9_dagger_240', 'crossvit_15_240', 'crossvit_15_dagger_240', 'crossvit_15_dagger_408', 'crossvit_18_240', 'crossvit_18_dagger_240', 'crossvit_18_dagger_408', 'crossvit_base_240', 'crossvit_small_240', 'crossvit_tiny_240', 'davit_base', 'davit_base_fl', 'davit_giant', 'davit_huge', 'davit_huge_fl', 'davit_large', 'davit_small', 'davit_tiny', 'efficientvit_b0', 'efficientvit_b1', 'efficientvit_b2', 'efficientvit_b3', 'efficientvit_l1', 'efficientvit_l2', 'efficientvit_l3', 'efficientvit_m0', 'efficientvit_m1', 'efficientvit_m2', 'efficientvit_m3', 'efficientvit_m4', 'efficientvit_m5', 'fastvit_ma36', 'fastvit_mci0', 'fastvit_mci1', 'fastvit_mci2', 'fastvit_s12', 'fastvit_sa12', 'fastvit_sa24', 'fastvit_sa36', 'fastvit_t8', 'fastvit_t12', 'flexivit_base', 'flexivit_large', 'flexivit_small', 'gcvit_base', 'gcvit_small', 'gcvit_tiny', 'gcvit_xtiny', 'gcvit_xxtiny', 'levit_128', 'levit_128s', 'levit_192', 'levit_256', 'levit_256d', 'levit_384', 'levit_384_s8', 'levit_512', 'levit_512_s8', 'levit_512d', 'levit_conv_128', 'levit_conv_128s', 'levit_conv_192', 'levit_conv_256', 'levit_conv_256d', 'levit_conv_384', 'levit_conv_384_s8', 'levit_conv_512', 'levit_conv_512_s8', 'levit_conv_512d', 'maxvit_base_tf_224', 'maxvit_base_tf_384', 'maxvit_base_tf_512', 'maxvit_large_tf_224', 'maxvit_large_tf_384', 'maxvit_large_tf_512', 'maxvit_nano_rw_256', 'maxvit_pico_rw_256', 'maxvit_rmlp_base_rw_224', 'maxvit_rmlp_base_rw_384', 'maxvit_rmlp_nano_rw_256', 'maxvit_rmlp_pico_rw_256', 'maxvit_rmlp_small_rw_224', 'maxvit_rmlp_small_rw_256', 'maxvit_rmlp_tiny_rw_256', 'maxvit_small_tf_224', 'maxvit_small_tf_384', 'maxvit_small_tf_512', 'maxvit_tiny_pm_256', 'maxvit_tiny_rw_224', 'maxvit_tiny_rw_256', 'maxvit_tiny_tf_224', 'maxvit_tiny_tf_384', 'maxvit_tiny_tf_512', 'maxvit_xlarge_tf_224', 'maxvit_xlarge_tf_384', 'maxvit_xlarge_tf_512', 'maxxvit_rmlp_nano_rw_256', 'maxxvit_rmlp_small_rw_256', 'maxxvit_rmlp_tiny_rw_256', 'maxxvitv2_nano_rw_256', 'maxxvitv2_rmlp_base_rw_224', 'maxxvitv2_rmlp_base_rw_384', 'maxxvitv2_rmlp_large_rw_224', 'mobilevit_s', 'mobilevit_xs', 'mobilevit_xxs', 'mobilevitv2_050', 'mobilevitv2_075', 'mobilevitv2_100', 'mobilevitv2_125', 'mobilevitv2_150', 'mobilevitv2_175', 'mobilevitv2_200', 'mvitv2_base', 'mvitv2_base_cls', 'mvitv2_huge_cls', 'mvitv2_large', 'mvitv2_large_cls', 'mvitv2_small', 'mvitv2_small_cls', 'mvitv2_tiny', 'nextvit_base', 'nextvit_large', 'nextvit_small', 'repvit_m0_9', 'repvit_m1', 'repvit_m1_0', 'repvit_m1_1', 'repvit_m1_5', 'repvit_m2', 'repvit_m2_3', 'repvit_m3', 'samvit_base_patch16', 'samvit_base_patch16_224', 'samvit_huge_patch16', 'samvit_large_patch16', 'test_vit', 'test_vit2', 'test_vit3', 'test_vit4', 'tiny_vit_5m_224', 'tiny_vit_11m_224', 'tiny_vit_21m_224', 'tiny_vit_21m_384', 'tiny_vit_21m_512', 'vit_base_mci_224', 'vit_base_patch8_224', 'vit_base_patch14_dinov2', 'vit_base_patch14_reg4_dinov2', 'vit_base_patch16_18x2_224', 'vit_base_patch16_224', 'vit_base_patch16_224_miil', 'vit_base_patch16_384', 'vit_base_patch16_clip_224', 'vit_base_patch16_clip_384', 'vit_base_patch16_clip_quickgelu_224', 'vit_base_patch16_gap_224', 'vit_base_patch16_plus_240', 'vit_base_patch16_plus_clip_240', 'vit_base_patch16_reg4_gap_256', 'vit_base_patch16_rope_reg1_gap_256', 'vit_base_patch16_rpn_224', 'vit_base_patch16_siglip_224', 'vit_base_patch16_siglip_256', 'vit_base_patch16_siglip_384', 'vit_base_patch16_siglip_512', 'vit_base_patch16_siglip_gap_224', 'vit_base_patch16_siglip_gap_256', 'vit_base_patch16_siglip_gap_384', 'vit_base_patch16_siglip_gap_512', 'vit_base_patch16_xp_224', 'vit_base_patch32_224', 'vit_base_patch32_384', 'vit_base_patch32_clip_224', 'vit_base_patch32_clip_256', 'vit_base_patch32_clip_384', 'vit_base_patch32_clip_448', 'vit_base_patch32_clip_quickgelu_224', 'vit_base_patch32_plus_256', 'vit_base_patch32_siglip_256', 'vit_base_patch32_siglip_gap_256', 'vit_base_r26_s32_224', 'vit_base_r50_s16_224', 'vit_base_r50_s16_384', 'vit_base_resnet26d_224', 'vit_base_resnet50d_224', 'vit_betwixt_patch16_gap_256', 'vit_betwixt_patch16_reg1_gap_256', 'vit_betwixt_patch16_reg4_gap_256', 'vit_betwixt_patch16_reg4_gap_384', 'vit_betwixt_patch16_rope_reg4_gap_256', 'vit_betwixt_patch32_clip_224', 'vit_giant_patch14_224', 'vit_giant_patch14_clip_224', 'vit_giant_patch14_dinov2', 'vit_giant_patch14_reg4_dinov2', 'vit_giant_patch16_gap_224', 'vit_giantopt_patch16_siglip_256', 'vit_giantopt_patch16_siglip_384', 'vit_giantopt_patch16_siglip_gap_256', 'vit_giantopt_patch16_siglip_gap_384', 'vit_gigantic_patch14_224', 'vit_gigantic_patch14_clip_224', 'vit_gigantic_patch14_clip_quickgelu_224', 'vit_huge_patch14_224', 'vit_huge_patch14_clip_224', 'vit_huge_patch14_clip_336', 'vit_huge_patch14_clip_378', 'vit_huge_patch14_clip_quickgelu_224', 'vit_huge_patch14_clip_quickgelu_378', 'vit_huge_patch14_gap_224', 'vit_huge_patch14_xp_224', 'vit_huge_patch16_gap_448', 'vit_intern300m_patch14_448', 'vit_large_patch14_224', 'vit_large_patch14_clip_224', 'vit_large_patch14_clip_336', 'vit_large_patch14_clip_quickgelu_224', 'vit_large_patch14_clip_quickgelu_336', 'vit_large_patch14_dinov2', 'vit_large_patch14_reg4_dinov2', 'vit_large_patch14_xp_224', 'vit_large_patch16_224', 'vit_large_patch16_384', 'vit_large_patch16_siglip_256', 'vit_large_patch16_siglip_384', 'vit_large_patch16_siglip_512', 'vit_large_patch16_siglip_gap_256', 'vit_large_patch16_siglip_gap_384', 'vit_large_patch16_siglip_gap_512', 'vit_large_patch32_224', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_384', 'vit_little_patch16_reg1_gap_256', 'vit_little_patch16_reg4_gap_256', 'vit_medium_patch16_clip_224', 'vit_medium_patch16_gap_240', 'vit_medium_patch16_gap_256', 'vit_medium_patch16_gap_384', 'vit_medium_patch16_reg1_gap_256', 'vit_medium_patch16_reg4_gap_256', 'vit_medium_patch16_rope_reg1_gap_256', 'vit_medium_patch32_clip_224', 'vit_mediumd_patch16_reg4_gap_256', 'vit_mediumd_patch16_reg4_gap_384', 'vit_mediumd_patch16_rope_reg1_gap_256', 'vit_pwee_patch16_reg1_gap_256', 'vit_relpos_base_patch16_224', 'vit_relpos_base_patch16_cls_224', 'vit_relpos_base_patch16_clsgap_224', 'vit_relpos_base_patch16_plus_240', 'vit_relpos_base_patch16_rpn_224', 'vit_relpos_base_patch32_plus_rpn_256', 'vit_relpos_medium_patch16_224', 'vit_relpos_medium_patch16_cls_224', 'vit_relpos_medium_patch16_rpn_224', 'vit_relpos_small_patch16_224', 'vit_relpos_small_patch16_rpn_224', 'vit_small_patch8_224', 'vit_small_patch14_dinov2', 'vit_small_patch14_reg4_dinov2', 'vit_small_patch16_18x2_224', 'vit_small_patch16_36x1_224', 'vit_small_patch16_224', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_384', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'vit_so150m2_patch16_reg1_gap_256', 'vit_so150m2_patch16_reg1_gap_384', 'vit_so150m2_patch16_reg1_gap_448', 'vit_so150m_patch16_reg4_gap_256', 'vit_so150m_patch16_reg4_gap_384', 'vit_so150m_patch16_reg4_map_256', 'vit_so400m_patch14_siglip_224', 'vit_so400m_patch14_siglip_378', 'vit_so400m_patch14_siglip_384', 'vit_so400m_patch14_siglip_gap_224', 'vit_so400m_patch14_siglip_gap_378', 'vit_so400m_patch14_siglip_gap_384', 'vit_so400m_patch14_siglip_gap_448', 'vit_so400m_patch14_siglip_gap_896', 'vit_so400m_patch16_siglip_256', 'vit_so400m_patch16_siglip_384', 'vit_so400m_patch16_siglip_512', 'vit_so400m_patch16_siglip_gap_256', 'vit_so400m_patch16_siglip_gap_384', 'vit_so400m_patch16_siglip_gap_512', 'vit_srelpos_medium_patch16_224', 'vit_srelpos_small_patch16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_384', 'vit_wee_patch16_reg1_gap_256', 'vit_xsmall_patch16_clip_224', 'vitamin_base_224', 'vitamin_large2_224', 'vitamin_large2_256', 'vitamin_large2_336', 'vitamin_large2_384', 'vitamin_large_224', 'vitamin_large_256', 'vitamin_large_336', 'vitamin_large_384', 'vitamin_small_224', 'vitamin_xlarge_256', 'vitamin_xlarge_336', 'vitamin_xlarge_384']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ComplexMiniGoogLeNetV2"
      ],
      "metadata": {
        "id": "MbOprWiUJN5f"
      },
      "id": "MbOprWiUJN5f"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiniInception(nn.Module):\n",
        "    def __init__(self, in_ch, c1, c3red, c3, pool_proj, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Conv2d(in_ch, c1, 1)\n",
        "        self.b2_1 = nn.Conv2d(in_ch, c3red, 1)\n",
        "        self.b2_2 = nn.Conv2d(c3red, c3, 3, padding=1)\n",
        "        self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.b3_proj = nn.Conv2d(in_ch, pool_proj, 1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "        b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "        b3 = self.b3_proj(self.b3_pool(x))\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        return self.dropout(out)\n",
        "\n",
        "class ComplexMiniGoogLeNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # ── Stem ──\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 48x48 → 24x24\n",
        "        )\n",
        "\n",
        "        # ── Inception blocks with increasing depth ──\n",
        "        self.inc1 = MiniInception(64, 32, 32, 64, 32, dropout=0.2)     # →128 ch\n",
        "        self.inc2 = MiniInception(128, 64, 48, 96, 48, dropout=0.2)    # →208 ch\n",
        "        self.inc3 = MiniInception(208, 96, 64, 128, 64, dropout=0.3)   # →288 ch\n",
        "\n",
        "        self.down1 = nn.MaxPool2d(2, 2)  # 24 → 12\n",
        "\n",
        "        self.inc4 = MiniInception(288, 96, 96, 160, 64, dropout=0.3)   # →320 ch\n",
        "        self.inc5 = MiniInception(320, 128, 96, 192, 64, dropout=0.4)  # →384 ch\n",
        "\n",
        "        # ── Auxiliary head ──\n",
        "        if aux_on:\n",
        "            self.aux = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d((4, 4)),\n",
        "                nn.Conv2d(208, 64, 1), nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(64 * 4 * 4, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "\n",
        "        # ── Classifier ──\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(384, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)       # → [B,64,24,24]\n",
        "        x = self.inc1(x)       # → [B,128,24,24]\n",
        "        x = self.inc2(x)       # → [B,208,24,24]\n",
        "\n",
        "        if self.training and self.aux_on:\n",
        "            _ = self.aux(x)\n",
        "\n",
        "        x = self.inc3(x)       # → [B,288,24,24]\n",
        "        x = self.down1(x)      # → [B,288,12,12]\n",
        "        x = self.inc4(x)       # → [B,320,12,12]\n",
        "        x = self.inc5(x)       # → [B,384,12,12]\n",
        "\n",
        "        x = self.pool(x)       # → [B,384,1,1]\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "QMsZ8NWPJOuK"
      },
      "id": "QMsZ8NWPJOuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## beefy model"
      ],
      "metadata": {
        "id": "quH7bCxnmvZs"
      },
      "id": "quH7bCxnmvZs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout_p=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout2d(dropout_p) if dropout_p > 0 else nn.Identity()\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class BeefyGigaModel(nn.Module):\n",
        "    def __init__(self, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # initial conv with higher channels\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # beefier residual stack\n",
        "        self.layer1 = ResidualBlock(64,   64,  stride=1, dropout_p=dropout_p)\n",
        "        self.layer2 = ResidualBlock(64,  128,  stride=2, dropout_p=dropout_p)\n",
        "        self.layer3 = ResidualBlock(128, 256,  stride=2, dropout_p=dropout_p)\n",
        "        self.layer4 = ResidualBlock(256, 512,  stride=2, dropout_p=dropout_p)\n",
        "        self.layer5 = ResidualBlock(512, 512,  stride=1, dropout_p=dropout_p)  # additional layer, no spatial downsample\n",
        "\n",
        "        # global pooling with both avg and max\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # wider MLP head\n",
        "        self.fc1 = nn.Linear(512 * 2, 1024, bias=False)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
        "        self.drop1 = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(1024, 512, bias=False)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.drop2 = nn.Dropout(dropout_p)\n",
        "        self.fc3 = nn.Linear(512, 256, bias=False)\n",
        "        self.bn_fc3 = nn.BatchNorm1d(256)\n",
        "        self.drop3 = nn.Dropout(dropout_p)\n",
        "        self.output = nn.Linear(256, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "\n",
        "        x_avg = self.avgpool(x)\n",
        "        x_max = self.maxpool(x)\n",
        "        x = torch.cat([x_avg, x_max], dim=1)  # shape: [B, 1024, 1, 1]\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn_fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "CNWWmIcMmxgg"
      },
      "id": "CNWWmIcMmxgg",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeefyGigaModelV2"
      ],
      "metadata": {
        "id": "Ph5bw36CsPZF"
      },
      "id": "Ph5bw36CsPZF"
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout_p=0.2, bottleneck=False):\n",
        "        super().__init__()\n",
        "        mid_channels = out_channels // 4 if bottleneck else out_channels\n",
        "\n",
        "        self.bottleneck = bottleneck\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1 if bottleneck else 3,\n",
        "                               stride=1, padding=0 if bottleneck else 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.act = Swish()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1 if bottleneck else 3,\n",
        "                               stride=1, padding=0 if bottleneck else 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.se = SEBlock(out_channels)\n",
        "        self.dropout = nn.Dropout2d(dropout_p)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.act(self.bn1(self.conv1(x)))\n",
        "        out = self.act(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.act(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class BeefyGigaModelV2(nn.Module):\n",
        "    def __init__(self, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.act = Swish()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            self.act\n",
        "        )\n",
        "\n",
        "        self.layer1 = ResidualBlock(64, 128, stride=1, dropout_p=dropout_p)\n",
        "        self.layer2 = ResidualBlock(128, 128, stride=1, dropout_p=dropout_p, bottleneck=True)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2, dropout_p=dropout_p)\n",
        "        self.layer4 = ResidualBlock(256, 512, stride=2, dropout_p=dropout_p)\n",
        "        self.layer5 = ResidualBlock(512, 512, stride=1, dropout_p=dropout_p, bottleneck=True)\n",
        "        self.layer6 = ResidualBlock(512, 512, stride=1, dropout_p=dropout_p)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.norm = nn.LayerNorm(1024)\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 1024, bias=False)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
        "        self.drop1 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 512, bias=False)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.drop2 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 256, bias=False)\n",
        "        self.bn_fc3 = nn.BatchNorm1d(256)\n",
        "        self.drop3 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.output = nn.Linear(256, 7)\n",
        "\n",
        "        self.shortcut = nn.Linear(1024, 256, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)\n",
        "\n",
        "        x_avg = self.avgpool(x)\n",
        "        x_max = self.maxpool(x)\n",
        "        x = torch.cat([x_avg, x_max], dim=1)  # [B, 1024, 1, 1]\n",
        "        x = self.flatten(x)                   # [B, 1024]\n",
        "        x = self.norm(x)\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn_fc3(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        residual = self.shortcut(residual)\n",
        "        x += residual\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tiJyOmkXsQN5"
      },
      "id": "tiJyOmkXsQN5",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleResNetSmaller"
      ],
      "metadata": {
        "id": "jJW-auFODAN7"
      },
      "id": "jJW-auFODAN7"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout(out)\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "class SimpleResNetSmaller(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.3):\n",
        "        super(SimpleResNetSmaller, self).__init__()\n",
        "\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            ResidualBlock(32, 64, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(64, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 512, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 512, downsample=False, dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "Q39CVahIC-Lr"
      },
      "id": "Q39CVahIC-Lr",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleResNetHybrid"
      ],
      "metadata": {
        "id": "UnT38KESIYr5"
      },
      "id": "UnT38KESIYr5"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = ConvBNReLU(in_channels, out_channels, stride=stride)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout(out)\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "class SimpleResNetHybrid(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            ConvBNReLU(in_channels, 32),\n",
        "            ConvBNReLU(32, 64),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            ResidualBlock(64, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "            ConvBNReLU(128, 128),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "            ConvBNReLU(256, 256),\n",
        "\n",
        "            ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ConvBNReLU(512, 512),\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Classifier head with multiple FCs\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KYKcjfEEIWjf"
      },
      "id": "KYKcjfEEIWjf",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## og resnet"
      ],
      "metadata": {
        "id": "GVtUtUZiTrHX"
      },
      "id": "GVtUtUZiTrHX"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class block(nn.Module):\n",
        "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Adapted for 28x28 images\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Removed aggressive downsampling\n",
        "        self.maxpool = nn.Identity()\n",
        "\n",
        "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, intermediate_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        for _ in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "alB1gqGmTxWy"
      },
      "id": "alB1gqGmTxWy",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# geting everything ready"
      ],
      "metadata": {
        "id": "zaVbntQmXODJ"
      },
      "id": "zaVbntQmXODJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train_dataset = get_data(train=True)\n",
        "    test_dataset = get_data(train=False)\n",
        "    final_test_dataset = get_data(train=False, test=True)\n",
        "    train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "    final_test_loader = make_loader(final_test_dataset, batch_size=config.batch_size)\n",
        "    # Make the model\n",
        "    model = ResNet(block, [2, 2, 2,2], 1, 7).to(device)\n",
        "    # model = timm.create_model('convit_tiny', pretrained=True, num_classes=7).to(device)\n",
        "    # model = ViT(\n",
        "    #     image_size=48,\n",
        "    #     patch_size=6,\n",
        "    #     num_classes=7,\n",
        "    #     dim=128,          # embedding dimension\n",
        "    #     depth=6,          # number of transformer layers\n",
        "    #     heads=4,          # number of self-attention heads\n",
        "    #     mlp_dim=256,      # hidden dim in the feedforward\n",
        "    #     pool='cls',       # use cls token pooling\n",
        "    #     channels=1,       # grayscale input\n",
        "    #     dim_head=32,      # dimension per attention head\n",
        "    #     dropout=0.1,      # transformer dropout\n",
        "    #     emb_dropout=0.1   # dropout after patch+pos embedding\n",
        "    # ).to(device)\n",
        "    # model = ViT(\n",
        "    #     image_size=48,\n",
        "    #     patch_size=6,\n",
        "    #     num_classes=7,\n",
        "    #     dim=256,          # embedding dim\n",
        "    #     depth=12,          # transformer depth (layers)\n",
        "    #     heads=6,          # number of attention heads\n",
        "    #     mlp_dim=512,      # feedforward hidden layer size\n",
        "    #     pool='cls',       # cls token pooling\n",
        "    #     channels=1,       # grayscale image\n",
        "    #     dim_head=64,      # per-head dimension\n",
        "    #     dropout=0.2,      # transformer dropout\n",
        "    #     emb_dropout=0.1,  # embedding dropout\n",
        "    #     patch_dropout=0.1 # optional patch dropout\n",
        "    # ).to(device)\n",
        "    # model = SimpleViT(\n",
        "    #     num_classes = 7,\n",
        "    #     image_size = 256,\n",
        "    #     patch_size = 8,\n",
        "    #     dim = 1024,\n",
        "    #     depth = 12,\n",
        "    #     heads = 8,\n",
        "    #     mlp_dim = 2048,\n",
        "    #     num_residual_streams = 8\n",
        "    # ).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer, final_test_loader"
      ],
      "metadata": {
        "id": "4GOnNEKyT1v2"
      },
      "id": "4GOnNEKyT1v2",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "Jwbd9gvXUp4R"
      },
      "id": "Jwbd9gvXUp4R",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## early stop training"
      ],
      "metadata": {
        "id": "6Vih9mc96ns6"
      },
      "id": "6Vih9mc96ns6"
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=15, min_delta=0.001)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            loss, batch_correct, batch_total = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            running_correct += batch_correct\n",
        "            running_total += batch_total\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        train_acc = running_correct / running_total\n",
        "\n",
        "        # ⏱️ Validate at the end of the epoch\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_accuracy\": train_acc\n",
        "        })\n",
        "        print(f\"Epoch {epoch + 1}: val_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}, train_acc = {train_acc:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopper(val_loss)\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "\n",
        "    return loss, correct, total"
      ],
      "metadata": {
        "id": "2gDrRJB56ihi"
      },
      "id": "2gDrRJB56ihi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normal training"
      ],
      "metadata": {
        "id": "Ts0xewao6rsq"
      },
      "id": "Ts0xewao6rsq"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config, class_names= [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]):\n",
        "    # wandb.init(\n",
        "    #     project=\"\",\n",
        "    #     config={\n",
        "    #         \"epochs\": config.epochs,\n",
        "    #         \"batch_size\": train_loader.batch_size,\n",
        "    #         \"optimizer\": optimizer.__class__.__name__,\n",
        "    #         \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "    #         \"criterion\": criterion.__class__.__name__,\n",
        "    #     }\n",
        "    # )\n",
        "    # wandb.watch(model, log=\"all\", log_freq=100)\n",
        "    model.to(config.device)\n",
        "\n",
        "    train_loss_plot, val_loss_plot = [], []\n",
        "    train_acc_plot, val_acc_plot = [], []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
        "            images, labels = images.to(config.device), labels.to(config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = total_train_loss / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc, all_targets, all_preds = validate(model, val_loader, criterion, config.device)\n",
        "\n",
        "        # F1 and Confusion Matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "        log_data = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "        }\n",
        "        for i, name in enumerate(class_names):\n",
        "            log_data[f\"f1_{name}\"] = f1_per_class[i]\n",
        "        wandb.log(log_data)\n",
        "\n",
        "        # Confusion matrix image\n",
        "        fig, ax = plt.subplots(figsize=(6,6))\n",
        "        im = ax.imshow(cm, cmap=\"Blues\")\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set_xticks(np.arange(len(class_names)))\n",
        "        ax.set_yticks(np.arange(len(class_names)))\n",
        "        ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
        "        ax.set_yticklabels(class_names)\n",
        "        for i in range(len(class_names)):\n",
        "            for j in range(len(class_names)):\n",
        "                ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "        ax.set_title(f\"Confusion Matrix - Epoch {epoch+1}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Store for final plots\n",
        "        train_loss_plot.append(train_loss)\n",
        "        val_loss_plot.append(val_loss)\n",
        "        train_acc_plot.append(train_acc)\n",
        "        val_acc_plot.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs}: \"\n",
        "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
        "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    # Final loss/accuracy curves\n",
        "    epochs_range = list(range(1, config.epochs + 1))\n",
        "\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(epochs_range, train_loss_plot, label=\"Train Loss\")\n",
        "    ax1.plot(epochs_range, val_loss_plot, label=\"Val Loss\")\n",
        "    ax1.set_title(\"Loss vs Epoch\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    wandb.log({\"loss_curve\": wandb.Image(fig1)})\n",
        "    plt.close(fig1)\n",
        "\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(epochs_range, train_acc_plot, label=\"Train Accuracy\")\n",
        "    ax2.plot(epochs_range, val_acc_plot, label=\"Val Accuracy\")\n",
        "    ax2.set_title(\"Accuracy vs Epoch\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    ax2.legend()\n",
        "    wandb.log({\"accuracy_curve\": wandb.Image(fig2)})\n",
        "    plt.close(fig2)\n",
        "\n",
        "    return model\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / total\n",
        "    val_acc = correct / total\n",
        "    return avg_val_loss, val_acc, all_targets, all_preds\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "def test(model, test_loader, device, class_names=None):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "    misclassified_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Save misclassified images\n",
        "            mis_mask = (predicted != labels)\n",
        "            mis_imgs = images[mis_mask]\n",
        "            mis_lbls = labels[mis_mask]\n",
        "            mis_preds = predicted[mis_mask]\n",
        "            for img, true_lbl, pred_lbl in zip(mis_imgs, mis_lbls, mis_preds):\n",
        "                misclassified_images.append((img.cpu(), true_lbl.item(), pred_lbl.item()))\n",
        "\n",
        "    acc = correct / total\n",
        "    f1_macro = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {acc:.2%}\")\n",
        "    print(f\"F1 Macro: {f1_macro:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_targets, all_preds, target_names=class_names))\n",
        "\n",
        "    # Log numeric metrics\n",
        "    log_data = {\n",
        "        \"test_accuracy\": acc,\n",
        "        \"test_f1_macro\": f1_macro,\n",
        "    }\n",
        "    if class_names:\n",
        "        for i, name in enumerate(class_names):\n",
        "            log_data[f\"f1_{name}\"] = f1_per_class[i]\n",
        "    wandb.log(log_data)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        xticklabels=class_names if class_names else \"auto\",\n",
        "        yticklabels=class_names if class_names else \"auto\",\n",
        "        cmap=\"Blues\"\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_title(\"Confusion Matrix (Test)\")\n",
        "    # wandb.log({\n",
        "    #     \"confusion_matrix_final\": wandb.Image(fig),\n",
        "    #     \"confusion_matrix_raw\": wandb.Table(data=cm.tolist(), columns=class_names, rows=class_names),\n",
        "    # })\n",
        "    plt.close(fig)\n",
        "\n",
        "    wandb.log({\n",
        "    \"confusion_matrix_final\": wandb.Image(fig),\n",
        "    \"confusion_matrix_raw\": wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=all_targets,\n",
        "        preds=all_preds,\n",
        "        class_names=class_names if class_names else None\n",
        "    )\n",
        "})\n",
        "    # Log a few misclassified images\n",
        "    if class_names:\n",
        "        wrong_table = wandb.Table(columns=[\"Image\", \"True Label\", \"Predicted Label\"])\n",
        "        for img, true_lbl, pred_lbl in misclassified_images[:25]:\n",
        "            img_np = img.squeeze().numpy()  # assumes grayscale\n",
        "            wrong_table.add_data(\n",
        "                wandb.Image(img_np, caption=f\"Pred: {class_names[pred_lbl]}, GT: {class_names[true_lbl]}\"),\n",
        "                class_names[true_lbl],\n",
        "                class_names[pred_lbl]\n",
        "            )\n",
        "        wandb.log({\"misclassified_samples\": wrong_table})\n",
        "\n",
        "    # Export ONNX\n",
        "    dummy_input = torch.randn(1, *images.shape[1:]).to(device)\n",
        "    torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")\n",
        "\n",
        "    # Optionally log model architecture as a string\n",
        "    model_arch_str = str(model)\n",
        "    wandb.log({\"model_architecture\": wandb.Html(f\"<pre>{model_arch_str}</pre>\")})\n",
        "\n",
        "\n",
        "\n",
        "def Final_test(model, test_loader, device, class_names=None):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "    misclassified_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Save misclassified images\n",
        "            mis_mask = (predicted != labels)\n",
        "            mis_imgs = images[mis_mask]\n",
        "            mis_lbls = labels[mis_mask]\n",
        "            mis_preds = predicted[mis_mask]\n",
        "            for img, true_lbl, pred_lbl in zip(mis_imgs, mis_lbls, mis_preds):\n",
        "                misclassified_images.append((img.cpu(), true_lbl.item(), pred_lbl.item()))\n",
        "\n",
        "    acc = correct / total\n",
        "    f1_macro = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "    print(f\"\\nFinal_Test Accuracy: {acc:.2%}\")\n",
        "    print(f\"Final_F1 Macro: {f1_macro:.4f}\")\n",
        "    print(\"\\nFinal_Classification Report:\\n\", classification_report(all_targets, all_preds, target_names=class_names))\n",
        "\n",
        "    # Log numeric metrics\n",
        "    log_data = {\n",
        "        \"Final_test_accuracy\": acc,\n",
        "        \"Final_test_f1_macro\": f1_macro,\n",
        "    }\n",
        "    if class_names:\n",
        "        for i, name in enumerate(class_names):\n",
        "            log_data[f\"f1_{name}\"] = f1_per_class[i]\n",
        "    wandb.log(log_data)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        xticklabels=class_names if class_names else \"auto\",\n",
        "        yticklabels=class_names if class_names else \"auto\",\n",
        "        cmap=\"Blues\"\n",
        "    )\n",
        "    ax.set_xlabel(\"Final_Predicted\")\n",
        "    ax.set_ylabel(\"Final_Actual\")\n",
        "    ax.set_title(\"Final_Confusion Matrix (Test)\")\n",
        "    # wandb.log({\n",
        "    #     \"confusion_matrix_final\": wandb.Image(fig),\n",
        "    #     \"confusion_matrix_raw\": wandb.Table(data=cm.tolist(), columns=class_names, rows=class_names),\n",
        "    # })\n",
        "    plt.close(fig)\n",
        "\n",
        "    wandb.log({\n",
        "    \"Final_confusion_matrix_final\": wandb.Image(fig),\n",
        "    \"Final_confusion_matrix_raw\": wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=all_targets,\n",
        "        preds=all_preds,\n",
        "        class_names=class_names if class_names else None\n",
        "    )\n",
        "})\n",
        "    # Log a few misclassified images\n",
        "    if class_names:\n",
        "        wrong_table = wandb.Table(columns=[\"Image\", \"True Label\", \"Predicted Label\"])\n",
        "        for img, true_lbl, pred_lbl in misclassified_images[:25]:\n",
        "            img_np = img.squeeze().numpy()  # assumes grayscale\n",
        "            wrong_table.add_data(\n",
        "                wandb.Image(img_np, caption=f\"Pred: {class_names[pred_lbl]}, GT: {class_names[true_lbl]}\"),\n",
        "                class_names[true_lbl],\n",
        "                class_names[pred_lbl]\n",
        "            )\n",
        "        wandb.log({\"Final_misclassified_samples\": wrong_table})\n",
        "\n",
        "    # Export ONNX\n",
        "    dummy_input = torch.randn(1, *images.shape[1:]).to(device)\n",
        "    # torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "    # wandb.save(\"model.onnx\")\n",
        "\n",
        "    # Optionally log model architecture as a string\n",
        "    model_arch_str = str(model)\n",
        "    wandb.log({\"Final_model_architecture\": wandb.Html(f\"<pre>{model_arch_str}</pre>\")})"
      ],
      "metadata": {
        "id": "F3VQ1HxlUk6V"
      },
      "id": "F3VQ1HxlUk6V",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipeline"
      ],
      "metadata": {
        "id": "UU0HM605Auy7"
      },
      "id": "UU0HM605Auy7"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"expression_dataset_final\",\n",
        "                    config=hyperparameters,\n",
        "                    name = \"resnet_og\"):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer, final_test_loader = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      # train(model, train_loader, criterion, optimizer, config)\n",
        "      wandb.watch(model, log=\"all\", log_freq=100)\n",
        "      # # and test its final performance\n",
        "      # test(model, test_loader)\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, config)\n",
        "      test(model, test_loader, config.device)  # final test; you can use actual test set here if available\n",
        "      Final_test(model, final_test_loader,config.device)\n",
        "\n",
        "    wandb.finish()\n",
        "    return model"
      ],
      "metadata": {
        "id": "cEsYfYCkUbYw"
      },
      "id": "cEsYfYCkUbYw",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=20,\n",
        "    classes=7,\n",
        "    #kernels=[32, 64, 128],\n",
        "    batch_size=256,\n",
        "    learning_rate=.3e-4,\n",
        "    dataset=\"Facial Expression Recognition\",\n",
        "    architecture=\"resnet_og\",\n",
        "    device = device,\n",
        "    weight_decay = 1e-4)"
      ],
      "metadata": {
        "id": "GoXMnqIWY9fa"
      },
      "id": "GoXMnqIWY9fa",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "train_dataset = get_data(train=True)\n",
        "test_dataset = get_data(train=False)\n",
        "train_loader = make_loader(train_dataset, batch_size=config['batch_size'])\n",
        "test_loader = make_loader(test_dataset, batch_size=config['batch_size'])\n",
        "for batch in train_loader:\n",
        "    images, labels = batch\n",
        "    total += len(images)\n",
        "# Count class occurrences\n",
        "all_labels = []\n",
        "for _, labels in train_loader:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "label_counts = Counter(all_labels)\n",
        "\n",
        "# Print counts\n",
        "print(\"Label distribution in augmented training data:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "# Optional: plot for visual confirmation\n",
        "plt.bar(label_counts.keys(), label_counts.values(), tick_label=[str(i) for i in label_counts.keys()])\n",
        "plt.xlabel('Emotion Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution After Augmentation')\n",
        "plt.show()\n",
        "print(f\"Total images seen via train_loader: {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "uTtkeP3VhRZO",
        "outputId": "1a6b68d6-d5b3-4ed5-d260-64f47639f2db"
      },
      "id": "uTtkeP3VhRZO",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "Label distribution in augmented training data:\n",
            "Class 0: 7215 samples\n",
            "Class 1: 7215 samples\n",
            "Class 2: 7215 samples\n",
            "Class 3: 7215 samples\n",
            "Class 4: 7215 samples\n",
            "Class 5: 7215 samples\n",
            "Class 6: 7215 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images seen via train_loader: 50505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train!"
      ],
      "metadata": {
        "id": "MQvkFnUNxXls"
      },
      "id": "MQvkFnUNxXls"
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MifXx_iuXfLi",
        "outputId": "7b755ec7-89bb-4462-9bf0-06c1a9b4d19f"
      },
      "id": "MifXx_iuXfLi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_183040-t2g05zn6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/t2g05zn6' target=\"_blank\">resnet_og</a></strong> to <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/t2g05zn6' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_final/runs/t2g05zn6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Loaded 'Training' => 28709 samples before augmentation/slicing.\n",
            "‣ Base dataset size: 28709\n",
            "‣ Augmenting 21796 extra samples to balance classes.\n",
            "‣ Resulting dataset size: 50505\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "‣ Loaded 'PublicTest' => 3589 samples before augmentation/slicing.\n",
            "ResNet(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (maxpool): Identity()\n",
            "  (layer1): Sequential(\n",
            "    (0): block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): block(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): block(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): block(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): block(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): block(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): block(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): block(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=7, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 198/198 [04:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20: Train Loss=1.7717, Train Acc=0.2816 | Val Loss=1.7700, Val Acc=0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]:  41%|████▏     | 82/198 [01:40<02:21,  1.22s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "PhusPwNHkvAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62f87b2-f01c-4699-f6d4-2348a2515f19"
      },
      "id": "PhusPwNHkvAx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}