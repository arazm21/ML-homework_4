{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arazm21/ML-homework_4/blob/main/expression_notebook_second.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the data and organising it"
      ],
      "metadata": {
        "id": "ShlkPaeoBQ3k"
      },
      "id": "ShlkPaeoBQ3k"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNNvBwljBQFE",
        "outputId": "5abe62b9-1fab-49bc-ecf3-bfca950e1e79"
      },
      "id": "dNNvBwljBQFE",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 93% 266M/285M [00:00<00:00, 519MB/s]\n",
            "100% 285M/285M [00:00<00:00, 565MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fXPkihKllSZ",
        "outputId": "8bc4f436-7346-49f9-f7c0-3472cdeeb0e7"
      },
      "id": "9fXPkihKllSZ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xib5nVmSLH0o",
        "outputId": "d8c82537-14c5-4ab3-cedb-19239ad3ea26"
      },
      "id": "xib5nVmSLH0o",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5Ptu8H4Lzx6"
      },
      "id": "b5Ptu8H4Lzx6",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "class ExpressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file, indices=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        if indices is not None:\n",
        "            self.data = self.data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "        self.images = self.data['pixels'].apply(\n",
        "            lambda x: np.fromstring(x, sep=' ', dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        self.images = torch.tensor(np.stack(self.images.values), dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(self.data['emotion'].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def get_data(csv_file=\"train.csv\", slice=5, train=True, val_ratio=0.2, random_state=42):\n",
        "    # Load full train.csv data\n",
        "    full_data = pd.read_csv(csv_file)\n",
        "    indices = list(range(len(full_data)))\n",
        "\n",
        "    # Stratified split indices for train/validation\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=full_data['emotion'],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Select which indices to use\n",
        "    selected_indices = train_indices if train else val_indices\n",
        "\n",
        "    # Create dataset with selected indices\n",
        "    dataset = ExpressionDataset(csv_file, indices=selected_indices)\n",
        "\n",
        "    # Slice dataset if requested\n",
        "    sliced_indices = list(range(0, len(dataset), slice))\n",
        "    return Subset(dataset, sliced_indices)\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True,\n",
        "                        num_workers=2)\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "D1ye0F1iHqSC"
      },
      "id": "D1ye0F1iHqSC",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test that loading was ok"
      ],
      "metadata": {
        "id": "RsFhOzV_PHxI"
      },
      "id": "RsFhOzV_PHxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and create loader\n",
        "dataset = get_data(slice=1, train=False)\n",
        "loader = make_loader(dataset, batch_size=3)\n",
        "\n",
        "# Get a batch\n",
        "images, labels = next(iter(loader))\n",
        "\n",
        "# Class names from FER2013\n",
        "emotion_names = [\n",
        "    \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"\n",
        "]\n",
        "\n",
        "# Plot the first 3 images\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    plt.title(f\"Label: {emotion_names[labels[i].item()]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "NvgplaWYHJX6",
        "outputId": "ee7a3aa0-3b87-42ef-9224-4f1e9a3e08cc"
      },
      "id": "NvgplaWYHJX6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connecting WANDB"
      ],
      "metadata": {
        "id": "XhwXYgieSf7u"
      },
      "id": "XhwXYgieSf7u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "zt2tymF2Se4p"
      },
      "id": "zt2tymF2Se4p",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "l2Op72b3SsY1",
        "outputId": "60454c9c-205c-4bb0-bf13-3aca4ab9aa11"
      },
      "id": "l2Op72b3SsY1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marazm21\u001b[0m (\u001b[33marazm21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# different attempted architectures"
      ],
      "metadata": {
        "id": "FjvVv9RPXFbN"
      },
      "id": "FjvVv9RPXFbN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convnet"
      ],
      "metadata": {
        "id": "jjCDTHA1RHhj"
      },
      "id": "jjCDTHA1RHhj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# class ConvNet_super_simple(nn.Module):\n",
        "#     def __init__(self, kernels, classes=7):\n",
        "#         super(ConvNet_super_simple, self).__init__()\n",
        "\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         # Assuming 48x48 input, after two 2x2 poolings -> 48/2/2 = 12x12\n",
        "#         self.fc = nn.Linear(12 * 12 * kernels[1], classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet_Improved(nn.Module):\n",
        "    def __init__(self, kernels, classes=7):\n",
        "        super(ConvNet_Improved, self).__init__()\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(1, kernels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(kernels[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(kernels[0], kernels[1], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(kernels[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(kernels[1], kernels[2], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(kernels[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Assuming 48x48 input → 3 poolings: 48 → 24 → 12 → 6\n",
        "        self.flattened_dim = 6 * 6 * kernels[2]\n",
        "        self.fc = nn.Linear(self.flattened_dim, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.dropout1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "        x = self.pool2(self.dropout2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "        x = self.pool3(self.dropout3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RaFvctrZTf1t"
      },
      "id": "RaFvctrZTf1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "cumrZOH9Sj1A"
      },
      "id": "cumrZOH9Sj1A"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout(out)  # ✅ Dropout after residual addition\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SimpleResNet15(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.34):\n",
        "        super(SimpleResNet15, self).__init__()\n",
        "\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            ResidualBlock(64, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(1024, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(2048, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(1024, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "            ResidualBlock(512, 256, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(256, 128, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(128, 64, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(64, 32, downsample=False, dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "PFYKchqhXTSt"
      },
      "id": "PFYKchqhXTSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## googlenet(mini)"
      ],
      "metadata": {
        "id": "ltk-bzVKSnDU"
      },
      "id": "ltk-bzVKSnDU"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class MiniInception(nn.Module):\n",
        "#     def __init__(self, in_ch, c1, c3red, c3, pool_proj):\n",
        "#         super().__init__()\n",
        "#         # 1×1 branch\n",
        "#         self.b1 = nn.Conv2d(in_ch, c1, kernel_size=1)\n",
        "#         # 1×1 → 3×3 branch\n",
        "#         self.b2_1 = nn.Conv2d(in_ch, c3red, kernel_size=1)\n",
        "#         self.b2_2 = nn.Conv2d(c3red, c3,   kernel_size=3, padding=1)\n",
        "#         # pool → 1×1 branch\n",
        "#         self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "#         self.b3_proj = nn.Conv2d(in_ch, pool_proj, kernel_size=1)\n",
        "#         self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b1 = self.b1(x)\n",
        "#         b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "#         b3 = self.b3_proj(self.b3_pool(x))\n",
        "#         out = torch.cat([b1, b2, b3], dim=1)\n",
        "#         return F.relu(self.bn(out))\n",
        "\n",
        "\n",
        "# class MiniGoogLeNet(nn.Module):\n",
        "#     def __init__(self, num_classes=7, aux_on=True):\n",
        "#         super().__init__()\n",
        "#         self.aux_on = aux_on\n",
        "\n",
        "#         # ---- stem ----\n",
        "#         self.stem = nn.Sequential(\n",
        "#             nn.Conv2d(1, 32, 3, padding=1),\n",
        "#             nn.BatchNorm2d(32), nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, 2)  # 48→24\n",
        "#         )\n",
        "\n",
        "#         # ---- two Inception blocks ----\n",
        "#         self.inc1 = MiniInception(32, c1=16, c3red=16, c3=24, pool_proj=16)  # outputs 56\n",
        "#         self.inc2 = MiniInception(56, c1=32, c3red=24, c3=32, pool_proj=24)  # outputs 88\n",
        "\n",
        "#         # auxiliary head (after inc1)\n",
        "#         if aux_on:\n",
        "#             self.aux = nn.Sequential(\n",
        "#                 nn.AdaptiveAvgPool2d((4,4)),\n",
        "#                 nn.Conv2d(56, 32, 1), nn.ReLU(),\n",
        "#                 nn.Flatten(),\n",
        "#                 nn.Linear(32*4*4, 128), nn.ReLU(), nn.Dropout(0.5),\n",
        "#                 nn.Linear(128, num_classes)\n",
        "#             )\n",
        "\n",
        "#         # ---- classifier head ----\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "#         self.fc   = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.Linear(88, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.stem(x)            # → [B,32,24,24]\n",
        "#         x1 = self.inc1(x)           # → [B,56,24,24]\n",
        "\n",
        "#         # you can still compute aux_out if you want,\n",
        "#         # but we won't return it so training code stays unchanged\n",
        "#         if self.training and self.aux_on:\n",
        "#             _ = self.aux(x1)\n",
        "\n",
        "#         x2 = F.max_pool2d(x1, 2, 2) # → [B,56,12,12]\n",
        "#         x2 = self.inc2(x2)          # → [B,88,12,12]\n",
        "\n",
        "#         x3 = self.pool(x2)          # → [B,88,1,1]\n",
        "#         main_out = self.fc(x3)      # → [B,num_classes]\n",
        "\n",
        "#         return main_out\n",
        "class MiniInception(nn.Module):\n",
        "    def __init__(self, in_ch, c1, c3red, c3, pool_proj, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 1×1 branch\n",
        "        self.b1 = nn.Conv2d(in_ch, c1, 1)\n",
        "        # 1×1 → 3×3 branch\n",
        "        self.b2_1 = nn.Conv2d(in_ch, c3red, 1)\n",
        "        self.b2_2 = nn.Conv2d(c3red, c3, 3, padding=1)\n",
        "        # pool → 1×1 branch\n",
        "        self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.b3_proj = nn.Conv2d(in_ch, pool_proj, 1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "        b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "        b3 = self.b3_proj(self.b3_pool(x))\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class ComplexMiniGoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # ── Stem ──\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 48 → 24\n",
        "        )\n",
        "\n",
        "        # ── Inception Blocks ──\n",
        "        self.inc1 = MiniInception(32, c1=16, c3red=16, c3=32, pool_proj=16, dropout=0.2)   # →64 ch\n",
        "        self.inc2 = MiniInception(64, c1=24, c3red=24, c3=48, pool_proj=24, dropout=0.2)   # →96 ch\n",
        "        self.inc3 = MiniInception(96, c1=32, c3red=32, c3=64, pool_proj=32, dropout=0.3)   # →128 ch\n",
        "\n",
        "        # ── Auxiliary head (computed but not returned) ──\n",
        "        if aux_on:\n",
        "            self.aux = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d((4, 4)),\n",
        "                nn.Conv2d(96, 48, 1), nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(48 * 4 * 4, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "\n",
        "        # ── Final classifier ──\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)          # → [B,32,24,24]\n",
        "        x1 = self.inc1(x)         # → [B,64,24,24]\n",
        "        x2 = self.inc2(x1)        # → [B,96,24,24]\n",
        "\n",
        "        # compute aux but ignore its output\n",
        "        if self.training and self.aux_on:\n",
        "            _ = self.aux(x2)\n",
        "\n",
        "        x3 = F.max_pool2d(x2, 2, 2)  # → [B,96,12,12]\n",
        "        x3 = self.inc3(x3)           # → [B,128,12,12]\n",
        "\n",
        "        x4 = self.final_pool(x3)     # → [B,128,1,1]\n",
        "        main_out = self.classifier(x4)\n",
        "        return main_out"
      ],
      "metadata": {
        "id": "2ydG-CJ4SsQ_"
      },
      "id": "2ydG-CJ4SsQ_",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## densenet"
      ],
      "metadata": {
        "id": "IH2tbOH_Hd8Z"
      },
      "id": "IH2tbOH_Hd8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        return torch.cat([x, out], dim=1)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(DenseLayer(in_channels, growth_rate))\n",
        "            in_channels += growth_rate\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.relu(self.bn(x)))\n",
        "        return self.pool(x)\n",
        "\n",
        "class MiniDenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 2 * growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        num_channels = 2 * growth_rate\n",
        "\n",
        "        self.block1 = DenseBlock(num_channels, growth_rate, num_layers=4)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans1 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.block2 = DenseBlock(num_channels, growth_rate, num_layers=4)\n",
        "        num_channels += 4 * growth_rate\n",
        "        self.trans2 = TransitionLayer(num_channels, num_channels // 2)\n",
        "        num_channels = num_channels // 2\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(num_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(self.block1(x))\n",
        "        x = self.trans2(self.block2(x))\n",
        "        x = self.relu(self.bn(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "s_2N0muZHftN"
      },
      "id": "s_2N0muZHftN",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vision transformer"
      ],
      "metadata": {
        "id": "LIri0mc2NmsS"
      },
      "id": "LIri0mc2NmsS"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import repeat\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=1, patch_size=8, emb_size=128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class ExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True, img_size=48, patch_size=8,\n",
        "                 emb_dim=128, n_layers=6, dropout=0.1, heads=4):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head (after layer n_layers//2, similar to your original design)\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after middle layer)\n",
        "            if i == self.n_layers // 2 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]  # Extract cls token\n",
        "                _ = self.aux_head(aux_cls_token)  # Compute but don't return\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "\n",
        "class CompactExpressionViT(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # Compact configuration for faster training\n",
        "        img_size = 48\n",
        "        patch_size = 6\n",
        "        emb_dim = 96\n",
        "        n_layers = 4\n",
        "        heads = 3\n",
        "        dropout = 0.2\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=1,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2  # 16 patches\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads=heads, dropout=dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim * 2, dropout=dropout)))\n",
        "            )\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Auxiliary head\n",
        "        if aux_on:\n",
        "            self.aux_head = nn.Sequential(\n",
        "                nn.LayerNorm(emb_dim),\n",
        "                nn.Linear(emb_dim, emb_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(emb_dim // 2, num_classes)\n",
        "            )\n",
        "\n",
        "        # Main classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Auxiliary output (after 2nd layer for compact model)\n",
        "            if i == 1 and self.training and self.aux_on:\n",
        "                aux_cls_token = x[:, 0, :]\n",
        "                _ = self.aux_head(aux_cls_token)\n",
        "\n",
        "        # Main output based on classification token\n",
        "        main_out = self.head(x[:, 0, :])\n",
        "        return main_out\n",
        "\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 1, patch_size = 8, emb_size = 128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "from einops import repeat\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, ch=1, img_size=48, patch_size=4, emb_dim=64,\n",
        "                n_layers=12, out_dim=7, dropout=0.2, heads=8):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.channels = ch\n",
        "        self.height = img_size\n",
        "        self.width = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "        # Output based on classification token\n",
        "        return self.head(x[:, 0, :])\n",
        "\n"
      ],
      "metadata": {
        "id": "_2nPvP7QNlqo"
      },
      "id": "_2nPvP7QNlqo",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def ViT(cf):\n",
        "    inputs = Input(shape=cf[\"input_shape\"])\n",
        "    patches = Patches(cf[\"patch_size\"])(inputs)\n",
        "    x = PatchEncoder(num_patches=cf[\"num_patches\"], projection_dim=cf[\"projection_dim\"])(patches)\n",
        "    cls_token = ClassToken()(x)\n",
        "    x = Concatenate(axis=1)([cls_token, x])\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = x[:, 0]\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "IzpTX__egM29"
      },
      "id": "IzpTX__egM29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# geting everything ready"
      ],
      "metadata": {
        "id": "zaVbntQmXODJ"
      },
      "id": "zaVbntQmXODJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train_dataset = get_data(train=True)\n",
        "    test_dataset = get_data(train=False)\n",
        "    train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = ViT().to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "4GOnNEKyT1v2"
      },
      "id": "4GOnNEKyT1v2",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "Jwbd9gvXUp4R"
      },
      "id": "Jwbd9gvXUp4R",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## early stop training"
      ],
      "metadata": {
        "id": "6Vih9mc96ns6"
      },
      "id": "6Vih9mc96ns6"
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=15, min_delta=0.001)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            loss, batch_correct, batch_total = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            running_correct += batch_correct\n",
        "            running_total += batch_total\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        train_acc = running_correct / running_total\n",
        "\n",
        "        # ⏱️ Validate at the end of the epoch\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_accuracy\": train_acc\n",
        "        })\n",
        "        print(f\"Epoch {epoch + 1}: val_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}, train_acc = {train_acc:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopper(val_loss)\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "\n",
        "    return loss, correct, total"
      ],
      "metadata": {
        "id": "2gDrRJB56ihi"
      },
      "id": "2gDrRJB56ihi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normal training"
      ],
      "metadata": {
        "id": "Ts0xewao6rsq"
      },
      "id": "Ts0xewao6rsq"
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(model, loader, criterion, optimizer, config):\n",
        "#     # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "#     # Run training and track with wandb\n",
        "#     total_batches = len(loader) * config.epochs\n",
        "#     example_ct = 0  # number of examples seen\n",
        "#     batch_ct = 0\n",
        "#     for epoch in tqdm(range(config.epochs)):\n",
        "#         for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "#             loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "#             example_ct +=  len(images)\n",
        "#             batch_ct += 1\n",
        "\n",
        "#             # Report metrics every 25th batch\n",
        "#             if ((batch_ct + 1) % 25) == 0:\n",
        "#                 train_log(loss, example_ct, epoch)\n",
        "#                 print(f\"batch number: {batch_ct + 1}\")\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        train_loss_accumulator = 0.0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Accumulate loss for the epoch\n",
        "            train_loss_accumulator += loss.item() * images.size(0)\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        # Final training metrics for the epoch\n",
        "        train_loss = train_loss_accumulator / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # ⏱️ Validation step\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "\n",
        "        # Log both train & val metrics\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: \"\n",
        "              f\"train_loss = {train_loss:.4f}, train_accuracy = {train_acc:.4f}, \"\n",
        "              f\"val_loss = {val_loss:.4f}, val_accuracy = {val_acc:.4f}\")\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "F3VQ1HxlUk6V"
      },
      "id": "F3VQ1HxlUk6V",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"expression_dataset_better_eval\",\n",
        "                    config=hyperparameters,\n",
        "                    name = \"ViT_bigger\"):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      # train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # # and test its final performance\n",
        "      # test(model, test_loader)\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, config)\n",
        "      test(model, test_loader)  # final test; you can use actual test set here if available\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "cEsYfYCkUbYw"
      },
      "id": "cEsYfYCkUbYw",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=250,\n",
        "    classes=7,\n",
        "    #kernels=[32, 64, 128],\n",
        "    batch_size=256,\n",
        "    learning_rate=2e-4,\n",
        "    dataset=\"Facial Expression Recognition\",\n",
        "    architecture=\"ViT_bigger\")"
      ],
      "metadata": {
        "id": "GoXMnqIWY9fa"
      },
      "id": "GoXMnqIWY9fa",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train!"
      ],
      "metadata": {
        "id": "MQvkFnUNxXls"
      },
      "id": "MQvkFnUNxXls"
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "294a86fc8faa4e87864a5fac1d372158",
            "219070ce13514230bec9f0251a3c2d7b",
            "29b7bd96d67d4c7db504e32a96ffd657",
            "5ad7bc3c2b7141bd9b4b83e766f2ed9b",
            "6b09d2025d4d4ee4a1e3bd4da5ee1414",
            "92dd2762d9854786b7db5f5b755732e9",
            "29dfc2ddb0f94292bb35bd4623b08027",
            "956287710db64601b8af10d855b8d42b",
            "bd50ad8ae56b4d4994c70f31eaa86854",
            "cae25c99cf374fd09cd12e492508ef7f",
            "e3ecffb41d77417aad2d1f61a3e2aba7"
          ]
        },
        "id": "MifXx_iuXfLi",
        "outputId": "dad1ac09-1d5b-49c8-9790-d0c97076ecc1"
      },
      "id": "MifXx_iuXfLi",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250528_141928-1xse4j9n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/1xse4j9n' target=\"_blank\">ViT_bigger</a></strong> to <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/1xse4j9n' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/1xse4j9n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT(\n",
            "  (patch_embedding): PatchEmbedding(\n",
            "    (projection): Sequential(\n",
            "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
            "      (1): Linear(in_features=16, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x Sequential(\n",
            "      (0): ResidualAdd(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (att): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "            )\n",
            "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualAdd(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.2, inplace=False)\n",
            "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (4): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=64, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "294a86fc8faa4e87864a5fac1d372158"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss = 1.8603, train_accuracy = 0.2249, val_loss = 1.8223, val_accuracy = 0.2341\n",
            "Loss after 06130 examples: 1.829\n",
            "batch number: 25\n",
            "Epoch 2: train_loss = 1.8214, train_accuracy = 0.2399, val_loss = 1.8267, val_accuracy = 0.2341\n",
            "Loss after 12516 examples: 1.831\n",
            "batch number: 50\n",
            "Epoch 3: train_loss = 1.8257, train_accuracy = 0.2427, val_loss = 1.8283, val_accuracy = 0.2341\n",
            "Epoch 4: train_loss = 1.8198, train_accuracy = 0.2473, val_loss = 1.8214, val_accuracy = 0.2341\n",
            "Loss after 18888 examples: 1.797\n",
            "batch number: 75\n",
            "Epoch 5: train_loss = 1.8189, train_accuracy = 0.2479, val_loss = 1.8207, val_accuracy = 0.2341\n",
            "Loss after 25274 examples: 1.825\n",
            "batch number: 100\n",
            "Epoch 6: train_loss = 1.8190, train_accuracy = 0.2471, val_loss = 1.8225, val_accuracy = 0.2341\n",
            "Loss after 31660 examples: 1.854\n",
            "batch number: 125\n",
            "Epoch 7: train_loss = 1.8193, train_accuracy = 0.2481, val_loss = 1.8205, val_accuracy = 0.2341\n",
            "Epoch 8: train_loss = 1.8208, train_accuracy = 0.2475, val_loss = 1.8222, val_accuracy = 0.2341\n",
            "Loss after 38032 examples: 1.796\n",
            "batch number: 150\n",
            "Epoch 9: train_loss = 1.8182, train_accuracy = 0.2440, val_loss = 1.8207, val_accuracy = 0.2341\n",
            "Loss after 44418 examples: 1.815\n",
            "batch number: 175\n",
            "Epoch 10: train_loss = 1.8189, train_accuracy = 0.2477, val_loss = 1.8207, val_accuracy = 0.2341\n",
            "Epoch 11: train_loss = 1.8209, train_accuracy = 0.2425, val_loss = 1.8263, val_accuracy = 0.2341\n",
            "Loss after 50790 examples: 1.821\n",
            "batch number: 200\n",
            "Epoch 12: train_loss = 1.8190, train_accuracy = 0.2462, val_loss = 1.8258, val_accuracy = 0.2341\n",
            "Loss after 57176 examples: 1.802\n",
            "batch number: 225\n",
            "Epoch 13: train_loss = 1.8170, train_accuracy = 0.2464, val_loss = 1.8236, val_accuracy = 0.2341\n",
            "Loss after 63562 examples: 1.822\n",
            "batch number: 250\n",
            "Epoch 14: train_loss = 1.8184, train_accuracy = 0.2481, val_loss = 1.8205, val_accuracy = 0.2341\n",
            "Epoch 15: train_loss = 1.8173, train_accuracy = 0.2486, val_loss = 1.8220, val_accuracy = 0.2341\n",
            "Loss after 69934 examples: 1.833\n",
            "batch number: 275\n",
            "Epoch 16: train_loss = 1.8166, train_accuracy = 0.2475, val_loss = 1.8242, val_accuracy = 0.2341\n",
            "Loss after 76320 examples: 1.783\n",
            "batch number: 300\n",
            "Epoch 17: train_loss = 1.8170, train_accuracy = 0.2486, val_loss = 1.8218, val_accuracy = 0.2341\n",
            "Loss after 82692 examples: 1.824\n",
            "batch number: 325\n",
            "Epoch 18: train_loss = 1.8197, train_accuracy = 0.2473, val_loss = 1.8215, val_accuracy = 0.2341\n",
            "Epoch 19: train_loss = 1.8166, train_accuracy = 0.2481, val_loss = 1.8206, val_accuracy = 0.2341\n",
            "Loss after 89078 examples: 1.816\n",
            "batch number: 350\n",
            "Epoch 20: train_loss = 1.8144, train_accuracy = 0.2486, val_loss = 1.8207, val_accuracy = 0.2341\n",
            "Loss after 95464 examples: 1.813\n",
            "batch number: 375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-35-57e8d0ca28dd>\", line 19, in model_pipeline\n",
            "    train(model, train_loader, test_loader, criterion, optimizer, config)\n",
            "  File \"<ipython-input-28-12b2cee27e64>\", line 66, in train\n",
            "    train_correct += (predicted == labels).sum().item()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>▆▆▂▅█▂▄▅▃▅▆▁▅▄▄</td></tr><tr><td>train_accuracy</td><td>▁▅▆█████▇█▆▇▇███████</td></tr><tr><td>train_loss</td><td>█▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▇█▂▁▃▁▃▁▁▆▆▄▁▂▄▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>loss</td><td>1.8126</td></tr><tr><td>train_accuracy</td><td>0.24859</td></tr><tr><td>train_loss</td><td>1.81445</td></tr><tr><td>val_accuracy</td><td>0.23412</td></tr><tr><td>val_loss</td><td>1.82066</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ViT_bigger</strong> at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/1xse4j9n' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/1xse4j9n</a><br> View project at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250528_141928-1xse4j9n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9807c214ebcc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-57e8d0ca28dd>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# # and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# test(model, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# final test; you can use actual test set here if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-12b2cee27e64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Accumulate loss for the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "PhusPwNHkvAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62f87b2-f01c-4699-f6d4-2348a2515f19"
      },
      "id": "PhusPwNHkvAx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "294a86fc8faa4e87864a5fac1d372158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_219070ce13514230bec9f0251a3c2d7b",
              "IPY_MODEL_29b7bd96d67d4c7db504e32a96ffd657",
              "IPY_MODEL_5ad7bc3c2b7141bd9b4b83e766f2ed9b"
            ],
            "layout": "IPY_MODEL_6b09d2025d4d4ee4a1e3bd4da5ee1414"
          }
        },
        "219070ce13514230bec9f0251a3c2d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92dd2762d9854786b7db5f5b755732e9",
            "placeholder": "​",
            "style": "IPY_MODEL_29dfc2ddb0f94292bb35bd4623b08027",
            "value": "  8%"
          }
        },
        "29b7bd96d67d4c7db504e32a96ffd657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_956287710db64601b8af10d855b8d42b",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd50ad8ae56b4d4994c70f31eaa86854",
            "value": 20
          }
        },
        "5ad7bc3c2b7141bd9b4b83e766f2ed9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae25c99cf374fd09cd12e492508ef7f",
            "placeholder": "​",
            "style": "IPY_MODEL_e3ecffb41d77417aad2d1f61a3e2aba7",
            "value": " 20/250 [03:26&lt;38:04,  9.93s/it]"
          }
        },
        "6b09d2025d4d4ee4a1e3bd4da5ee1414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92dd2762d9854786b7db5f5b755732e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29dfc2ddb0f94292bb35bd4623b08027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "956287710db64601b8af10d855b8d42b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd50ad8ae56b4d4994c70f31eaa86854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cae25c99cf374fd09cd12e492508ef7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ecffb41d77417aad2d1f61a3e2aba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}