{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arazm21/ML-homework_4/blob/main/expression_notebook_second.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the data and organising it"
      ],
      "metadata": {
        "id": "ShlkPaeoBQ3k"
      },
      "id": "ShlkPaeoBQ3k"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNNvBwljBQFE",
        "outputId": "e7a26d6e-fc34-4162-8051-f6ab048ee64d"
      },
      "id": "dNNvBwljBQFE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 76% 218M/285M [00:00<00:00, 571MB/s] \n",
            "100% 285M/285M [00:03<00:00, 75.3MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fXPkihKllSZ",
        "outputId": "cb6759a9-8e44-4357-aa1a-ecf6daa8f126"
      },
      "id": "9fXPkihKllSZ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xib5nVmSLH0o",
        "outputId": "38116e93-fe30-4f9d-81f4-c25bc2735ae9"
      },
      "id": "xib5nVmSLH0o",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5Ptu8H4Lzx6"
      },
      "id": "b5Ptu8H4Lzx6",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "class ExpressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file, indices=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        if indices is not None:\n",
        "            self.data = self.data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "        self.images = self.data['pixels'].apply(\n",
        "            lambda x: np.fromstring(x, sep=' ', dtype=np.uint8).reshape(48, 48)\n",
        "        )\n",
        "        self.images = torch.tensor(np.stack(self.images.values), dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "        self.labels = torch.tensor(self.data['emotion'].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def get_data(csv_file=\"train.csv\", slice=5, train=True, val_ratio=0.2, random_state=42):\n",
        "    # Load full train.csv data\n",
        "    full_data = pd.read_csv(csv_file)\n",
        "    indices = list(range(len(full_data)))\n",
        "\n",
        "    # Stratified split indices for train/validation\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=full_data['emotion'],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Select which indices to use\n",
        "    selected_indices = train_indices if train else val_indices\n",
        "\n",
        "    # Create dataset with selected indices\n",
        "    dataset = ExpressionDataset(csv_file, indices=selected_indices)\n",
        "\n",
        "    # Slice dataset if requested\n",
        "    sliced_indices = list(range(0, len(dataset), slice))\n",
        "    return Subset(dataset, sliced_indices)\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True,\n",
        "                        num_workers=2)\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "D1ye0F1iHqSC"
      },
      "id": "D1ye0F1iHqSC",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test that loading was ok"
      ],
      "metadata": {
        "id": "RsFhOzV_PHxI"
      },
      "id": "RsFhOzV_PHxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and create loader\n",
        "dataset = get_data(slice=1, train=False)\n",
        "loader = make_loader(dataset, batch_size=3)\n",
        "\n",
        "# Get a batch\n",
        "images, labels = next(iter(loader))\n",
        "\n",
        "# Class names from FER2013\n",
        "emotion_names = [\n",
        "    \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"\n",
        "]\n",
        "\n",
        "# Plot the first 3 images\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    plt.title(f\"Label: {emotion_names[labels[i].item()]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "NvgplaWYHJX6",
        "outputId": "d0f6ae13-dc17-485e-d1c0-f719142cb26f"
      },
      "id": "NvgplaWYHJX6",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connecting WANDB"
      ],
      "metadata": {
        "id": "XhwXYgieSf7u"
      },
      "id": "XhwXYgieSf7u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "zt2tymF2Se4p"
      },
      "id": "zt2tymF2Se4p",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "l2Op72b3SsY1",
        "outputId": "2d8014ab-a534-4a36-e654-259aef7e8e0a"
      },
      "id": "l2Op72b3SsY1",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marazm21\u001b[0m (\u001b[33marazm21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymLc1hz_XE5W"
      },
      "id": "ymLc1hz_XE5W",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# different attempted architectures"
      ],
      "metadata": {
        "id": "FjvVv9RPXFbN"
      },
      "id": "FjvVv9RPXFbN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simple *convnet*"
      ],
      "metadata": {
        "id": "LAxPFOOx3TLj"
      },
      "id": "LAxPFOOx3TLj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# class ConvNet_super_simple(nn.Module):\n",
        "#     def __init__(self, kernels, classes=7):\n",
        "#         super(ConvNet_super_simple, self).__init__()\n",
        "\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         # Assuming 48x48 input, after two 2x2 poolings -> 48/2/2 = 12x12\n",
        "#         self.fc = nn.Linear(12 * 12 * kernels[1], classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet_Improved(nn.Module):\n",
        "    def __init__(self, kernels, classes=7):\n",
        "        super(ConvNet_Improved, self).__init__()\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(1, kernels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(kernels[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(kernels[0], kernels[1], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(kernels[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(kernels[1], kernels[2], kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(kernels[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Assuming 48x48 input → 3 poolings: 48 → 24 → 12 → 6\n",
        "        self.flattened_dim = 6 * 6 * kernels[2]\n",
        "        self.fc = nn.Linear(self.flattened_dim, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.dropout1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "        x = self.pool2(self.dropout2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "        x = self.pool3(self.dropout3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RaFvctrZTf1t"
      },
      "id": "RaFvctrZTf1t",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "cumrZOH9Sj1A"
      },
      "id": "cumrZOH9Sj1A"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        stride = 2 if downsample else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)  # ✅ Dropout after residual addition\n",
        "        return out\n",
        "\n",
        "class SimpleResNet15(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=7, dropout_rate=0.3):\n",
        "        super(SimpleResNet15, self).__init__()\n",
        "\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            ResidualBlock(32, 128, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(128, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(256, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(1024, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(2048, 2048, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(2048, 1024, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(1024, 512, downsample=True, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(512, 256, downsample=True, dropout_rate=dropout_rate),\n",
        "\n",
        "            ResidualBlock(512, 256, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(256, 128, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(128, 64, downsample=False, dropout_rate=dropout_rate),\n",
        "            ResidualBlock(64, 32, downsample=False, dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "PFYKchqhXTSt"
      },
      "id": "PFYKchqhXTSt",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## googlenet(mini)"
      ],
      "metadata": {
        "id": "ltk-bzVKSnDU"
      },
      "id": "ltk-bzVKSnDU"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiniInception(nn.Module):\n",
        "    def __init__(self, in_ch, c1, c3red, c3, pool_proj):\n",
        "        super().__init__()\n",
        "        # 1×1 branch\n",
        "        self.b1 = nn.Conv2d(in_ch, c1, kernel_size=1)\n",
        "        # 1×1 → 3×3 branch\n",
        "        self.b2_1 = nn.Conv2d(in_ch, c3red, kernel_size=1)\n",
        "        self.b2_2 = nn.Conv2d(c3red, c3,   kernel_size=3, padding=1)\n",
        "        # pool → 1×1 branch\n",
        "        self.b3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.b3_proj = nn.Conv2d(in_ch, pool_proj, kernel_size=1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1 + c3 + pool_proj)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "        b2 = self.b2_2(F.relu(self.b2_1(x)))\n",
        "        b3 = self.b3_proj(self.b3_pool(x))\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "        return F.relu(self.bn(out))\n",
        "\n",
        "\n",
        "class MiniGoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, aux_on=True):\n",
        "        super().__init__()\n",
        "        self.aux_on = aux_on\n",
        "\n",
        "        # ---- stem ----\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 48→24\n",
        "        )\n",
        "\n",
        "        # ---- two Inception blocks ----\n",
        "        self.inc1 = MiniInception(32, c1=16, c3red=16, c3=24, pool_proj=16)  # outputs 56\n",
        "        self.inc2 = MiniInception(56, c1=32, c3red=24, c3=32, pool_proj=24)  # outputs 88\n",
        "\n",
        "        # auxiliary head (after inc1)\n",
        "        if aux_on:\n",
        "            self.aux = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d( (4,4) ),\n",
        "                nn.Conv2d(56, 32, 1), nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(32*4*4, 128), nn.ReLU(), nn.Dropout(0.3),\n",
        "                nn.Linear(128, num_classes)\n",
        "            )\n",
        "\n",
        "        # ---- classifier head ----\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc   = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(88, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)            # → [B,32,24,24]\n",
        "        x1 = self.inc1(x)           # → [B,56,24,24]\n",
        "\n",
        "        if self.training and self.aux_on:\n",
        "            aux_out = self.aux(x1)\n",
        "\n",
        "        x2 = F.max_pool2d(x1, 2, 2) # → [B,56,12,12]\n",
        "        x2 = self.inc2(x2)          # → [B,88,12,12]\n",
        "\n",
        "        x3 = self.pool(x2)          # → [B,88,1,1]\n",
        "        main_out = self.fc(x3)      # → [B,num_classes]\n",
        "\n",
        "        return (main_out, aux_out) if (self.training and self.aux_on) else main_out\n"
      ],
      "metadata": {
        "id": "2ydG-CJ4SsQ_"
      },
      "id": "2ydG-CJ4SsQ_",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# geting everything ready"
      ],
      "metadata": {
        "id": "zaVbntQmXODJ"
      },
      "id": "zaVbntQmXODJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train_dataset = get_data(train=True)\n",
        "    test_dataset = get_data(train=False)\n",
        "    train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = SimpleResNet15().to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "4GOnNEKyT1v2"
      },
      "id": "4GOnNEKyT1v2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "Jwbd9gvXUp4R"
      },
      "id": "Jwbd9gvXUp4R",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## early stop training"
      ],
      "metadata": {
        "id": "6Vih9mc96ns6"
      },
      "id": "6Vih9mc96ns6"
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=15, min_delta=0.001)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            loss, batch_correct, batch_total = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            running_correct += batch_correct\n",
        "            running_total += batch_total\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        train_acc = running_correct / running_total\n",
        "\n",
        "        # ⏱️ Validate at the end of the epoch\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_accuracy\": train_acc\n",
        "        })\n",
        "        print(f\"Epoch {epoch + 1}: val_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}, train_acc = {train_acc:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopper(val_loss)\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "\n",
        "    return loss, correct, total"
      ],
      "metadata": {
        "id": "2gDrRJB56ihi"
      },
      "id": "2gDrRJB56ihi",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normal training"
      ],
      "metadata": {
        "id": "Ts0xewao6rsq"
      },
      "id": "Ts0xewao6rsq"
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(model, loader, criterion, optimizer, config):\n",
        "#     # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "#     # Run training and track with wandb\n",
        "#     total_batches = len(loader) * config.epochs\n",
        "#     example_ct = 0  # number of examples seen\n",
        "#     batch_ct = 0\n",
        "#     for epoch in tqdm(range(config.epochs)):\n",
        "#         for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "#             loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "#             example_ct +=  len(images)\n",
        "#             batch_ct += 1\n",
        "\n",
        "#             # Report metrics every 25th batch\n",
        "#             if ((batch_ct + 1) % 25) == 0:\n",
        "#                 train_log(loss, example_ct, epoch)\n",
        "#                 print(f\"batch number: {batch_ct + 1}\")\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        train_loss_accumulator = 0.0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            example_ct += len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Accumulate loss for the epoch\n",
        "            train_loss_accumulator += loss.item() * images.size(0)\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "                print(f\"batch number: {batch_ct + 1}\")\n",
        "\n",
        "        # Final training metrics for the epoch\n",
        "        train_loss = train_loss_accumulator / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # ⏱️ Validation step\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "\n",
        "        # Log both train & val metrics\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: \"\n",
        "              f\"train_loss = {train_loss:.4f}, train_accuracy = {train_acc:.4f}, \"\n",
        "              f\"val_loss = {val_loss:.4f}, val_accuracy = {val_acc:.4f}\")\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "F3VQ1HxlUk6V"
      },
      "id": "F3VQ1HxlUk6V",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test(model, test_loader):\n",
        "#     model.eval()\n",
        "\n",
        "#     # Run the model on some test examples\n",
        "#     with torch.no_grad():\n",
        "#         correct, total = 0, 0\n",
        "#         for images, labels in test_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         print(f\"Accuracy of the model on the {total} \" +\n",
        "#               f\"test images: {correct / total:%}\")\n",
        "\n",
        "#         wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "#     # Save the model in the exchangeable ONNX format\n",
        "#     torch.onnx.export(model, images, \"model.onnx\")\n",
        "#     wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "JYjjS-iHUuUG"
      },
      "id": "JYjjS-iHUuUG",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## building the pipeline"
      ],
      "metadata": {
        "id": "p-d74cgs4tSS"
      },
      "id": "p-d74cgs4tSS"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"expression_dataset_better_eval\",\n",
        "                    config=hyperparameters,\n",
        "                    name = \"SimpleResNet15\"):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      # train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # # and test its final performance\n",
        "      # test(model, test_loader)\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, config)\n",
        "      test(model, test_loader)  # final test; you can use actual test set here if available\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "cEsYfYCkUbYw"
      },
      "id": "cEsYfYCkUbYw",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=250,\n",
        "    classes=7,\n",
        "    #kernels=[32, 64, 128],\n",
        "    batch_size=256,\n",
        "    learning_rate=1e-4,\n",
        "    dataset=\"Facial Expression Recognition\",\n",
        "    architecture=\"SimpleResNet15\")"
      ],
      "metadata": {
        "id": "GoXMnqIWY9fa"
      },
      "id": "GoXMnqIWY9fa",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train!"
      ],
      "metadata": {
        "id": "MQvkFnUNxXls"
      },
      "id": "MQvkFnUNxXls"
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5ccd149db6554c459f48881c6d70e16b",
            "745d7737b43740049aff2edc40da28f2",
            "9d9f6035d6094347b7c8da077fd4172c",
            "e3f05b3d8ddb4fce8553df40b7bd7933",
            "eedd71b3b64d4333802c8d7b4afdb92d",
            "c75a7f1bba104864ad13098c827fdb32",
            "c6f3ceedf6354a529127bc2baa210b40",
            "ffbda0e9ab0249919c5a0d3530be3c5c",
            "2c9f166db7834252823f660694cf993c",
            "4ea4b236b56547d4949adeb73acebdc3",
            "64784e7402504082b2e3313f2391e65c"
          ]
        },
        "id": "MifXx_iuXfLi",
        "outputId": "5921f33c-e2d4-4a5f-ec57-bbcfbd050608"
      },
      "id": "MifXx_iuXfLi",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250527_084416-xkgs18p9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/xkgs18p9' target=\"_blank\">SimpleResNet15</a></strong> to <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/xkgs18p9' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/xkgs18p9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniGoogLeNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (inc1): MiniInception(\n",
            "    (b1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (b3_pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (b3_proj): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (bn): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (inc2): MiniInception(\n",
            "    (b1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_1): Conv2d(56, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (b2_2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (b3_pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (b3_proj): Conv2d(56, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (bn): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (aux): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(4, 4))\n",
            "    (1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU()\n",
            "    (3): Flatten(start_dim=1, end_dim=-1)\n",
            "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "    (7): Linear(in_features=128, out_features=7, bias=True)\n",
            "  )\n",
            "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Dropout(p=0.5, inplace=False)\n",
            "    (2): Linear(in_features=88, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ccd149db6554c459f48881c6d70e16b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-29-d271a52df45f>\", line 19, in model_pipeline\n",
            "    train(model, train_loader, test_loader, criterion, optimizer, config)\n",
            "  File \"<ipython-input-28-552860a888ed>\", line 54, in train\n",
            "    loss = criterion(outputs, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\", line 1295, in forward\n",
            "    return F.cross_entropy(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 3494, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">SimpleResNet15</strong> at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/xkgs18p9' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval/runs/xkgs18p9</a><br> View project at: <a href='https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval' target=\"_blank\">https://wandb.ai/arazm21-free-university-of-tbilisi-/expression_dataset_better_eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250527_084416-xkgs18p9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9807c214ebcc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-d271a52df45f>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# # and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# test(model, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# final test; you can use actual test set here if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-552860a888ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PhusPwNHkvAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef35bf1-ca78-4d60-874b-0e246d8c49a0"
      },
      "id": "PhusPwNHkvAx",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ccd149db6554c459f48881c6d70e16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_745d7737b43740049aff2edc40da28f2",
              "IPY_MODEL_9d9f6035d6094347b7c8da077fd4172c",
              "IPY_MODEL_e3f05b3d8ddb4fce8553df40b7bd7933"
            ],
            "layout": "IPY_MODEL_eedd71b3b64d4333802c8d7b4afdb92d"
          }
        },
        "745d7737b43740049aff2edc40da28f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c75a7f1bba104864ad13098c827fdb32",
            "placeholder": "​",
            "style": "IPY_MODEL_c6f3ceedf6354a529127bc2baa210b40",
            "value": "  0%"
          }
        },
        "9d9f6035d6094347b7c8da077fd4172c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffbda0e9ab0249919c5a0d3530be3c5c",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c9f166db7834252823f660694cf993c",
            "value": 0
          }
        },
        "e3f05b3d8ddb4fce8553df40b7bd7933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea4b236b56547d4949adeb73acebdc3",
            "placeholder": "​",
            "style": "IPY_MODEL_64784e7402504082b2e3313f2391e65c",
            "value": " 0/250 [00:02&lt;?, ?it/s]"
          }
        },
        "eedd71b3b64d4333802c8d7b4afdb92d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75a7f1bba104864ad13098c827fdb32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f3ceedf6354a529127bc2baa210b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffbda0e9ab0249919c5a0d3530be3c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9f166db7834252823f660694cf993c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ea4b236b56547d4949adeb73acebdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64784e7402504082b2e3313f2391e65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}